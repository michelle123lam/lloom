doc_id,text,Author(s),Title,academic,affiliations,ambiguous opt out,clustering subject preference,country,human review,image based PDF,impact statement,impact statement title,industry,manually corrected,mixed,opt out,paper authors (separate scrape),paper identifier,paper link,paper title (separate scrape),paper title (subjects),primary subject area,secondary subject areas,sentence count,statement,statement_id,title,url,word count
3,"This work proposes a novel continual learning algorithm which will contribute to the advance of
related methods. Continual learning of dynamic tasks has not been well-explored in machine learning
so far, but will likely be important for ﬁelds such as robotics and developing artiﬁcial intelligent
agents more generally. Furthermore, we utilize the framework of recurrent networks to test and reﬁne
hypotheses about computation in biological systems. Advances in this area will contribute to the
design of new experiments and aid the analyses of recorded data in the ﬁeld of neuroscience.
","Lea Duncker, Laura Driscoll, Krishna V. Shenoy, Maneesh Sahani, David Sussillo",Organizing recurrent network dynamics by task-computation to enable continual learning,1.0,"{'Stanford', 'Gatsby Unit, UCL', 'Stanford University'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)","{'UK', 'USA'}",False,False,"This work proposes a novel continual learning algorithm which will contribute to the advance of related methods. Continual learning of dynamic tasks has not been well-explored in machine learning so far, but will likely be important for fields such as robotics and developing artificial intelligent agents more generally. Furthermore, we utilize the framework of recurrent networks to test and refine hypotheses about computation in biological systems. Advances in this area will contribute to the design of new experiments and aid the analyses of recorded data in the field of neuroscience.",Broader Impact,0.0,False,0.0,,"Lea Duncker, Laura Driscoll, Krishna V. Shenoy, Maneesh Sahani, David Sussillo",a576eafbce762079f7d1f77fca1c5cc2,https://proceedings.neurips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf,Organizing recurrent network dynamics by task-computation to enable continual learning,Organizing recurrent network dynamics by task-computation to enable continual learning,Neuroscience and Cognitive Science -> Neuroscience,Algorithms -> Continual Learning; Algorithms -> Dynamical Systems; Algorithms -> Multitask and Transfer Learning; Deep Learning -> Recurrent Networks; Neuroscience and Cognitive Science -> Memory,4.0,"This work proposes a novel continual learning algorithm which will contribute to the advance of
related methods. Continual learning of dynamic tasks has not been well-explored in machine learning
so far, but will likely be important for ﬁelds such as robotics and developing artiﬁcial intelligent
agents more generally. Furthermore, we utilize the framework of recurrent networks to test and reﬁne
hypotheses about computation in biological systems. Advances in this area will contribute to the
design of new experiments and aid the analyses of recorded data in the ﬁeld of neuroscience.
",3,Organizing recurrent network dynamics by task-computation to enable continual learning,https://papers.nips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf,90.0
8,"
For the potential positive impacts, we anticipate that the work may raise the public attention about
the security and accountability issues of graph-based machine learning techniques, especially when
they are applied to real-world social networks. Even without accessing any information about the
model training, the graph structure alone can be exploited to damage a deep learning framework with
a rather executable strategy.

On the potential negative side, as our work demonstrates that there is a chance to attack existing GNN
models effectively without any knowledge but a simple graph structure, this may expose a serious
alert to technology companies who maintain the platforms and operate various applications based
on the graphs. However, we believe making this security concern transparent can help practitioners
detect potential attack in this form and better defend the machine learning driven applications.

","Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei",Towards More Practical Adversarial Attacks on Graph Neural Networks,1.0,{'University of Michigan'},,Deep learning,{'USA'},False,False,"For the potential positive impacts, we anticipate that the work may raise the public attention about the security and accountability issues of graph-based machine learning techniques, especially when they are applied to real-world social networks. Even without accessing any information about the model training, the graph structure alone can be exploited to damage a deep learning framework with a rather executable strategy. On the potential negative side, as our work demonstrates that there is a chance to attack existing GNN models effectively without any knowledge but a simple graph structure, this may expose a serious alert to technology companies who maintain the platforms and operate various applications based on the graphs. However, we believe making this security concern transparent can help practitioners detect potential attack in this form and better defend the machine learning driven applications.",Broader Impact,0.0,False,0.0,,"Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei",32bb90e8976aab5298d5da10fe66f21d,https://proceedings.neurips.cc/paper/2020/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,Towards More Practical Adversarial Attacks on Graph Neural Networks,Towards More Practical Adversarial Attacks on Graph Neural Networks,Algorithms,Algorithms -> Adversarial Learning; Optimization -> Submodular Optimization; Social Aspects of Machine Learning -> AI Safety,4.0,"
For the potential positive impacts, we anticipate that the work may raise the public attention about
the security and accountability issues of graph-based machine learning techniques, especially when
they are applied to real-world social networks. Even without accessing any information about the
model training, the graph structure alone can be exploited to damage a deep learning framework with
a rather executable strategy.

On the potential negative side, as our work demonstrates that there is a chance to attack existing GNN
models effectively without any knowledge but a simple graph structure, this may expose a serious
alert to technology companies who maintain the platforms and operate various applications based
on the graphs. However, we believe making this security concern transparent can help practitioners
detect potential attack in this form and better defend the machine learning driven applications.

",8,Towards More Practical Adversarial Attacks on Graph Neural Networks,https://papers.nips.cc/paper/2020/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,136.0
11,"
Who may beneﬁt from this research? We believe users and developers of approximate inference
methods will beneﬁt from our work. Our framework works as an “outer wrapper” that can improve
the effectiveness of any ﬂow-based variational inference method by guiding its structure. We hope to
make expressive ﬂow-based variational inference more tractable, efﬁcient, and broadly applicable,
particularly in high dimensions, by developing automated tests for low-dimensional structure and
ﬂexible ways to exploit it. The trace diagnostic developed in our work rigorously assesses the quality
of transport/ﬂow-based inference, and may be of independent interest.

Who may be put at disadvantage from this research? We don’t believe anyone is put at disad-
vantage due to this research.

What are the consequences of failure of the system? We speciﬁcally point out that one contribu-
tion of this work is identifying when a poor posterior approximation has occurred. A potential failure
mode of our framework would be inaccurate estimation of the diagnostic matrix H or its spectrum,
suggesting that the approximate posterior is more accurate than it truly is. However, computing
the eigenvalues or trace of a symmetric matrix, even one estimated from samples, is a well studied
problem. And numerical software guards against poor eigenvalue estimation or at least warns if this
occurs. We believe the theoretical underpinnings of this work make it robust to undetected failure.

Does the task/method leverage biases in the data? We don’t believe our method leverages data
bias. As a method for variational inference, our goal is to accurately approximate a posterior
distribution. It is very possible to encode biases for/against a particular result in a Bayesian inference
problem, but that occurs at the level of modeling (choosing the prior, deﬁning the likelihood) and
collecting data, not at the level of approximating the posterior.

","Michael Brennan, Daniele Bigoni, Olivier Zahm, Alessio Spantini, Youssef Marzouk",Greedy inference with structure-exploiting lazy maps,1.0,"{'Massachusetts Institute of Technology', 'INRIA'}",,Probabilistic methods and inference,"{'France', 'USA'}",True,False,"Who may benefit from this research? We believe users and developers of approximate inference methods will benefit from our work. Our framework works as an “outer wrapper” that can improve the effectiveness of any flow-based variational inference method by guiding its structure. We hope to make expressive flow-based variational inference more tractable, efficient, and broadly applicable, particularly in high dimensions, by developing automated tests for low-dimensional structure and flexible ways to exploit it. The trace diagnostic developed in our work rigorously assesses the quality of transport/flow-based inference, and may be of independent interest. Who may be put at disadvantage from this research? We don’t believe anyone is put at disadvantage due to this research. What are the consequences of failure of the system? We specifically point out that one contribution of this work is identifying when a poor posterior approximation has occurred. A potential failure mode of our framework would be inaccurate estimation of the diagnostic matrix H or its spectrum, suggesting that the approximate posterior is more accurate than it truly is. However, computing the eigenvalues or trace of a symmetric matrix, even one estimated from samples, is a well studied problem. And numerical software guards against poor eigenvalue estimation or at least warns if this occurs. We believe the theoretical underpinnings of this work make it robust to undetected failure. Does the task/method leverage biases in the data? We don’t believe our method leverages data bias. As a method for variational inference, our goal is to accurately approximate a posterior distribution. It is very possible to encode biases for/against a particular result in a Bayesian inference problem, but that occurs at the level of modeling (choosing the prior, defining the likelihood) and collecting data, not at the level of approximating the posterior.",Broader Impact,0.0,True,0.0,,"Michael Brennan, Daniele Bigoni, Olivier Zahm, Alessio Spantini, Youssef Marzouk",5ef20b89bab8fed38253e98a12f26316,https://proceedings.neurips.cc/paper/2020/file/5ef20b89bab8fed38253e98a12f26316-Paper.pdf,Greedy inference with structure-exploiting lazy maps,Greedy inference with structure-exploiting lazy maps,Probabilistic Methods -> Variational Inference,Algorithms -> Density Estimation; Deep Learning -> Efficient Inference Methods; Deep Learning -> Generative Models; Theory -> High-Dimensional Inference,17.0,"
Who may beneﬁt from this research? We believe users and developers of approximate inference
methods will beneﬁt from our work. Our framework works as an “outer wrapper” that can improve
the effectiveness of any ﬂow-based variational inference method by guiding its structure. We hope to
make expressive ﬂow-based variational inference more tractable, efﬁcient, and broadly applicable,
particularly in high dimensions, by developing automated tests for low-dimensional structure and
ﬂexible ways to exploit it. The trace diagnostic developed in our work rigorously assesses the quality
of transport/ﬂow-based inference, and may be of independent interest.

Who may be put at disadvantage from this research? We don’t believe anyone is put at disad-
vantage due to this research.

What are the consequences of failure of the system? We speciﬁcally point out that one contribu-
tion of this work is identifying when a poor posterior approximation has occurred. A potential failure
mode of our framework would be inaccurate estimation of the diagnostic matrix H or its spectrum,
suggesting that the approximate posterior is more accurate than it truly is. However, computing
the eigenvalues or trace of a symmetric matrix, even one estimated from samples, is a well studied
problem. And numerical software guards against poor eigenvalue estimation or at least warns if this
occurs. We believe the theoretical underpinnings of this work make it robust to undetected failure.

Does the task/method leverage biases in the data? We don’t believe our method leverages data
bias. As a method for variational inference, our goal is to accurately approximate a posterior
distribution. It is very possible to encode biases for/against a particular result in a Bayesian inference
problem, but that occurs at the level of modeling (choosing the prior, deﬁning the likelihood) and
collecting data, not at the level of approximating the posterior.

",11,Greedy inference with structure-exploiting lazy maps,https://papers.nips.cc/paper/2020/file/5ef20b89bab8fed38253e98a12f26316-Paper.pdf,294.0
41,"Unsupervised representation learning can improve learning when only small amounts of labeled data
are available. This is the case in many applications of societal interest, such as medical data analysis
[5, 31], the sciences [22], or drug discovery and repurposing [38]. Improving representation learning,
as we do here, can potentially beneﬁt all these applications.

However, biases in the data can naturally lead to biases in the learned representation [29]. These
biases can, for example, lead to worse performance for smaller classes or groups. For instance, the
majority groups are sampled more frequently than the minority ones [16]. In this respect, our method
may suffer from similar biases as standard contrastive learning, and it is an interesting avenue of
future research to thoroughly test and evaluate this.
","Ching-Yao Chuang, Joshua Robinson, Yen-Chen  Lin, Antonio Torralba, Stefanie Jegelka",Debiased Contrastive Learning,1.0,{'MIT'},,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},True,False,"Unsupervised representation learning can improve learning when only small amounts of labeled data are available. This is the case in many applications of societal interest, such as medical data analysis [ 5, 31], the sciences [22], or drug discovery and repurposing [38]. Improving representation learning, as we do here, can potentially benefit all these applications. However, biases in the data can naturally lead to biases in the learned representation [29]. These biases can, for example, lead to worse performance for smaller classes or groups. For instance, the majority groups are sampled more frequently than the minority ones [16]. In this respect, our method may suffer from similar biases as standard contrastive learning, and it is an interesting avenue of future research to thoroughly test and evaluate this.",Broader Impact,0.0,True,0.0,,"Ching-Yao Chuang, Joshua Robinson, Yen-Chen  Lin, Antonio Torralba, Stefanie Jegelka",63c3ddcc7b23daa1e42dc41f9a44a873,https://proceedings.neurips.cc/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf,Debiased Contrastive Learning,Debiased Contrastive Learning,Algorithms -> Representation Learning,Algorithms -> Unsupervised Learning; Deep Learning -> Embedding Approaches,7.0,"Unsupervised representation learning can improve learning when only small amounts of labeled data
are available. This is the case in many applications of societal interest, such as medical data analysis
[5, 31], the sciences [22], or drug discovery and repurposing [38]. Improving representation learning,
as we do here, can potentially beneﬁt all these applications.

However, biases in the data can naturally lead to biases in the learned representation [29]. These
biases can, for example, lead to worse performance for smaller classes or groups. For instance, the
majority groups are sampled more frequently than the minority ones [16]. In this respect, our method
may suffer from similar biases as standard contrastive learning, and it is an interesting avenue of
future research to thoroughly test and evaluate this.
",41,Debiased Contrastive Learning,https://papers.nips.cc/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf,127.0
44,"Causality and interpretability are crucial aspects of modern machine learning systems. Graphical
models in particular are a promising tool at the intersection of causality and interpretability, and
our work provides an intuitive approach to balance these issues against modeling ﬂexibility with
nonparametric models. That being said, as this work is primarily theoretical, the broader impacts and
ethical implications of our work are most likely to be felt downstream in applications. For example,
while DAGs can provide causal insights under certain assumptions, these models can potentially be
used to provide a false sense of security when they are not applied and deployed carefully. Along
these lines, our work attempts to provide a rigourous sense of when ﬂexible nonparametric causal
models can be learned from data, by developing both theory and algorithms to justify these models
from both mathematical and empirical perspectives.

","Ming Gao, Yi Ding, Bryon Aragam",A polynomial-time algorithm for learning nonparametric causal graphs,1.0,"{'University of Chicago', 'the University of Chicago'}",,Causality,{'USA'},False,False,"Causality and interpretability are crucial aspects of modern machine learning systems. Graphical models in particular are a promising tool at the intersection of causality and interpretability, and our work provides an intuitive approach to balance these issues against modeling flexibility with nonparametric models. That being said, as this work is primarily theoretical, the broader impacts and ethical implications of our work are most likely to be felt downstream in applications. For example, while DAGs can provide causal insights under certain assumptions, these models can potentially be used to provide a false sense of security when they are not applied and deployed carefully. Along these lines, our work attempts to provide a rigourous sense of when flexible nonparametric causal models can be learned from data, by developing both theory and algorithms to justify these models from both mathematical and empirical perspectives.",Broader Impact,0.0,False,0.0,,"Ming Gao, Yi Ding, Bryon Aragam",85c9f9efab89cee90a95cb98f15feacd,https://proceedings.neurips.cc/paper/2020/file/85c9f9efab89cee90a95cb98f15feacd-Paper.pdf,A polynomial-time algorithm for learning nonparametric causal graphs,A polynomial-time algorithm for learning nonparametric causal graphs,Theory -> Statistical Learning Theory,Algorithms -> Model Selection and Structure Learning; Probabilistic Methods -> Graphical Models,5.0,"Causality and interpretability are crucial aspects of modern machine learning systems. Graphical
models in particular are a promising tool at the intersection of causality and interpretability, and
our work provides an intuitive approach to balance these issues against modeling ﬂexibility with
nonparametric models. That being said, as this work is primarily theoretical, the broader impacts and
ethical implications of our work are most likely to be felt downstream in applications. For example,
while DAGs can provide causal insights under certain assumptions, these models can potentially be
used to provide a false sense of security when they are not applied and deployed carefully. Along
these lines, our work attempts to provide a rigourous sense of when ﬂexible nonparametric causal
models can be learned from data, by developing both theory and algorithms to justify these models
from both mathematical and empirical perspectives.

",44,A polynomial-time algorithm for learning nonparametric causal graphs,https://papers.nips.cc/paper/2020/file/85c9f9efab89cee90a95cb98f15feacd-Paper.pdf,140.0
46,"As reinforcement learning becomes increasingly popular in practice and the problem dimension
grows, there is a soaring demand for data-efﬁcient learning algorithms. Through the lens of low-rank
representation of so-called Q-function, this work proposes a theoretical framework to devise efﬁcient
RL algorithms. The resulting “low-rank” algorithm, which utilizes a novel matrix estimation method,
offers both strong theoretical guarantees and appealing empirical performance.

In particular, the novel “low-rank” perspective about RL provides an effective tool to tackle RL
problems with both state and action spaces continuous, which have received much less attention
despite their practical signiﬁcance. We believe that this work serves as an important step towards
provable efﬁcient RL for continuous problems. The theoretical insights in this work can motivate
further research in both efﬁcient RL and ME, while the empirical results should be beneﬁcial more
broadly for practitioners working in continuous controls.

","Devavrat Shah, Dogyoon Song, Zhi Xu, Yuzhe Yang",Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation,1.0,"{'Massachusetts Institute of Technology', 'MIT'}",,Reinforcement learning and planning,{'USA'},False,False,"As reinforcement learning becomes increasingly popular in practice and the problem dimension grows, there is a soaring demand for data-efficient learning algorithms. Through the lens of low-rank representation of so-called Q -function, this work proposes a theoretical framework to devise efficient RL algorithms. The resulting “low-rank” algorithm, which utilizes a novel matrix estimation method, offers both strong theoretical guarantees and appealing empirical performance. In particular, the novel “low-rank” perspective about RL provides an effective tool to tackle RL problems with both state and action spaces continuous, which have received much less attention despite their practical significance. We believe that this work serves as an important step towards provable efficient RL for continuous problems. The theoretical insights in this work can motivate further research in both efficient RL and ME, while the empirical results should be beneficial more broadly for practitioners working in continuous controls.",Broader Impact,0.0,False,0.0,,"Devavrat Shah, Dogyoon Song, Zhi Xu, Yuzhe Yang",8d2355364e9a2ba1f82f975414937b43,https://proceedings.neurips.cc/paper/2020/file/8d2355364e9a2ba1f82f975414937b43-Paper.pdf,Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation,Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation,Reinforcement Learning and Planning,Theory,6.0,"As reinforcement learning becomes increasingly popular in practice and the problem dimension
grows, there is a soaring demand for data-efﬁcient learning algorithms. Through the lens of low-rank
representation of so-called Q-function, this work proposes a theoretical framework to devise efﬁcient
RL algorithms. The resulting “low-rank” algorithm, which utilizes a novel matrix estimation method,
offers both strong theoretical guarantees and appealing empirical performance.

In particular, the novel “low-rank” perspective about RL provides an effective tool to tackle RL
problems with both state and action spaces continuous, which have received much less attention
despite their practical signiﬁcance. We believe that this work serves as an important step towards
provable efﬁcient RL for continuous problems. The theoretical insights in this work can motivate
further research in both efﬁcient RL and ME, while the empirical results should be beneﬁcial more
broadly for practitioners working in continuous controls.

",46,Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation,https://papers.nips.cc/paper/2020/file/8d2355364e9a2ba1f82f975414937b43-Paper.pdf,144.0
67,"
This is a theoretical contribution that, nevertheless, has the potential of impacting a wide range of
application domains in business, engineering and science. In particular, all of those in which the
Wasserstein distance has been extensively used as a statistical inference tool (e.g. image analysis and
computer vision, signal processing, operations research, and so on). Because our paper provides a
step towards breaking the curse of dimensionality in statistical rates of convergence, we believe that
we have the potential of enabling more applications to multiple hypothesis testing (e.g., certifying
Wasserstein GANs). In turn, we plan to improve human resource development by including some of
the main ﬁndings in this paper in Ph.D. courses.

","Nian Si, Jose Blanchet, Soumyadip Ghosh, Mark Squillante",Quantifying the Empirical Wasserstein Distance to a Set of Measures: Beating the Curse of Dimensionality,1.0,"{'Stanford University', 'IBM Research'}",,Theory (including computational and statistical analyses),{'USA'},False,False,"This is a theoretical contribution that, nevertheless, has the potential of impacting a wide range of application domains in business, engineering and science. In particular, all of those in which the Wasserstein distance has been extensively used as a statistical inference tool (e.g. image analysis and computer vision, signal processing, operations research, and so on). Because our paper provides a step towards breaking the curse of dimensionality in statistical rates of convergence, we believe that we have the potential of enabling more applications to multiple hypothesis testing (e.g., certifying Wasserstein GANs). In turn, we plan to improve human resource development by including some of the main findings in this paper in Ph.D. courses.",Broader Impact,1.0,False,1.0,,"Nian Si, Jose Blanchet, Soumyadip Ghosh, Mark Squillante",f3507289cfdc8c9ae93f4098111a13f9,https://proceedings.neurips.cc/paper/2020/file/f3507289cfdc8c9ae93f4098111a13f9-Paper.pdf,Quantifying the Empirical Wasserstein Distance to a Set of Measures: Beating the Curse of Dimensionality,Quantifying the Empirical Wasserstein Distance to a Set of Measures: Beating the Curse of Dimensionality,Theory -> Large Deviations and Asymptotic Analysis,Algorithms -> Nonlinear Dimensionality Reduction and Manifold Learning; Theory -> Models of Learning and Generalization ; Theory -> Spaces of Functions and Kernels; Theory -> Statistical Learning Theory,4.0,"
This is a theoretical contribution that, nevertheless, has the potential of impacting a wide range of
application domains in business, engineering and science. In particular, all of those in which the
Wasserstein distance has been extensively used as a statistical inference tool (e.g. image analysis and
computer vision, signal processing, operations research, and so on). Because our paper provides a
step towards breaking the curse of dimensionality in statistical rates of convergence, we believe that
we have the potential of enabling more applications to multiple hypothesis testing (e.g., certifying
Wasserstein GANs). In turn, we plan to improve human resource development by including some of
the main ﬁndings in this paper in Ph.D. courses.

",67,Quantifying the Empirical Wasserstein Distance to a Set of Measures: Beating the Curse of Dimensionality,https://papers.nips.cc/paper/2020/file/f3507289cfdc8c9ae93f4098111a13f9-Paper.pdf,113.0
76,"
Our work aims to advance the algorithmic foundations of adversarially robust machine learning. This
subﬁeld focuses on protecting machine learning models (especially their predictions) against small
perturbations of the input data. This broad goal is a pressing challenge in many real-world scenarios,
where successful adversarial example attacks can have far-reaching implications given the adoption
of machine learning in a wide variety of applications, from self-driving cars to banking.

Since the primary focus of our work is theoretical and addresses a simple concept class, we do not
expect our results to have immediate societal impact. Nonetheless, we believe that our ﬁndings
provide interesting insights on the algorithmic possibilities and fundamental computational limitations
of adversarially robust learning. We hope that, in the future, these insights could be useful in the
design of practically relevant adversarially robust classiﬁers in the presence of noisy data.

","Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi",The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,1.0,"{'Google', 'UW Madison', 'UCSD'}",,,{'USA'},False,False,"Our work aims to advance the algorithmic foundations of adversarially robust machine learning. This subfield focuses on protecting machine learning models (especially their predictions) against small perturbations of the input data. This broad goal is a pressing challenge in many real-world scenarios, where successful adversarial example attacks can have far-reaching implications given the adoption of machine learning in a wide variety of applications, from self-driving cars to banking. Since the primary focus of our work is theoretical and addresses a simple concept class, we do not expect our results to have immediate societal impact. Nonetheless, we believe that our findings provide interesting insights on the algorithmic possibilities and fundamental computational limitations of adversarially robust learning. We hope that, in the future, these insights could be useful in the design of practically relevant adversarially robust classifiers in the presence of noisy data.",Broader Impact,1.0,False,1.0,,"Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi",ebd64e2bf193fc8c658af2b91952ce8d,https://proceedings.neurips.cc/paper/2020/file/ebd64e2bf193fc8c658af2b91952ce8d-Paper.pdf,The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,Theory -> Computational Learning Theory,Theory -> Hardness of Learning and Approximations,6.0,"
Our work aims to advance the algorithmic foundations of adversarially robust machine learning. This
subﬁeld focuses on protecting machine learning models (especially their predictions) against small
perturbations of the input data. This broad goal is a pressing challenge in many real-world scenarios,
where successful adversarial example attacks can have far-reaching implications given the adoption
of machine learning in a wide variety of applications, from self-driving cars to banking.

Since the primary focus of our work is theoretical and addresses a simple concept class, we do not
expect our results to have immediate societal impact. Nonetheless, we believe that our ﬁndings
provide interesting insights on the algorithmic possibilities and fundamental computational limitations
of adversarially robust learning. We hope that, in the future, these insights could be useful in the
design of practically relevant adversarially robust classiﬁers in the presence of noisy data.

",76,The Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic Noise,https://papers.nips.cc/paper/2020/file/ebd64e2bf193fc8c658af2b91952ce8d-Paper.pdf,141.0
104,"
As a theoretical paper we do not foresee our work directly having any societal consequences. However,
transfer learning is a tool increasingly used in practical machine learning applications. Theoretical
explorations related to transfer learning may help provide frameworks through which to reason about,
and design, safer and more reliable algorithms.

","Nilesh Tripuraneni, Michael Jordan, Chi Jin",On the Theory of Transfer Learning: The Importance of Task Diversity,1.0,"{'UC Berkeley', 'Princeton University'}",True,,{'USA'},False,False,"As a theoretical paper we do not foresee our work directly having any societal consequences. However, transfer learning is a tool increasingly used in practical machine learning applications. Theoretical explorations related to transfer learning may help provide frameworks through which to reason about, and design, safer and more reliable algorithms.",Broader Impact,0.0,False,0.0,True,"Nilesh Tripuraneni, Michael Jordan, Chi Jin",59587bffec1c7846f3e34230141556ae,https://proceedings.neurips.cc/paper/2020/file/59587bffec1c7846f3e34230141556ae-Paper.pdf,On the Theory of Transfer Learning: The Importance of Task Diversity,On the Theory of Transfer Learning: The Importance of Task Diversity,Theory -> Statistical Learning Theory,Theory -> Models of Learning and Generalization,3.0,"
As a theoretical paper we do not foresee our work directly having any societal consequences. However,
transfer learning is a tool increasingly used in practical machine learning applications. Theoretical
explorations related to transfer learning may help provide frameworks through which to reason about,
and design, safer and more reliable algorithms.

",104,On the Theory of Transfer Learning: The Importance of Task Diversity,https://papers.nips.cc/paper/2020/file/59587bffec1c7846f3e34230141556ae-Paper.pdf,50.0
116,"Convolutional neural networks (CNNs) can achieve superhuman performance on image classiﬁcation
tasks. This advantage allows their deployment to computer vision applications such as medical
imaging, security, and autonomous driving. However, CNNs trained on natural images tend to overﬁt
to image textures. Such ﬂaw can cause a CNN to fail against adversarial attacks and on distorted
images. This may further lead to unreliable predictions potentially causing false medical diagnoses,
trafﬁc accidents, and false identiﬁcation of criminal suspects. To address the robustness issues in
CNNs, CNN-F adopts an architectural design which resembles human vision mechanisms in certain
aspects. The deployment of CNN-F renders more robust AI systems.

Despite the improved robustness, current method does not tackle other social and ethical issues
intrinsic to a CNN. A CNN can imitate human biases in the image datasets. In automated surveillance,
biased training datasets can improperly calibrate CNN-F systems to make incorrect decisions based
on race, gender, and age. Furthermore, while robust, human-like computer vision systems can provide
a net positive societal impact, there exists potential use cases with nefarious, unethical purposes.
More human-like computer vision algorithms, for example, could circumvent human veriﬁcation
software. Motivated by these limitations, we encourage research into human bias in machine learning
and security in computer vision algorithms. We also recommend researchers and policymakers
examine how people abuse CNN models and mitigate their exploitation.

","Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Tsao, Anima Anandkumar",Neural Networks with Recurrent Generative Feedback,1.0,"{'NVIDIA', 'California Institute of Technology', 'Caltech', 'Rice University', 'NVIDIA / Caltech'}",,Deep learning,{'USA'},False,False,"Convolutional neural networks (CNNs) can achieve superhuman performance on image classification tasks. This advantage allows their deployment to computer vision applications such as medical imaging, security, and autonomous driving. However, CNNs trained on natural images tend to overfit to image textures. Such flaw can cause a CNN to fail against adversarial attacks and on distorted images. This may further lead to unreliable predictions potentially causing false medical diagnoses, traffic accidents, and false identification of criminal suspects. To address the robustness issues in CNNs, CNN-F adopts an architectural design which resembles human vision mechanisms in certain aspects. The deployment of CNN-F renders more robust AI systems. Despite the improved robustness, current method does not tackle other social and ethical issues intrinsic to a CNN. A CNN can imitate human biases in the image datasets. In automated surveillance, biased training datasets can improperly calibrate CNN-F systems to make incorrect decisions based on race, gender, and age. Furthermore, while robust, human-like computer vision systems can provide a net positive societal impact, there exists potential use cases with nefarious, unethical purposes. More human-like computer vision algorithms, for example, could circumvent human verification software. Motivated by these limitations, we encourage research into human bias in machine learning and security in computer vision algorithms. We also recommend researchers and policymakers examine how people abuse CNN models and mitigate their exploitation.",Broader Impacts,1.0,False,1.0,,"Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Tsao, Anima Anandkumar",0660895c22f8a14eb039bfb9beb0778f,https://proceedings.neurips.cc/paper/2020/file/0660895c22f8a14eb039bfb9beb0778f-Paper.pdf,Neural Networks with Recurrent Generative Feedback,Neural Networks with Recurrent Generative Feedback,Neuroscience and Cognitive Science,Neuroscience and Cognitive Science -> Visual Perception,14.0,"Convolutional neural networks (CNNs) can achieve superhuman performance on image classiﬁcation
tasks. This advantage allows their deployment to computer vision applications such as medical
imaging, security, and autonomous driving. However, CNNs trained on natural images tend to overﬁt
to image textures. Such ﬂaw can cause a CNN to fail against adversarial attacks and on distorted
images. This may further lead to unreliable predictions potentially causing false medical diagnoses,
trafﬁc accidents, and false identiﬁcation of criminal suspects. To address the robustness issues in
CNNs, CNN-F adopts an architectural design which resembles human vision mechanisms in certain
aspects. The deployment of CNN-F renders more robust AI systems.

Despite the improved robustness, current method does not tackle other social and ethical issues
intrinsic to a CNN. A CNN can imitate human biases in the image datasets. In automated surveillance,
biased training datasets can improperly calibrate CNN-F systems to make incorrect decisions based
on race, gender, and age. Furthermore, while robust, human-like computer vision systems can provide
a net positive societal impact, there exists potential use cases with nefarious, unethical purposes.
More human-like computer vision algorithms, for example, could circumvent human veriﬁcation
software. Motivated by these limitations, we encourage research into human bias in machine learning
and security in computer vision algorithms. We also recommend researchers and policymakers
examine how people abuse CNN models and mitigate their exploitation.

",116,Neural Networks with Recurrent Generative Feedback,https://papers.nips.cc/paper/2020/file/0660895c22f8a14eb039bfb9beb0778f-Paper.pdf,224.0
149,"
Our contribution is primarily theoretical. Therefore, a broader impact discussion is not applicable.","Eduard Gorbunov, Marina Danilova, Alexander Gasnikov",Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping,1.0,"{'SkolTech', 'Moscow Institute of Physics and Technology', 'ICS RAS'}",False,Optimization Methods (continuous or discrete),{'Russia'},False,False,"Our contribution is primarily theoretical. Therefore, a broader impact discussion is not applicable.",Broader Impact,0.0,False,0.0,True,"Eduard Gorbunov, Marina Danilova, Alexander Gasnikov",abd1c782880cc59759f4112fda0b8f98,https://proceedings.neurips.cc/paper/2020/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf,Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping,Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping,Optimization -> Stochastic Optimization,Optimization -> Convex Optimization,2.0,"
Our contribution is primarily theoretical. Therefore, a broader impact discussion is not applicable.",149,Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping,https://papers.nips.cc/paper/2020/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf,13.0
180,"While the main thrust of our work is foundational in nature, we do demonstrate the potential for
implicit models to become practical alternatives to traditional deep networks. Owing to their improved
memory efficiency, these networks have the potential to further applications of AI methods on edge
devices, where they are currently largely impractical. However, the work is still largely algorithmic
in nature, and thus it is much less clear the immediate societal-level benefits (or harms) that could
result from the specific tehniques we propose and demonstrate in this paper.","Ezra Winston, J. Zico Kolter",Monotone operator equilibrium networks,1.0,"{'Carnegie Mellon University', 'Carnegie Mellon University / Bosch Center for AI'}",,Deep learning,"{'USA', 'Germany'}",False,False,"While the main thrust of our work is foundational in nature, we do demonstrate the potential for implicit models to become practical alternatives to traditional deep networks. Owing to their improved memory efficiency, these networks have the potential to further applications of AI methods on edge devices, where they are currently largely impractical. However, the work is still largely algorithmic in nature, and thus it is much less clear the immediate societal-level benefits (or harms) that could result from the specific tehniques we propose and demonstrate in this paper.",Broader impact statement,1.0,False,1.0,,"Ezra Winston, J. Zico Kolter",798d1c2813cbdf8bcdb388db0e32d496,https://proceedings.neurips.cc/paper/2020/file/798d1c2813cbdf8bcdb388db0e32d496-Paper.pdf,Monotone operator equilibrium networks,Monotone operator equilibrium networks,Deep Learning,Deep Learning -> Analysis and Understanding of Deep Networks; Deep Learning -> Optimization for Deep Networks; Optimization -> Convex Optimization,3.0,"While the main thrust of our work is foundational in nature, we do demonstrate the potential for
implicit models to become practical alternatives to traditional deep networks. Owing to their improved
memory efficiency, these networks have the potential to further applications of AI methods on edge
devices, where they are currently largely impractical. However, the work is still largely algorithmic
in nature, and thus it is much less clear the immediate societal-level benefits (or harms) that could
result from the specific tehniques we propose and demonstrate in this paper.",180,Monotone operator equilibrium networks,https://papers.nips.cc/paper/2020/file/798d1c2813cbdf8bcdb388db0e32d496-Paper.pdf,89.0
193,"
Interpretability tools for understanding feature representations. Recently, a number of works
have focused on explaining or interpreting deep learning models; such research is often known as
explainable AI (XAI) or interpretability [22]. Due to the highly-parameterized nature of CNNs, most
researchers treat such models as black-boxes and primarily evaluate them based on task performance
on well-curated datasets (e.g. ImageNet classiﬁcation). However, as deep learning is increasingly
applied to high-impact, yet high-risk domains (e.g. autonomous driving and medical applications),
there is a great need for tools that help us understand CNNs, so that we can in turn understand their
limitations and biases. Our work contributes to the development of interpretability tools that can help
society to responsible use and interrogate advanced technology built on deep learning. We primarily
do this in two ways.

Development of principled interpretability metrics. First, we present a principled framework
for evaluating the human-interpretability of CNN representations. While this may seem trivial, the
interpretability research community has been lagging behind in the development of such metrics.
There have been two main shortcomings of most interpretability evaluation: 1., they are often based
on subjective or qualitative inspection, and 2., they fail to evaluate the faithfulness and interpretability
of an explanation, that is, it should be both an accurate description of CNN behavior and easy-to-
understand. These two shortcomings often go hand-in-hand. For example, [2, 20, 45, 58] highlight
this issue for attribution heatmaps, which explain what parts of an image are responsible for the
model’s output decision. In particular, [2] shows that a number of attribution methods that are
typically preferred for their visual appearance actually do not accurately describe the CNN being
explained. Most metrics focus on evaluating the interpretability of an explanation without also
measuring its faithfulness. This is a major limitation, as an explanation is not useful if it does not
accurately describe the phenomenon being explained.

The typical methodology for human evaluation of CNN interpretability asks humans subjective
questions like, “which explanatory visualization do you prefer or trust more?” [77], “do these images
systematically describe a common visual concept?” [24], and “if so, name that concept” [76]. Such
evaluations tend to evaluate the interpretability without faithfulness (i.e. how can we verify that
this is the most accurate name for the concept?). In contrast, our work evaluates using both criteria
by shifting from using humans as subjective annotators to using them as more learners that can
be evaluated objectively. Our coherence metric objectively measures how interpretable a CNN-
discovered cluster of images is, while our describability metric quantiﬁes how faithfully a natural
language description accurately characterizes such a cluster. We hope that our work serves as a
springboard for future work that enables the use of human annotators in evaluating the interpretability
of CNNs in a more principled manner.

Understanding self-supervised representations. Second, we focus on understanding self-
supervised representations. Most work to date has focused on understanding CNNs trained for
image classiﬁcation.4 However, supervised methods like image classiﬁers are limited in that they
require expensive, manual annotation of highly-curated datasets. Thus, recent developments of self-
and un-supervised methods is exciting, as they do not require manual labels. That said, there has
been relatively little work dedicated to understanding self-supervised representations. The few works
that do explore self-supervised representations typically apply techniques developed on supervised
image classiﬁers to them [5, 19].

In contrast, we developed our evaluation paradigm with self-supervised methods in mind. In particular,
we were motivated to develop an evaluation framework that could measure the interpretability of
coherent, visual concepts that fall outside the limits of being described by labelled datasets. For
example, in Fig. 3, we show that one self-supervised method discovered distinct clusters that highlight
different environments of the same concept (e.g. different environments for playing volleyball).
Standard interpretability methods of describing such clusters using a labelled dataset [5, 19] would
likely map them onto the same label (e.g. “volleyball”) and fail to characterize the subtle nuances
captured by different clusters. Lastly, by design, our paradigm is agnostic to method and can also be
used to understand other kinds of image representations, including non-CNN ones. We hope our work
encourages further research on understanding other kinds of representations beyond image classiﬁers
and developing interpretability methods explicitly for those settings.

4This machine learning workshop highlights this over-emphasis and encourages more diverse XAI work.

","Iro Laina, Ruth Fong, Andrea Vedaldi",Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning,1.0,"{'Facebook AI Research and University of Oxford', 'University of Oxford'}",,Deep learning,{'UK'},True,False,"Interpretability tools for understanding feature representations. Recently, a number of works have focused on explaining or interpreting deep learning models; such research is often known as explainable AI (XAI) or interpretability [22]. Due to the highly-parameterized nature of CNNs, most researchers treat such models as black-boxes and primarily evaluate them based on task performance on well-curated datasets ( e . g . ImageNet classification). However, as deep learning is increasingly applied to high-impact, yet high-risk domains ( e . g . autonomous driving and medical applications), there is a great need for tools that help us understand CNNs, so that we can in turn understand their limitations and biases. Our work contributes to the development of interpretability tools that can help society to responsible use and interrogate advanced technology built on deep learning. We primarily do this in two ways. Development of principled interpretability metrics. First, we present a principled framework for evaluating the human-interpretability of CNN representations. While this may seem trivial, the interpretability research community has been lagging behind in the development of such metrics. There have been two main shortcomings of most interpretability evaluation: 1., they are often based on subjective or qualitative inspection, and 2., they fail to evaluate the faithfulness and interpretability of an explanation, that is, it should be both an accurate description of CNN behavior and easy-to- understand. These two shortcomings often go hand-in-hand. For example, [2, 20, 45, 58] highlight this issue for attribution heatmaps, which explain what parts of an image are responsible for the model’s output decision. In particular, [2] shows that a number of attribution methods that are typically preferred for their visual appearance actually do not accurately describe the CNN being explained. Most metrics focus on evaluating the interpretability of an explanation without also measuring its faithfulness. This is a major limitation, as an explanation is not useful if it does not accurately describe the phenomenon being explained. The typical methodology for human evaluation of CNN interpretability asks humans subjective questions like, “which explanatory visualization do you prefer or trust more?” [77], “do these images systematically describe a common visual concept?” [24], and “if so, name that concept” [76]. Such evaluations tend to evaluate the interpretability without faithfulness ( i . e . how can we verify that this is the most accurate name for the concept?). In contrast, our work evaluates using both criteria by shifting from using humans as subjective annotators to using them as more learners that can be evaluated objectively. Our coherence metric objectively measures how interpretable a CNN- discovered cluster of images is, while our describability metric quantifies how faithfully a natural language description accurately characterizes such a cluster. We hope that our work serves as a springboard for future work that enables the use of human annotators in evaluating the interpretability of CNNs in a more principled manner. Understanding self-supervised representations. Second, we focus on understanding self- supervised representations. Most work to date has focused on understanding CNNs trained for image classification. 4 However, supervised methods like image classifiers are limited in that they require expensive, manual annotation of highly-curated datasets. Thus, recent developments of self- and un-supervised methods is exciting, as they do not require manual labels. That said, there has been relatively little work dedicated to understanding self-supervised representations. The few works that do explore self-supervised representations typically apply techniques developed on supervised image classifiers to them [5, 19]. In contrast, we developed our evaluation paradigm with self-supervised methods in mind. In particular, we were motivated to develop an evaluation framework that could measure the interpretability of coherent, visual concepts that fall outside the limits of being described by labelled datasets. For example, in Fig. 3, we show that one self-supervised method discovered distinct clusters that highlight different environments of the same concept ( e . g . different environments for playing volleyball). Standard interpretability methods of describing such clusters using a labelled dataset [5, 19] would likely map them onto the same label ( e . g . “volleyball”) and fail to characterize the subtle nuances captured by different clusters. Lastly, by design, our paradigm is agnostic to method and can also be used to understand other kinds of image representations, including non-CNN ones. We hope our work encourages further research on understanding other kinds of representations beyond image classifiers and developing interpretability methods explicitly for those settings. 4 This machine learning workshop highlights this over-emphasis and encourages more diverse XAI work.",Broader Impact,0.0,True,0.0,,"Iro Laina, Ruth Fong, Andrea Vedaldi",98dce83da57b0395e163467c9dae521b,https://proceedings.neurips.cc/paper/2020/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning,Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning,"Deep Learning -> Visualization, Interpretability, and Explainability",Applications -> Computer Vision,45.0,"
Interpretability tools for understanding feature representations. Recently, a number of works
have focused on explaining or interpreting deep learning models; such research is often known as
explainable AI (XAI) or interpretability [22]. Due to the highly-parameterized nature of CNNs, most
researchers treat such models as black-boxes and primarily evaluate them based on task performance
on well-curated datasets (e.g. ImageNet classiﬁcation). However, as deep learning is increasingly
applied to high-impact, yet high-risk domains (e.g. autonomous driving and medical applications),
there is a great need for tools that help us understand CNNs, so that we can in turn understand their
limitations and biases. Our work contributes to the development of interpretability tools that can help
society to responsible use and interrogate advanced technology built on deep learning. We primarily
do this in two ways.

Development of principled interpretability metrics. First, we present a principled framework
for evaluating the human-interpretability of CNN representations. While this may seem trivial, the
interpretability research community has been lagging behind in the development of such metrics.
There have been two main shortcomings of most interpretability evaluation: 1., they are often based
on subjective or qualitative inspection, and 2., they fail to evaluate the faithfulness and interpretability
of an explanation, that is, it should be both an accurate description of CNN behavior and easy-to-
understand. These two shortcomings often go hand-in-hand. For example, [2, 20, 45, 58] highlight
this issue for attribution heatmaps, which explain what parts of an image are responsible for the
model’s output decision. In particular, [2] shows that a number of attribution methods that are
typically preferred for their visual appearance actually do not accurately describe the CNN being
explained. Most metrics focus on evaluating the interpretability of an explanation without also
measuring its faithfulness. This is a major limitation, as an explanation is not useful if it does not
accurately describe the phenomenon being explained.

The typical methodology for human evaluation of CNN interpretability asks humans subjective
questions like, “which explanatory visualization do you prefer or trust more?” [77], “do these images
systematically describe a common visual concept?” [24], and “if so, name that concept” [76]. Such
evaluations tend to evaluate the interpretability without faithfulness (i.e. how can we verify that
this is the most accurate name for the concept?). In contrast, our work evaluates using both criteria
by shifting from using humans as subjective annotators to using them as more learners that can
be evaluated objectively. Our coherence metric objectively measures how interpretable a CNN-
discovered cluster of images is, while our describability metric quantiﬁes how faithfully a natural
language description accurately characterizes such a cluster. We hope that our work serves as a
springboard for future work that enables the use of human annotators in evaluating the interpretability
of CNNs in a more principled manner.

Understanding self-supervised representations. Second, we focus on understanding self-
supervised representations. Most work to date has focused on understanding CNNs trained for
image classiﬁcation.4 However, supervised methods like image classiﬁers are limited in that they
require expensive, manual annotation of highly-curated datasets. Thus, recent developments of self-
and un-supervised methods is exciting, as they do not require manual labels. That said, there has
been relatively little work dedicated to understanding self-supervised representations. The few works
that do explore self-supervised representations typically apply techniques developed on supervised
image classiﬁers to them [5, 19].

In contrast, we developed our evaluation paradigm with self-supervised methods in mind. In particular,
we were motivated to develop an evaluation framework that could measure the interpretability of
coherent, visual concepts that fall outside the limits of being described by labelled datasets. For
example, in Fig. 3, we show that one self-supervised method discovered distinct clusters that highlight
different environments of the same concept (e.g. different environments for playing volleyball).
Standard interpretability methods of describing such clusters using a labelled dataset [5, 19] would
likely map them onto the same label (e.g. “volleyball”) and fail to characterize the subtle nuances
captured by different clusters. Lastly, by design, our paradigm is agnostic to method and can also be
used to understand other kinds of image representations, including non-CNN ones. We hope our work
encourages further research on understanding other kinds of representations beyond image classiﬁers
and developing interpretability methods explicitly for those settings.

4This machine learning workshop highlights this over-emphasis and encourages more diverse XAI work.

",193,Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning,https://papers.nips.cc/paper/2020/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,743.0
201,"
Deep learning has transformed modern science in an unprecedented manner and created a new
force for technological developments. However, its black-box nature and the lacking of theoretical
justiﬁcations have hindered its applications in ﬁelds where correct interpretations play an essential
role. In many applications, a linear model with a justiﬁed conﬁdence interval and a rigorous feature
selection procedure is much more favored than a deep learning system that cannot be interpreted.
Usage of deep learning in a process that requires transparency such as judicial and public decisions is
still completely out of the question.

To the best of our knowledge, this is the ﬁrst work that establishes feature selection consistency, an
important cornerstone of interpretable statistical inference, for deep learning. The results of this
work will greatly extend the set of problems to which statistical inference with deep learning can be
applied. Medical sciences, public health decisions, and various ﬁelds of engineering, which depend
upon well-founded estimates of uncertainty, fall naturally on the domain the work tries to explore.
Researchers from these ﬁelds and the public alike may beneﬁt from such a development and no one
is put at disadvantage from this research.

By trying to select a parsimonious and transparent model out of an over-parametrized deep learning
system, the approach of this work further provides a systematic way to detect and reduce bias in
machine learning analysis. The analytical tools and the theoretical framework derived in this work
may also be of independent interest in statistics, machine learning, and other ﬁelds of applied sciences.","Vu C. Dinh, Lam S. Ho",Consistent feature selection for analytic deep neural networks,,,,,,,,,,,,,,,,,,,,,,"
Deep learning has transformed modern science in an unprecedented manner and created a new
force for technological developments. However, its black-box nature and the lacking of theoretical
justiﬁcations have hindered its applications in ﬁelds where correct interpretations play an essential
role. In many applications, a linear model with a justiﬁed conﬁdence interval and a rigorous feature
selection procedure is much more favored than a deep learning system that cannot be interpreted.
Usage of deep learning in a process that requires transparency such as judicial and public decisions is
still completely out of the question.

To the best of our knowledge, this is the ﬁrst work that establishes feature selection consistency, an
important cornerstone of interpretable statistical inference, for deep learning. The results of this
work will greatly extend the set of problems to which statistical inference with deep learning can be
applied. Medical sciences, public health decisions, and various ﬁelds of engineering, which depend
upon well-founded estimates of uncertainty, fall naturally on the domain the work tries to explore.
Researchers from these ﬁelds and the public alike may beneﬁt from such a development and no one
is put at disadvantage from this research.

By trying to select a parsimonious and transparent model out of an over-parametrized deep learning
system, the approach of this work further provides a systematic way to detect and reduce bias in
machine learning analysis. The analytical tools and the theoretical framework derived in this work
may also be of independent interest in statistics, machine learning, and other ﬁelds of applied sciences.",201,,https://papers.nips.cc/paper/2020/file/1959eb9d5a0f7ebc58ebde81d5df400d-Paper.pdf,
208,"
Our work helps advance and verify the current understanding of the nature of solutions that meta-
learning brings about (our empirical work focused on modern recurrent neural network architectures
and training algorithms, but we expect the ﬁndings to qualitatively hold for a large range of AI
systems that are trained through meta-learning). Understanding how advanced AI and ML systems
work is of paramount importance for safe deployment and reliable operation of such systems. This
has also been recognized by the wider machine-learning community with a rapidly growing body of
literature in this emerging ﬁeld of “Analysis and Understanding” of deep learning. While increased
understanding is likely to ultimately also contribute towards building more capable AI systems, thus
potentially amplifying their negative aspects, we strongly believe that the merits of understanding
how these systems work clearly outweigh the potential risks in this case.

We argue that understanding meta-learning on a fundamental level is important, since meta-learning
subsumes many speciﬁc learning tasks and is thought to play an important role for AI systems that
generalize well to novel situations. Accordingly we expect meta-learning to be highly relevant over
the next decade(s) in AI research and in the development of powerful AI algorithms and applications.
In this work we also show a proof-of-concept implementation for analysis methods that might
potentially allow one to separate (heterogeneous) agents into certain equivalence classes, which
would allow to safely generalize ﬁndings about an individual agent to the whole equivalence class.
We believe that this might open up interesting future opportunities to boost the generality of analysis
methods and automatic diagnostic tools for monitoring of AI systems.","Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, Pedro Ortega",Meta-trained agents implement Bayes-optimal agents,0.0,"{'DeepMind', 'Google DeepMind', 'Deepmind'}",,Theory (including computational and statistical analyses),{'UK'},False,False,"Our work helps advance and verify the current understanding of the nature of solutions that metalearning brings about (our empirical work focused on modern recurrent neural network architectures and training algorithms, but we expect the findings to qualitatively hold for a large range of AI systems that are trained through meta-learning). Understanding how advanced AI and ML systems work is of paramount importance for safe deployment and reliable operation of such systems. This has also been recognized by the wider machine-learning community with a rapidly growing body of literature in this emerging field of “Analysis and Understanding” of deep learning. While increased understanding is likely to ultimately also contribute towards building more capable AI systems, thus potentially amplifying their negative aspects, we strongly believe that the merits of understanding how these systems work clearly outweigh the potential risks in this case. We argue that understanding meta-learning on a fundamental level is important, since meta-learning subsumes many specific learning tasks and is thought to play an important role for AI systems that generalize well to novel situations. Accordingly we expect meta-learning to be highly relevant over the next decade(s) in AI research and in the development of powerful AI algorithms and applications. In this work we also show a proof-of-concept implementation for analysis methods that might potentially allow one to separate (heterogeneous) agents into certain equivalence classes, which would allow to safely generalize findings about an individual agent to the whole equivalence class. We believe that this might open up interesting future opportunities to boost the generality of analysis methods and automatic diagnostic tools for monitoring of AI systems.",6 Broader Impact,1.0,False,0.0,,"Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, Pedro Ortega",d902c3ce47124c66ce615d5ad9ba304f,https://proceedings.neurips.cc/paper/2020/file/d902c3ce47124c66ce615d5ad9ba304f-Paper.pdf,Meta-trained agents implement Bayes-optimal agents,Meta-trained agents implement Bayes-optimal agents,Deep Learning -> Analysis and Understanding of Deep Networks,"Algorithms -> Meta-Learning; Deep Learning -> Recurrent Networks; Deep Learning -> Visualization, Interpretability, and Explainability; Social Aspects of Machine Learning -> AI Safety",8.0,"
Our work helps advance and verify the current understanding of the nature of solutions that meta-
learning brings about (our empirical work focused on modern recurrent neural network architectures
and training algorithms, but we expect the ﬁndings to qualitatively hold for a large range of AI
systems that are trained through meta-learning). Understanding how advanced AI and ML systems
work is of paramount importance for safe deployment and reliable operation of such systems. This
has also been recognized by the wider machine-learning community with a rapidly growing body of
literature in this emerging ﬁeld of “Analysis and Understanding” of deep learning. While increased
understanding is likely to ultimately also contribute towards building more capable AI systems, thus
potentially amplifying their negative aspects, we strongly believe that the merits of understanding
how these systems work clearly outweigh the potential risks in this case.

We argue that understanding meta-learning on a fundamental level is important, since meta-learning
subsumes many speciﬁc learning tasks and is thought to play an important role for AI systems that
generalize well to novel situations. Accordingly we expect meta-learning to be highly relevant over
the next decade(s) in AI research and in the development of powerful AI algorithms and applications.
In this work we also show a proof-of-concept implementation for analysis methods that might
potentially allow one to separate (heterogeneous) agents into certain equivalence classes, which
would allow to safely generalize ﬁndings about an individual agent to the whole equivalence class.
We believe that this might open up interesting future opportunities to boost the generality of analysis
methods and automatic diagnostic tools for monitoring of AI systems.",208,Meta-trained agents implement Bayes-optimal agents,https://papers.nips.cc/paper/2020/file/d902c3ce47124c66ce615d5ad9ba304f-Paper.pdf,268.0
219,"
This work performs theoretical analysis that aims to extend our understanding of training and
generalization in multi-layer neural networks. A better theoretical understanding of training and
generalization in these models may ultimately help us to (1) understand the mechanisms by which
social biases may be propagated by artiﬁcial systems, and prevent this from occurring, and (2)
increase the robustness and fault-tolerance of artiﬁcial systems built on such models.

","Zhou Fan, Zhichao Wang",Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks,,,,,,,,,,,,,,,,,,,,,,"
This work performs theoretical analysis that aims to extend our understanding of training and
generalization in multi-layer neural networks. A better theoretical understanding of training and
generalization in these models may ultimately help us to (1) understand the mechanisms by which
social biases may be propagated by artiﬁcial systems, and prevent this from occurring, and (2)
increase the robustness and fault-tolerance of artiﬁcial systems built on such models.

",219,,https://papers.nips.cc/paper/2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf,
222,"
In this paper, we study calibration of survival analysis models and suggest an objective for improving
calibration during model training. Since calibration means that modeled probabilities correspond to
the actual observed risk of an event, practitioners may feel more conﬁdent about using model outputs
directly for decision making e.g. to decide how many emergency room staff members qualiﬁed for
performing a given procedure should be present tomorrow given all current ER patients. But if the
distribution of event times in these patients differs from validation data, because say the population
has different demographics, calibration should not provide the practitioner with more conﬁdence to
directly use such model outputs.

","Mark Goldstein, Xintian Han, Aahlad Manas Puli, Adler Perotte , Rajesh Ranganath",X-CAL: Explicit Calibration for Survival Analysis,1.0,"{'New York University', 'Columbia University', 'NYU'}",,Healthcare,{'USA'},False,False,"In this paper, we study calibration of survival analysis models and suggest an objective for improving calibration during model training. Since calibration means that modeled probabilities correspond to the actual observed risk of an event, practitioners may feel more confident about using model outputs directly for decision making e.g. to decide how many emergency room staff members qualified for performing a given procedure should be present tomorrow given all current ER patients. But if the distribution of event times in these patients differs from validation data, because say the population has different demographics, calibration should not provide the practitioner with more confidence to directly use such model outputs.",Broader Impact,0.0,False,0.0,,"Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte , Rajesh Ranganath",d4a93297083a23cc099f7bd6a8621131,https://proceedings.neurips.cc/paper/2020/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf,X-CAL: Explicit Calibration for Survival Analysis,X-CAL: Explicit Calibration for Survival Analysis,Applications -> Health,,3.0,"
In this paper, we study calibration of survival analysis models and suggest an objective for improving
calibration during model training. Since calibration means that modeled probabilities correspond to
the actual observed risk of an event, practitioners may feel more conﬁdent about using model outputs
directly for decision making e.g. to decide how many emergency room staff members qualiﬁed for
performing a given procedure should be present tomorrow given all current ER patients. But if the
distribution of event times in these patients differs from validation data, because say the population
has different demographics, calibration should not provide the practitioner with more conﬁdence to
directly use such model outputs.

",222,X-CAL: Explicit Calibration for Survival Analysis,https://papers.nips.cc/paper/2020/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf,108.0
225,"Off-policy interval evaluation not only can advise end-user to deploy new policy,
but can also serve as an intermediate step for latter policy optimization. Our proposed methods also
fill in the gap of theoretical understanding of Markov structure in Lipschitz regression. We current
work stands as a contribution to the fundamental ML methodology, and we do not foresee potential
negative impacts.","Ziyang Tang, Yihao Feng, Na Zhang, Jian Peng, Qiang Liu",Off-Policy Interval Estimation with Lipschitz Value Iteration,1.0,"{'University of Illinois at Urbana-Champaign', 'Tsinghua University', 'UT Austin'}",False,Reinforcement learning and planning,"{'USA', 'China'}",True,False,"Off-policy interval evaluation not only can advise end-user to deploy new policy, but can also serve as an intermediate step for latter policy optimization. Our proposed methods also fill in the gap of theoretical understanding of Markov structure in Lipschitz regression. We current work stands as a contribution to the fundamental ML methodology, and we do not foresee potential negative impacts.",Broader Impact,0.0,True,0.0,False,"Ziyang Tang, Yihao Feng, Na Zhang, Jian Peng, Qiang Liu",59accb9fe696ce55e28b7d23a009e2d1,https://proceedings.neurips.cc/paper/2020/file/59accb9fe696ce55e28b7d23a009e2d1-Paper.pdf,Off-Policy Interval Estimation with Lipschitz Value Iteration,Off-Policy Interval Estimation with Lipschitz Value Iteration,Reinforcement Learning and Planning,"Applications -> Health; Reinforcement Learning and Planning -> Decision and Control; Reinforcement Learning and Planning -> Reinforcement Learning; Social Aspects of Machine Learning -> Fairness, Accountability, and Transparency",3.0,"Off-policy interval evaluation not only can advise end-user to deploy new policy,
but can also serve as an intermediate step for latter policy optimization. Our proposed methods also
fill in the gap of theoretical understanding of Markov structure in Lipschitz regression. We current
work stands as a contribution to the fundamental ML methodology, and we do not foresee potential
negative impacts.",225,Off-Policy Interval Estimation with Lipschitz Value Iteration,https://papers.nips.cc/paper/2020/file/59accb9fe696ce55e28b7d23a009e2d1-Paper.pdf,61.0
226,"
This paper provides an unexpected behavior of the `1 norm in learning sparse graphs, which may
greatly benefit the community of signal processing and machine learning over graphs. This paper
further provides a solution to solve the issue with theoretical guarantees.","Jiaxi Ying, José Vinícius de Miranda Cardoso , Daniel Palomar",Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model,1.0,"{'HKUST', 'The Hong Kong University of Science and Technology'}",False,Optimization Methods (continuous or discrete),"{'Chile', 'China'}",False,False,"This paper provides an unexpected behavior of the 1 norm in learning sparse graphs, which may greatly benefit the community of signal processing and machine learning over graphs. This paper further provides a solution to solve the issue with theoretical guarantees.",Broader Impact,0.0,False,0.0,False,"Jiaxi Ying, José Vinícius de Miranda Cardoso , Daniel Palomar",4ef42b32bccc9485b10b8183507e5d82,https://proceedings.neurips.cc/paper/2020/file/4ef42b32bccc9485b10b8183507e5d82-Paper.pdf,Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model,Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model,Optimization,Algorithms -> Sparsity and Compressed Sensing; Algorithms -> Unsupervised Learning; Applications -> Signal Processing; Probabilistic Methods -> Graphical Models,2.0,"
This paper provides an unexpected behavior of the `1 norm in learning sparse graphs, which may
greatly benefit the community of signal processing and machine learning over graphs. This paper
further provides a solution to solve the issue with theoretical guarantees.",226,Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model,https://papers.nips.cc/paper/2020/file/4ef42b32bccc9485b10b8183507e5d82-Paper.pdf,41.0
228,"
Bayesian approaches to deep learning problems are often proposed in situations where uncertainty
estimation is critical. Often the justiﬁcation given for this approach is the probabilistic framework
of Bayesian inference. However, in cases where approximations are made, the quality of these
approximations should also be taken into account. Our work illustrates that the uncertainty estimates
given by approximate inference with commonly used algorithms may not qualitatively resemble the
uncertainty estimates implied by Bayesian modelling assumptions. This may possibly have adverse
consequences if Bayesian neural networks are used in safety-critical applications. Our work motivates
a careful consideration of these situations.

","Andrew Foong, David Burt, Yingzhen Li, Richard Turner",On the Expressiveness of Approximate Inference in Bayesian Neural Networks,1.0,"{'University of Cambridge', 'Microsoft Research Cambridge'}",,Probabilistic methods and inference,"{'UK', 'USA'}",False,False,"Bayesian approaches to deep learning problems are often proposed in situations where uncertainty estimation is critical. Often the justification given for this approach is the probabilistic framework of Bayesian inference. However, in cases where approximations are made, the quality of these approximations should also be taken into account. Our work illustrates that the uncertainty estimates given by approximate inference with commonly used algorithms may not qualitatively resemble the uncertainty estimates implied by Bayesian modelling assumptions. This may possibly have adverse consequences if Bayesian neural networks are used in safety-critical applications. Our work motivates a careful consideration of these situations.",Broader Impact,1.0,False,1.0,,"Andrew Foong, David Burt, Yingzhen Li, Richard Turner",b6dfd41875bc090bd31d0b1740eb5b1b,https://proceedings.neurips.cc/paper/2020/file/b6dfd41875bc090bd31d0b1740eb5b1b-Paper.pdf,On the Expressiveness of Approximate Inference in Bayesian Neural Networks,On the Expressiveness of Approximate Inference in Bayesian Neural Networks,Probabilistic Methods -> Variational Inference,Algorithms -> Uncertainty Estimation,6.0,"
Bayesian approaches to deep learning problems are often proposed in situations where uncertainty
estimation is critical. Often the justiﬁcation given for this approach is the probabilistic framework
of Bayesian inference. However, in cases where approximations are made, the quality of these
approximations should also be taken into account. Our work illustrates that the uncertainty estimates
given by approximate inference with commonly used algorithms may not qualitatively resemble the
uncertainty estimates implied by Bayesian modelling assumptions. This may possibly have adverse
consequences if Bayesian neural networks are used in safety-critical applications. Our work motivates
a careful consideration of these situations.

",228,On the Expressiveness of Approximate Inference in Bayesian Neural Networks,https://papers.nips.cc/paper/2020/file/b6dfd41875bc090bd31d0b1740eb5b1b-Paper.pdf,99.0
229,"
Langevin algorithms are core Markov Chain Monte Carlo (MCMC) methods for solving machine
learning problems. These methods arise in several contexts in machine learning and data science.
For example, they can be applied to Bayesian inference problems. They can also be used to solve
stochastic non-convex optimization problems including the challenging problems arising in deep
learning. Our paper argues that the non-reversible variants of the classical Langevin algorithms can
perform better by providing rigorous mathematical analysis, and bridges a gap between theory and
practice. Therefore, our paper contributes to the growing literature on theoretical foundations of
MCMC methods. Researchers in the machine learning community and beyond will beneﬁt from this
research by having a better understanding of why non-reversible variants of the classical Langevin
algorithms can improve performance.

","Xuefeng GAO, Mert Gurbuzbalaban, Lingjiong  Zhu",Breaking Reversibility Accelerates Langevin Dynamics for Non-Convex Optimization,1.0,"{'The Chinese University of Hong Kong', 'Florida State University', 'Rutgers'}",,Optimization Methods (continuous or discrete),"{'USA', 'China'}",False,False,"Langevin algorithms are core Markov Chain Monte Carlo (MCMC) methods for solving machine learning problems. These methods arise in several contexts in machine learning and data science. For example, they can be applied to Bayesian inference problems. They can also be used to solve stochastic non-convex optimization problems including the challenging problems arising in deep learning. Our paper argues that the non-reversible variants of the classical Langevin algorithms can perform better by providing rigorous mathematical analysis, and bridges a gap between theory and practice. Therefore, our paper contributes to the growing literature on theoretical foundations of MCMC methods. Researchers in the machine learning community and beyond will benefit from this research by having a better understanding of why non-reversible variants of the classical Langevin algorithms can improve performance.",Broader Impact,0.0,False,0.0,,"Xuefeng GAO, Mert Gurbuzbalaban, Lingjiong  Zhu",cebd648f9146a6345d604ab093b02c73,https://proceedings.neurips.cc/paper/2020/file/cebd648f9146a6345d604ab093b02c73-Paper.pdf,Breaking Reversibility Accelerates Langevin Dynamics for Non-Convex Optimization,Breaking Reversibility Accelerates Langevin Dynamics for Non-Convex Optimization,Optimization,Optimization -> Non-Convex Optimization,7.0,"
Langevin algorithms are core Markov Chain Monte Carlo (MCMC) methods for solving machine
learning problems. These methods arise in several contexts in machine learning and data science.
For example, they can be applied to Bayesian inference problems. They can also be used to solve
stochastic non-convex optimization problems including the challenging problems arising in deep
learning. Our paper argues that the non-reversible variants of the classical Langevin algorithms can
perform better by providing rigorous mathematical analysis, and bridges a gap between theory and
practice. Therefore, our paper contributes to the growing literature on theoretical foundations of
MCMC methods. Researchers in the machine learning community and beyond will beneﬁt from this
research by having a better understanding of why non-reversible variants of the classical Langevin
algorithms can improve performance.

",229,Breaking Reversibility Accelerates Langevin Dynamics for Non-Convex Optimization,https://papers.nips.cc/paper/2020/file/cebd648f9146a6345d604ab093b02c73-Paper.pdf,128.0
231,"This paper is a theoretical study that brings together two seemingly disjoint but equally impact-
ful ﬁelds of sparse recovery and mixture models: the ﬁrst having numerous applications in signal
processing while the second being the main statistical model for clustering. Given that, this work
belongs to the foundational area of data science and enhances our understanding of some basic the-
oretical questions. We feel the methodology developed in this paper is instructive, and exempliﬁes
the use of several combinatorial objects and techniques in signal recovery and classiﬁcation, that are
hitherto underused. Therefore we foresee the technical content of this paper to form good teach-
ing material in foundational data science and signal processing courses. The content of this paper
can raise interest of students or young researchers in discrete mathematics to applications areas and
problems of signal processing and machine learning.

While primarily of theoretical interest, the results of the paper can be immediately applicable to
some real-life scenarios and be useful in recommendation systems, one of the major drivers of data
science research. In particular, if in any case of feedback/rating from users of a service there is
ambiguity about the source of the feedback, our framework can be used. This is also applicable to
crowdsourcing applications.
","Venkata Gandikota, Arya Mazumdar, Soumyabrata Pal",Recovery of sparse linear classifiers from mixture of responses,1.0,"{'University of Massachusetts, Amherst', 'University of Massachusetts Amherst'}",,Theory (including computational and statistical analyses),{'USA'},True,False,"This paper is a theoretical study that brings together two seemingly disjoint but equally impact- ful fields of sparse recovery and mixture models: the first having numerous applications in signal processing while the second being the main statistical model for clustering. Given that, this work belongs to the foundational area of data science and enhances our understanding of some basic the- oretical questions. We feel the methodology developed in this paper is instructive, and exemplifies the use of several combinatorial objects and techniques in signal recovery and classification, that are hitherto underused. Therefore we foresee the technical content of this paper to form good teach- ing material in foundational data science and signal processing courses. The content of this paper can raise interest of students or young researchers in discrete mathematics to applications areas and problems of signal processing and machine learning. While primarily of theoretical interest, the results of the paper can be immediately applicable to some real-life scenarios and be useful in recommendation systems, one of the major drivers of data science research. In particular, if in any case of feedback/rating from users of a service there is ambiguity about the source of the feedback, our framework can be used. This is also applicable to crowdsourcing applications.",Broader Impact,0.0,False,0.0,,"Venkata Gandikota, Arya Mazumdar, Soumyabrata Pal",a89b71bb5227c75d463dd82a03115738,https://proceedings.neurips.cc/paper/2020/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf,Recovery of sparse linear classifiers from mixture of responses,Recovery of sparse linear classifiers from mixture of responses,Algorithms -> Sparsity and Compressed Sensing,Algorithms -> Active Learning; Algorithms -> Classification; Theory -> Information Theory,8.0,"This paper is a theoretical study that brings together two seemingly disjoint but equally impact-
ful ﬁelds of sparse recovery and mixture models: the ﬁrst having numerous applications in signal
processing while the second being the main statistical model for clustering. Given that, this work
belongs to the foundational area of data science and enhances our understanding of some basic the-
oretical questions. We feel the methodology developed in this paper is instructive, and exempliﬁes
the use of several combinatorial objects and techniques in signal recovery and classiﬁcation, that are
hitherto underused. Therefore we foresee the technical content of this paper to form good teach-
ing material in foundational data science and signal processing courses. The content of this paper
can raise interest of students or young researchers in discrete mathematics to applications areas and
problems of signal processing and machine learning.

While primarily of theoretical interest, the results of the paper can be immediately applicable to
some real-life scenarios and be useful in recommendation systems, one of the major drivers of data
science research. In particular, if in any case of feedback/rating from users of a service there is
ambiguity about the source of the feedback, our framework can be used. This is also applicable to
crowdsourcing applications.
",231,Recovery of sparse linear classifiers from mixture of responses,https://papers.nips.cc/paper/2020/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf,209.0
236,"Causality is an important concern in medicine, biology, econometrics and science in general (Pearl,
2009; Spirtes et al., 2000; Peters et al., 2017). A causal understanding of the world is required
to correctly predict the effect of actions or external factors on a system, but also to develop fair
algorithms. It is well-known that learning causal relations from observational data alone is not possible
in general (except in special cases or under very strong assumptions); in these cases experimental
(“interventional”) data is necessary to resolve ambiguities.
In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized
controlled trials to develop a new drug or gene knockout experiments. These settings crucially
rely on experiment design, or more precisely intervention design, i.e. finding a cost-optimal set
of interventions that can fully identify a causal model. The ultimate goal of intervention design is
accelerating scientific discovery by decreasing its costs, both in terms of actual costs of performing
the experiments and in terms of automation of new discoveries.
Our work focuses on intervention design for learning causal DAGs, which have been notably
employed as models in system biology, e.g. for gene regulatory networks (Friedman et al., 2000) or
for protein signalling networks (Sachs et al., 2005). Protein signalling networks represent the way
cells communicate with each other, and having reliable models of cell signalling is crucial to develop
new treatments for many diseases, including cancer. Understanding how genes influence each other
has also important healthcare applications, but is also crucial in other fields, e.g. agriculture or the
food industry. Since even the genome of a simple organism as the common yeast contains 6275
genes, interventions like gene knockouts have to be carefully planned. Moreover, experimental design
algorithms may prove to be a useful tool for driving down the time and cost of investigating the
impact of cell type, drug exposure, and other factors on gene expression. These benefits suggest that
there is a potential for experimental design algorithms such as ours to be a commonplace component
of the future biological workflow.
In particular, our work establishes a number of new theoretical tools and results that 1) may drive
development of new experimental design algorithms, 2) allow practitioners to estimate, prior to
beginning experimentation, how costly their task may be, 3) offer an intervention policy that is able
to run on much larger graphs than most of the related work, and provides more efficient intervention
schedules than the rest.
Importantly, our work and in general intervention design algorithms have some limitations. In
particular, as we have mentioned in the main paper, all these algorithms have relatively strong
assumptions (e.g. no latent confounders or selection bias, infinite observational data, noiseless
interventions, or in some case limitations on the graph structure (Greenewald et al., 2019)). If
these assumptions are not satisfied in the data, or the practitioner does not realize their importance,
the outcome of these algorithms could be misinterpreted or over-interpreted, leading to wasteful
experiments or overconfident causal conclusions. Wrong causal conclusions may lead to potentially
severe unintended side effects or unintended perpetuation of bias in algorithms.
Even in case of correct causal conclusions, the actualized impact of experimental design depends on
the experiments in which it is used. Potential positive uses cases include decreasing the cost of drug
development, in turn leading to better and cheaper medicine for consumers.","Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, Karthikeyan Shanmugam",Active Structure Learning of Causal DAGs via Directed Clique Trees,1.0,"{'Massachusetts Institute of Technology', 'IBM Research, NY', 'IBM Research', 'MIT-IBM Watson AI Lab'}",,Causality,{'USA'},False,False,"Causality is an important concern in medicine, biology, econometrics and science in general (Pearl, 2009; Spirtes et al., 2000; Peters et al., 2017). A causal understanding of the world is required to correctly predict the effect of actions or external factors on a system, but also to develop fair algorithms. It is well-known that learning causal relations from observational data alone is not possible in general (except in special cases or under very strong assumptions); in these cases experimental (“interventional”) data is necessary to resolve ambiguities. In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized controlled trials to develop a new drug or gene knockout experiments. These settings crucially rely on experiment design , or more precisely intervention design , i.e. finding a cost-optimal set of interventions that can fully identify a causal model. The ultimate goal of intervention design is accelerating scientific discovery by decreasing its costs, both in terms of actual costs of performing the experiments and in terms of automation of new discoveries. Our work focuses on intervention design for learning causal DAGs, which have been notably employed as models in system biology, e.g. for gene regulatory networks (Friedman et al., 2000) or for protein signalling networks (Sachs et al., 2005). Protein signalling networks represent the way cells communicate with each other, and having reliable models of cell signalling is crucial to develop new treatments for many diseases, including cancer. Understanding how genes influence each other has also important healthcare applications, but is also crucial in other fields, e.g. agriculture or the food industry. Since even the genome of a simple organism as the common yeast contains 6275 genes, interventions like gene knockouts have to be carefully planned. Moreover, experimental design algorithms may prove to be a useful tool for driving down the time and cost of investigating the impact of cell type, drug exposure, and other factors on gene expression. These benefits suggest that there is a potential for experimental design algorithms such as ours to be a commonplace component of the future biological workflow. In particular, our work establishes a number of new theoretical tools and results that 1) may drive development of new experimental design algorithms, 2) allow practitioners to estimate, prior to beginning experimentation, how costly their task may be, 3) offer an intervention policy that is able to run on much larger graphs than most of the related work, and provides more efficient intervention schedules than the rest. Importantly, our work and in general intervention design algorithms have some limitations. In particular, as we have mentioned in the main paper, all these algorithms have relatively strong assumptions (e.g. no latent confounders or selection bias, infinite observational data, noiseless interventions, or in some case limitations on the graph structure (Greenewald et al., 2019)). If these assumptions are not satisfied in the data, or the practitioner does not realize their importance, the outcome of these algorithms could be misinterpreted or over-interpreted, leading to wasteful experiments or overconfident causal conclusions. Wrong causal conclusions may lead to potentially severe unintended side effects or unintended perpetuation of bias in algorithms. Even in case of correct causal conclusions, the actualized impact of experimental design depends on the experiments in which it is used. Potential positive uses cases include decreasing the cost of drug development, in turn leading to better and cheaper medicine for consumers.",Broader impact statement,1.0,False,1.0,,"Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, Karthikeyan Shanmugam",f57bd0a58e953e5c43cd4a4e5af46138,https://proceedings.neurips.cc/paper/2020/file/f57bd0a58e953e5c43cd4a4e5af46138-Paper.pdf,Active Structure Learning of Causal DAGs via Directed Clique Trees,Active Structure Learning of Causal DAGs via Directed Clique Trees,Probabilistic Methods -> Causal Inference,Algorithms -> Model Selection and Structure Learning; Probabilistic Methods -> Graphical Models,19.0,"Causality is an important concern in medicine, biology, econometrics and science in general (Pearl,
2009; Spirtes et al., 2000; Peters et al., 2017). A causal understanding of the world is required
to correctly predict the effect of actions or external factors on a system, but also to develop fair
algorithms. It is well-known that learning causal relations from observational data alone is not possible
in general (except in special cases or under very strong assumptions); in these cases experimental
(“interventional”) data is necessary to resolve ambiguities.
In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized
controlled trials to develop a new drug or gene knockout experiments. These settings crucially
rely on experiment design, or more precisely intervention design, i.e. finding a cost-optimal set
of interventions that can fully identify a causal model. The ultimate goal of intervention design is
accelerating scientific discovery by decreasing its costs, both in terms of actual costs of performing
the experiments and in terms of automation of new discoveries.
Our work focuses on intervention design for learning causal DAGs, which have been notably
employed as models in system biology, e.g. for gene regulatory networks (Friedman et al., 2000) or
for protein signalling networks (Sachs et al., 2005). Protein signalling networks represent the way
cells communicate with each other, and having reliable models of cell signalling is crucial to develop
new treatments for many diseases, including cancer. Understanding how genes influence each other
has also important healthcare applications, but is also crucial in other fields, e.g. agriculture or the
food industry. Since even the genome of a simple organism as the common yeast contains 6275
genes, interventions like gene knockouts have to be carefully planned. Moreover, experimental design
algorithms may prove to be a useful tool for driving down the time and cost of investigating the
impact of cell type, drug exposure, and other factors on gene expression. These benefits suggest that
there is a potential for experimental design algorithms such as ours to be a commonplace component
of the future biological workflow.
In particular, our work establishes a number of new theoretical tools and results that 1) may drive
development of new experimental design algorithms, 2) allow practitioners to estimate, prior to
beginning experimentation, how costly their task may be, 3) offer an intervention policy that is able
to run on much larger graphs than most of the related work, and provides more efficient intervention
schedules than the rest.
Importantly, our work and in general intervention design algorithms have some limitations. In
particular, as we have mentioned in the main paper, all these algorithms have relatively strong
assumptions (e.g. no latent confounders or selection bias, infinite observational data, noiseless
interventions, or in some case limitations on the graph structure (Greenewald et al., 2019)). If
these assumptions are not satisfied in the data, or the practitioner does not realize their importance,
the outcome of these algorithms could be misinterpreted or over-interpreted, leading to wasteful
experiments or overconfident causal conclusions. Wrong causal conclusions may lead to potentially
severe unintended side effects or unintended perpetuation of bias in algorithms.
Even in case of correct causal conclusions, the actualized impact of experimental design depends on
the experiments in which it is used. Potential positive uses cases include decreasing the cost of drug
development, in turn leading to better and cheaper medicine for consumers.",236,Active Structure Learning of Causal DAGs via Directed Clique Trees,https://papers.nips.cc/paper/2020/file/f57bd0a58e953e5c43cd4a4e5af46138-Paper.pdf,559.0
239,"
Explainable AI (XAI) has gained a lot of traction in industry and government given the proliferation
of black-box models such as neural networks. The General Data Protection Regulation (GDPR)
[30] passed in Europe requires automated systems making decisions that affect humans to be able
to explain themselves. There are mainly two types of explainability:
local and global. Local
explainability is about per sample explanations, while global explainability is about understanding
the whole model. Although signiﬁcant amount of work has been done for each type, there is little
work that tries to combine these two paradigms in an attempt to create global models that are also
locally consistent.

Our work tries to bridge this gap for contrastive/counterfactual explanations which have been deemed
as being one of the most important parts of an explanation [18]. The beneﬁt of our method is thus that
one can create globally transparent models that are locally consistent more than other schemes. This
can be beneﬁcial in appropriating trust in the black-box model with more conﬁdence. The risks are
similar to other methods that build global models (viz. distillation, profweight) that such models are
still proxy models and hence, may not entirely replicate the reasoning done by the black-box model
they are trying to explain.

9

","Tejaswini Pedapati, Avinash  Balakrishnan, Karthikeyan Shanmugam, Amit Dhurandhar",Learning Global Transparent Models consistent with Local Contrastive Explanations,,,,,,,,,,,,,,,,,,,,,,"
Explainable AI (XAI) has gained a lot of traction in industry and government given the proliferation
of black-box models such as neural networks. The General Data Protection Regulation (GDPR)
[30] passed in Europe requires automated systems making decisions that affect humans to be able
to explain themselves. There are mainly two types of explainability:
local and global. Local
explainability is about per sample explanations, while global explainability is about understanding
the whole model. Although signiﬁcant amount of work has been done for each type, there is little
work that tries to combine these two paradigms in an attempt to create global models that are also
locally consistent.

Our work tries to bridge this gap for contrastive/counterfactual explanations which have been deemed
as being one of the most important parts of an explanation [18]. The beneﬁt of our method is thus that
one can create globally transparent models that are locally consistent more than other schemes. This
can be beneﬁcial in appropriating trust in the black-box model with more conﬁdence. The risks are
similar to other methods that build global models (viz. distillation, profweight) that such models are
still proxy models and hence, may not entirely replicate the reasoning done by the black-box model
they are trying to explain.

9

",239,,https://papers.nips.cc/paper/2020/file/24aef8cb3281a2422a59b51659f1ad2e-Paper.pdf,
242,"Researchers working to understand temporal phenomena may consider the problems raised in this
work, and may find the proposed methods useful for their analysis. More importantly, this work is a
stepping stone towards building better models for language processing in the brain that can not only
help investigate cortical language processing but also simulate brain responses. This could be useful
for diagnosing, treating, and assisting people with language deficits like aphasia, especially since processing information at different timescales is critical to human language. On the contrary, these
tools may also serve as a stepping stone toward unethical brain decoding practices that could be used
by, for example, insurance companies or attorneys for erroneous evidence collection on a trial. In
general, advances in brain-reading technology may raise issues in neuroethics, especially regarding
mental privacy.
Negative consequences from this research may affect the participants themselves. The fMRI data
for this work was acquired in accordance with IRB protocols, which included informed consent
of the risks involved with MRI. In addition to physical risks, such as peripheral nerve stimulation,
participants were informed about the steps taken to protect their data. While personal identifying
information about participants is stored in a physical, locked, separate location from the neuroimaging
data, a failure in this system could potentially lead to a breach of confidentiality.
As with much of the research submitted to NeurIPS, training neural network models consumes large
amounts of energy. If this energy was generated by non-renewable fuel, this would have a negative
impact on the environment.
","Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S. Turek, Alexander Huth",Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech,1.0,"{'Intel Corporation', 'The University of Texas at Austin', 'Intel Labs'}",,Neuroscience and cognitive science,{'USA'},False,False,"Researchers working to understand temporal phenomena may consider the problems raised in this work, and may find the proposed methods useful for their analysis. More importantly, this work is a stepping stone towards building better models for language processing in the brain that can not only help investigate cortical language processing but also simulate brain responses. This could be useful for diagnosing, treating, and assisting people with language deficits like aphasia, especially since  processing information at different timescales is critical to human language. On the contrary, these tools may also serve as a stepping stone toward unethical brain decoding practices that could be used by, for example, insurance companies or attorneys for erroneous evidence collection on a trial. In general, advances in brain-reading technology may raise issues in neuroethics, especially regarding mental privacy. Negative consequences from this research may affect the participants themselves. The fMRI data for this work was acquired in accordance with IRB protocols, which included informed consent of the risks involved with MRI. In addition to physical risks, such as peripheral nerve stimulation, participants were informed about the steps taken to protect their data. While personal identifying information about participants is stored in a physical, locked, separate location from the neuroimaging data, a failure in this system could potentially lead to a breach of confidentiality. As with much of the research submitted to NeurIPS, training neural network models consumes large amounts of energy. If this energy was generated by non-renewable fuel, this would have a negative impact on the environment.",Broader Impact,1.0,False,1.0,,"Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S. Turek, Alexander Huth",9e9a30b74c49d07d8150c8c83b1ccf07,https://proceedings.neurips.cc/paper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf,Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech,Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech,Neuroscience and Cognitive Science -> Brain Mapping,Applications -> Natural Language Processing; Neuroscience and Cognitive Science -> Brain Imaging; Neuroscience and Cognitive Science -> Neuroscience,11.0,"Researchers working to understand temporal phenomena may consider the problems raised in this
work, and may find the proposed methods useful for their analysis. More importantly, this work is a
stepping stone towards building better models for language processing in the brain that can not only
help investigate cortical language processing but also simulate brain responses. This could be useful
for diagnosing, treating, and assisting people with language deficits like aphasia, especially since processing information at different timescales is critical to human language. On the contrary, these
tools may also serve as a stepping stone toward unethical brain decoding practices that could be used
by, for example, insurance companies or attorneys for erroneous evidence collection on a trial. In
general, advances in brain-reading technology may raise issues in neuroethics, especially regarding
mental privacy.
Negative consequences from this research may affect the participants themselves. The fMRI data
for this work was acquired in accordance with IRB protocols, which included informed consent
of the risks involved with MRI. In addition to physical risks, such as peripheral nerve stimulation,
participants were informed about the steps taken to protect their data. While personal identifying
information about participants is stored in a physical, locked, separate location from the neuroimaging
data, a failure in this system could potentially lead to a breach of confidentiality.
As with much of the research submitted to NeurIPS, training neural network models consumes large
amounts of energy. If this energy was generated by non-renewable fuel, this would have a negative
impact on the environment.
",242,Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech,https://papers.nips.cc/paper/2020/file/9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf,253.0
245,"
Predictive models are increasingly being investigated, sometimes legally regulated for deployment
in critical settings. Interpretability methods promise to provide insights about how models make
decisions. This may increase user trust and provide the evidence needed to ensure that models
deployed in mission-critical settings function adequately. The goal of our work is to investigate this
literature with a critical eye: can attribution methods signal that there may be issues with the model,
data or at test-time setting? We provide both quantitative and qualitative approaches to evaluate many
popular attribution methods in order to provide practitioners and researchers with a set of debugging
tests which may be used in validation. We hope our work is one of the ﬁrst of many to bridge the gap
between methods developed in academia and practical usage of those methods in the real world.

","Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim",Debugging Tests for Model Explanations,1.0,"{'Google', 'Stanford University', 'MIT'}",,,{'USA'},False,False,"Predictive models are increasingly being investigated, sometimes legally regulated for deployment in critical settings. Interpretability methods promise to provide insights about how models make decisions. This may increase user trust and provide the evidence needed to ensure that models deployed in mission-critical settings function adequately. The goal of our work is to investigate this literature with a critical eye: can attribution methods signal that there may be issues with the model, data or at test-time setting? We provide both quantitative and qualitative approaches to evaluate many popular attribution methods in order to provide practitioners and researchers with a set of debugging tests which may be used in validation. We hope our work is one of the first of many to bridge the gap between methods developed in academia and practical usage of those methods in the real world.",Broader Impact,1.0,False,1.0,,"Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim",075b051ec3d22dac7b33f788da631fd4,https://proceedings.neurips.cc/paper/2020/file/075b051ec3d22dac7b33f788da631fd4-Paper.pdf,Debugging Tests for Model Explanations,Debugging Tests for Model Explanations,"Deep Learning -> Visualization, Interpretability, and Explainability","Social Aspects of Machine Learning -> Fairness, Accountability, and Transparency",6.0,"
Predictive models are increasingly being investigated, sometimes legally regulated for deployment
in critical settings. Interpretability methods promise to provide insights about how models make
decisions. This may increase user trust and provide the evidence needed to ensure that models
deployed in mission-critical settings function adequately. The goal of our work is to investigate this
literature with a critical eye: can attribution methods signal that there may be issues with the model,
data or at test-time setting? We provide both quantitative and qualitative approaches to evaluate many
popular attribution methods in order to provide practitioners and researchers with a set of debugging
tests which may be used in validation. We hope our work is one of the ﬁrst of many to bridge the gap
between methods developed in academia and practical usage of those methods in the real world.

",245,Debugging Tests for Model Explanations,https://papers.nips.cc/paper/2020/file/075b051ec3d22dac7b33f788da631fd4-Paper.pdf,138.0
253,"
This work presents theoretical analysis on the convergence rates of random forests in the machine
learning community. This is a pure theoretical work without particular application foreseen.

","Wei Gao, Zhi-Hua Zhou",Towards Convergence Rate Analysis of Random Forests for Classification,1.0,{'Nanjing University'},False,Theory (including computational and statistical analyses),{'China'},False,False,This work presents theoretical analysis on the convergence rates of random forests in the machine learning community. This is a pure theoretical work without particular application foreseen.,Broader Impact,0.0,False,0.0,True,"Wei Gao, Zhi-Hua Zhou",6925f2a16026e36e4fc112f82dd79406,https://proceedings.neurips.cc/paper/2020/file/6925f2a16026e36e4fc112f82dd79406-Paper.pdf,Towards Convergence Rate Analysis of Random Forests for Classification,Towards Convergence Rate Analysis of Random Forests for Classification,Theory -> Statistical Learning Theory,Algorithms -> Boosting and Ensemble Methods,2.0,"
This work presents theoretical analysis on the convergence rates of random forests in the machine
learning community. This is a pure theoretical work without particular application foreseen.

",253,Towards Convergence Rate Analysis of Random Forests for Classification,https://papers.nips.cc/paper/2020/file/6925f2a16026e36e4fc112f82dd79406-Paper.pdf,27.0
268,"Since the nature of our work is mostly theoretical with no immediate practical applications, we do
not anticipate any direct societal impact. However, on the long-term, our work can have an impact on
related research communities such as neuroscience or deep learning, which can have both positive
and negative societal impact, depending on how these fields develop. For example, we show that
the TP framework, when using a new reconstruction loss, is a viable credit assignment method
for feedforward networks that fundamentally differs from the standard training method known as
BP. Furthermore, TP only uses information which is local to each neuron and mitigates the weight
transport and signed error transmission problem, the two major criticisms of BP. This renders TP
appealing for neuroscientists that investigate how credit assignment is organized in the brain (Lillicrap
et al., 2020; Richards et al., 2019) and how neural circuits (dys)function in health and disease. From
a machine learning perspective, the TP framework has inspired new training methods for recurrent
neural networks (RNNs) (Manchev and Spratling, 2020; Ororbia et al., 2020; DePasquale et al., 2018;
Abbott et al., 2016), which is beneficial because the conventional backpropagation-through-time
method (Werbos, 1988; Robinson and Fallside, 1987; Mozer, 1995) for training RNNs still suffers
from significant drawbacks, such as vanishing and exploding gradients (Hochreiter, 1991; Hochreiter
and Schmidhuber, 1997). Here, our work provides a new angle for the field to investigate the
theoretical underpinnings of credit assignment in RNNs based on TP.","Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, Benjamin F. Grewe",A Theoretical Framework for Target Propagation,1.0,"{'ETH Zurich', 'Institute of Neuroinformatics, University of Zurich and ETH Zurich', 'KU Leuven'}",,Neuroscience and cognitive science,"{'Belgium', 'Switzerland'}",False,False,"Since the nature of our work is mostly theoretical with no immediate practical applications, we do not anticipate any direct societal impact. However, on the long-term, our work can have an impact on related research communities such as neuroscience or deep learning, which can have both positive and negative societal impact, depending on how these fields develop. For example, we show that the TP framework, when using a new reconstruction loss, is a viable credit assignment method for feedforward networks that fundamentally differs from the standard training method known as BP. Furthermore, TP only uses information which is local to each neuron and mitigates the weight transport and signed error transmission problem, the two major criticisms of BP. This renders TP appealing for neuroscientists that investigate how credit assignment is organized in the brain (Lillicrap et al., 2020; Richards et al., 2019) and how neural circuits (dys)function in health and disease. From a machine learning perspective, the TP framework has inspired new training methods for recurrent neural networks (RNNs) (Manchev and Spratling, 2020; Ororbia et al., 2020; DePasquale et al., 2018; Abbott et al., 2016), which is beneficial because the conventional backpropagation-through-time method (Werbos, 1988; Robinson and Fallside, 1987; Mozer, 1995) for training RNNs still suffers from significant drawbacks, such as vanishing and exploding gradients (Hochreiter, 1991; Hochreiter and Schmidhuber, 1997). Here, our work provides a new angle for the field to investigate the theoretical underpinnings of credit assignment in RNNs based on TP.",Broader impact,0.0,False,0.0,,"Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, Benjamin F. Grewe",e7a425c6ece20cbc9056f98699b53c6f,https://proceedings.neurips.cc/paper/2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf,A Theoretical Framework for Target Propagation,A Theoretical Framework for Target Propagation,Deep Learning -> Biologically Plausible Deep Networks,Deep Learning -> Analysis and Understanding of Deep Networks,7.0,"Since the nature of our work is mostly theoretical with no immediate practical applications, we do
not anticipate any direct societal impact. However, on the long-term, our work can have an impact on
related research communities such as neuroscience or deep learning, which can have both positive
and negative societal impact, depending on how these fields develop. For example, we show that
the TP framework, when using a new reconstruction loss, is a viable credit assignment method
for feedforward networks that fundamentally differs from the standard training method known as
BP. Furthermore, TP only uses information which is local to each neuron and mitigates the weight
transport and signed error transmission problem, the two major criticisms of BP. This renders TP
appealing for neuroscientists that investigate how credit assignment is organized in the brain (Lillicrap
et al., 2020; Richards et al., 2019) and how neural circuits (dys)function in health and disease. From
a machine learning perspective, the TP framework has inspired new training methods for recurrent
neural networks (RNNs) (Manchev and Spratling, 2020; Ororbia et al., 2020; DePasquale et al., 2018;
Abbott et al., 2016), which is beneficial because the conventional backpropagation-through-time
method (Werbos, 1988; Robinson and Fallside, 1987; Mozer, 1995) for training RNNs still suffers
from significant drawbacks, such as vanishing and exploding gradients (Hochreiter, 1991; Hochreiter
and Schmidhuber, 1997). Here, our work provides a new angle for the field to investigate the
theoretical underpinnings of credit assignment in RNNs based on TP.",268,A Theoretical Framework for Target Propagation,https://papers.nips.cc/paper/2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf,244.0
272,"
The main motivation of this work has been to design a simple yet effective Bayesian method for
dealing with the few-shot learning setting. The ability to learn from a reduced amount of data is crucial
if we want to have systems that are able to deal with concrete real-world problems. Applications
include (but are not limited to): classiﬁcation and regression under constrained computational
resources, medical diagnosis from small datasets, biometric identiﬁcation from a handful of images,
etc. Our method is one of the few which is able to provide a measure of uncertainty as a feedback
for the decision maker. However, it is important to wisely choose the data on which the system is
trained, since the low-data regime may be prone to bias more than the standard counterpart. If data is
biased our method is not guaranteed to provide a correct estimation; this could harm the ﬁnal users
and should be carefully taken into account.
","Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O'Boyle, Amos J. Storkey",Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,1.0,{'University of Edinburgh'},,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'UK'},False,False,"The main motivation of this work has been to design a simple yet effective Bayesian method for dealing with the few-shot learning setting. The ability to learn from a reduced amount of data is crucial if we want to have systems that are able to deal with concrete real-world problems. Applications include (but are not limited to): classification and regression under constrained computational resources, medical diagnosis from small datasets, biometric identification from a handful of images, etc. Our method is one of the few which is able to provide a measure of uncertainty as a feedback for the decision maker. However, it is important to wisely choose the data on which the system is trained, since the low-data regime may be prone to bias more than the standard counterpart. If data is biased our method is not guaranteed to provide a correct estimation; this could harm the final users and should be carefully taken into account.",Broader Impact,0.0,False,0.0,,"Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O'Boyle, Amos J. Storkey",b9cfe8b6042cf759dc4c0cccb27a6737,https://proceedings.neurips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf,Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,Algorithms -> Few-Shot Learning,Algorithms -> Meta-Learning; Algorithms -> Regression; Probabilistic Methods -> Bayesian Nonparametrics,6.0,"
The main motivation of this work has been to design a simple yet effective Bayesian method for
dealing with the few-shot learning setting. The ability to learn from a reduced amount of data is crucial
if we want to have systems that are able to deal with concrete real-world problems. Applications
include (but are not limited to): classiﬁcation and regression under constrained computational
resources, medical diagnosis from small datasets, biometric identiﬁcation from a handful of images,
etc. Our method is one of the few which is able to provide a measure of uncertainty as a feedback
for the decision maker. However, it is important to wisely choose the data on which the system is
trained, since the low-data regime may be prone to bias more than the standard counterpart. If data is
biased our method is not guaranteed to provide a correct estimation; this could harm the ﬁnal users
and should be carefully taken into account.
",272,Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,https://papers.nips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf,156.0
279,"
As mentioned in the paper, differential privacy is undergoing an exciting transition from theory
to practice and there is an increasing number of deployment of differential private algorithms and
systems in both the private and public sectors.

The focus of the work is to bring a mature and classical technique in differential privacy — Sparse
Vector Technique — to practice by improving the privacy-utility trade-off, and also to conduct
numerical studies so as to provide recommendations on which method to use in each regime. This
task is strongly tied to the goal of AI/ML for social good and responsible computing.

Notice that in practice, the computation of privacy loss or the calibration of noise to privacy budgets
is extremely important as these seemingly theoretical calculations will affect the number of data
points to collect, and affect the statistical power in sensitive applications such as clinical studies.

Moreover, our application to adaptive data analysis is strongly tied to the reproducibility crisis in
science and the problems of overﬁtting common benchmarks that we are currently experiencing in
the machine learning community.
","Yuqing Zhu, Yu-Xiang Wang",Improving Sparse Vector Technique with Renyi Differential Privacy,1.0,"{'University of California Santa Barbara', 'UC Santa Barbara'}",,"Social aspects of machine learning (e.g., fairness, safety, privacy)",{'USA'},False,False,"As mentioned in the paper, differential privacy is undergoing an exciting transition from theory to practice and there is an increasing number of deployment of differential private algorithms and systems in both the private and public sectors. The focus of the work is to bring a mature and classical technique in differential privacy — Sparse Vector Technique — to practice by improving the privacy-utility trade-off, and also to conduct numerical studies so as to provide recommendations on which method to use in each regime. This task is strongly tied to the goal of AI/ML for social good and responsible computing. Notice that in practice, the computation of privacy loss or the calibration of noise to privacy budgets is extremely important as these seemingly theoretical calculations will affect the number of data points to collect, and affect the statistical power in sensitive applications such as clinical studies. Moreover, our application to adaptive data analysis is strongly tied to the reproducibility crisis in science and the problems of overfitting common benchmarks that we are currently experiencing in the machine learning community.",Broader impacts,0.0,False,0.0,,"Yuqing Zhu, Yu-Xiang Wang",e9bf14a419d77534105016f5ec122d62,https://proceedings.neurips.cc/paper/2020/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf,Improving Sparse Vector Technique with Renyi Differential Privacy,Improving Sparse Vector Technique with Renyi Differential Privacy,"Social Aspects of Machine Learning -> Privacy, Anonymity, and Security",Algorithms -> Adaptive Data Analysis,5.0,"
As mentioned in the paper, differential privacy is undergoing an exciting transition from theory
to practice and there is an increasing number of deployment of differential private algorithms and
systems in both the private and public sectors.

The focus of the work is to bring a mature and classical technique in differential privacy — Sparse
Vector Technique — to practice by improving the privacy-utility trade-off, and also to conduct
numerical studies so as to provide recommendations on which method to use in each regime. This
task is strongly tied to the goal of AI/ML for social good and responsible computing.

Notice that in practice, the computation of privacy loss or the calibration of noise to privacy budgets
is extremely important as these seemingly theoretical calculations will affect the number of data
points to collect, and affect the statistical power in sensitive applications such as clinical studies.

Moreover, our application to adaptive data analysis is strongly tied to the reproducibility crisis in
science and the problems of overﬁtting common benchmarks that we are currently experiencing in
the machine learning community.
",279,Improving Sparse Vector Technique with Renyi Differential Privacy,https://papers.nips.cc/paper/2020/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf,179.0
282,"
Our work ﬁts within a broader agenda of algorithmic high-dimensional robust statistics and aims to
advance the algorithmic foundations of robust learning in the presence of a large fraction of arbitrary
outliers. An important motivation for this line of work is to design provable defenses of machine
learning systems against data poisoning attacks. This goal has become a pressing challenge in many
real-world scenarios, where the data of a machine learning system can be untrusted (including, e.g.,
crowdsourcing).

Since the primary focus of our work is theoretical, we do not expect our results to have immediate
societal impact. Nonetheless, we believe that our algorithm is practical and that our ﬁndings provide
interesting insights that could be useful in the design of practically relevant estimators in highly noisy
environments.","Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard",List-Decodable Mean Estimation via Iterative Multi-Filtering,1.0,"{'UW Madison', 'UCSD'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'USA'},False,False,"Our work fits within a broader agenda of algorithmic high-dimensional robust statistics and aims to advance the algorithmic foundations of robust learning in the presence of a large fraction of arbitrary outliers. An important motivation for this line of work is to design provable defenses of machine learning systems against data poisoning attacks . This goal has become a pressing challenge in many real-world scenarios, where the data of a machine learning system can be untrusted (including, e.g., crowdsourcing). Since the primary focus of our work is theoretical, we do not expect our results to have immediate societal impact. Nonetheless, we believe that our algorithm is practical and that our findings provide interesting insights that could be useful in the design of practically relevant estimators in highly noisy environments.",Broader Impact,0.0,False,0.0,,"Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard",6933b5648c59d618bbb30986c84080fe,https://proceedings.neurips.cc/paper/2020/file/6933b5648c59d618bbb30986c84080fe-Paper.pdf,List-Decodable Mean Estimation via Iterative Multi-Filtering,List-Decodable Mean Estimation via Iterative Multi-Filtering,Theory -> Computational Learning Theory,,5.0,"
Our work ﬁts within a broader agenda of algorithmic high-dimensional robust statistics and aims to
advance the algorithmic foundations of robust learning in the presence of a large fraction of arbitrary
outliers. An important motivation for this line of work is to design provable defenses of machine
learning systems against data poisoning attacks. This goal has become a pressing challenge in many
real-world scenarios, where the data of a machine learning system can be untrusted (including, e.g.,
crowdsourcing).

Since the primary focus of our work is theoretical, we do not expect our results to have immediate
societal impact. Nonetheless, we believe that our algorithm is practical and that our ﬁndings provide
interesting insights that could be useful in the design of practically relevant estimators in highly noisy
environments.",282,List-Decodable Mean Estimation via Iterative Multi-Filtering,https://papers.nips.cc/paper/2020/file/6933b5648c59d618bbb30986c84080fe-Paper.pdf,129.0
287,"
This paper presents a new setting for semi-supervised learning and achieves higher efﬁciency of
making use of annotation. We summarize the potential impact of our work in the following aspects.

• To the research community. The one-bit supervision setting is a new problem to the
community. It raises two new challenges, namely, how to obtain more positive labels and
how to learn from negative labels. We provide a simple baseline, but also notice that much
room is left for improvement. We believe the study on these problems can advance the
research community.

• To training with limited labeled data. It is an urgent requirement to extract knowledge
from unlabeled or weakly-labeled data. Our work provides a new methodology that largely
reduces the burden of annotation. The range of application will be even broadened after
follow-up efforts generalize this framework to other vision tasks.

• To the downstream engineers. We provide a new framework for data annotation that can
ease the downstream engineers to develop AI-based systems, especially for some scenarios
in which collecting training data is difﬁcult and/or expensive. While this may help to develop
AI-based applications, there exist risks that some engineers, with relatively less knowledge
in deep learning, can deliberately use the algorithm, e.g., without considering the form of
one-bit information, which may actually harm the performance of the designed system.
• To the society. There is a long-lasting debate on the impact that AI can bring to the human
society. Our method has the potential to generalize the existing AI algorithms to more
applications, while it also raises a serious concern of privacy, since one-bit annotation
is easily collected from some ‘weak’ behaviors of web users, e.g., if he/she views the
recommended images. Therefore, in general, our work can bring both beneﬁcial and harmful
impacts and it really depends on the motivation of the users.

We also encourage the community to investigate the following problems.

1. Is it possible to generalize one-bit supervision to other forms, e.g., introducing other types

of light-weighted information that helps model training?

2. Are there any other solutions that can achieve higher efﬁciency than the proposed multi-stage

training and negative label suppression methods?

3. How to generalize one-bit or few-bit supervision to other vision scenarios? In particular,
what is a proper form of one-bit supervision in object detection or semantic segmentation?

","hengtong hu, Lingxi Xie, Zewei Du, Richang Hong, Qi Tian",One-bit Supervision for Image Classification,1.0,{'Hefei University of Technology'},False,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'China'},True,False,"This paper presents a new setting for semi-supervised learning and achieves higher efficiency of making use of annotation. We summarize the potential impact of our work in the following aspects. • To the research community. The one-bit supervision setting is a new problem to the community. It raises two new challenges, namely, how to obtain more positive labels and how to learn from negative labels. We provide a simple baseline, but also notice that much room is left for improvement. We believe the study on these problems can advance the research community. • To training with limited labeled data. It is an urgent requirement to extract knowledge from unlabeled or weakly-labeled data. Our work provides a new methodology that largely reduces the burden of annotation. The range of application will be even broadened after follow-up efforts generalize this framework to other vision tasks. • To the downstream engineers. We provide a new framework for data annotation that can ease the downstream engineers to develop AI-based systems, especially for some scenarios in which collecting training data is difficult and/or expensive. While this may help to develop AI-based applications, there exist risks that some engineers, with relatively less knowledge in deep learning, can deliberately use the algorithm, e.g., without considering the form of one-bit information, which may actually harm the performance of the designed system. • To the society. There is a long-lasting debate on the impact that AI can bring to the human society. Our method has the potential to generalize the existing AI algorithms to more applications, while it also raises a serious concern of privacy, since one-bit annotation is easily collected from some ‘weak’ behaviors of web users, e.g., if he/she views the recommended images. Therefore, in general, our work can bring both beneficial and harmful impacts and it really depends on the motivation of the users. We also encourage the community to investigate the following problems. 1. Is it possible to generalize one-bit supervision to other forms, e.g., introducing other types of light-weighted information that helps model training? 2. Are there any other solutions that can achieve higher efficiency than the proposed multi-stage training and negative label suppression methods? 3. How to generalize one-bit or few-bit supervision to other vision scenarios? In particular, what is a proper form of one-bit supervision in object detection or semantic segmentation?",Broader Impact,0.0,True,0.0,False,"Hengtong Hu, Lingxi Xie, Zewei Du, Richang Hong, Qi Tian",05f971b5ec196b8c65b75d2ef8267331,https://proceedings.neurips.cc/paper/2020/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf,One-bit Supervision for Image Classification,One-bit Supervision for Image Classification,Algorithms -> Classification,Algorithms -> Active Learning; Algorithms -> Semi-Supervised Learning; Applications -> Computer Vision,26.0,"
This paper presents a new setting for semi-supervised learning and achieves higher efﬁciency of
making use of annotation. We summarize the potential impact of our work in the following aspects.

• To the research community. The one-bit supervision setting is a new problem to the
community. It raises two new challenges, namely, how to obtain more positive labels and
how to learn from negative labels. We provide a simple baseline, but also notice that much
room is left for improvement. We believe the study on these problems can advance the
research community.

• To training with limited labeled data. It is an urgent requirement to extract knowledge
from unlabeled or weakly-labeled data. Our work provides a new methodology that largely
reduces the burden of annotation. The range of application will be even broadened after
follow-up efforts generalize this framework to other vision tasks.

• To the downstream engineers. We provide a new framework for data annotation that can
ease the downstream engineers to develop AI-based systems, especially for some scenarios
in which collecting training data is difﬁcult and/or expensive. While this may help to develop
AI-based applications, there exist risks that some engineers, with relatively less knowledge
in deep learning, can deliberately use the algorithm, e.g., without considering the form of
one-bit information, which may actually harm the performance of the designed system.
• To the society. There is a long-lasting debate on the impact that AI can bring to the human
society. Our method has the potential to generalize the existing AI algorithms to more
applications, while it also raises a serious concern of privacy, since one-bit annotation
is easily collected from some ‘weak’ behaviors of web users, e.g., if he/she views the
recommended images. Therefore, in general, our work can bring both beneﬁcial and harmful
impacts and it really depends on the motivation of the users.

We also encourage the community to investigate the following problems.

1. Is it possible to generalize one-bit supervision to other forms, e.g., introducing other types

of light-weighted information that helps model training?

2. Are there any other solutions that can achieve higher efﬁciency than the proposed multi-stage

training and negative label suppression methods?

3. How to generalize one-bit or few-bit supervision to other vision scenarios? In particular,
what is a proper form of one-bit supervision in object detection or semantic segmentation?

",287,One-bit Supervision for Image Classification,https://papers.nips.cc/paper/2020/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf,389.0
293,"
This study presents a novel generative causal inference framework, called BV-NICE, that brings
together ideas from both statistical and machine learning based causal modeling. By joining the
strength of variational inference, R-learning, and Fenchel mini-max learning, the resulting procedure
fully acknowledges the representation uncertainty and enables accurate, reliable direct estimation of
individualized causal effect in a ﬂexible, scalable manner. Importantly, while there has been growing
consensus that generative causal modeling such as CE-VAE is more suited for many applications yet
with suboptimal performance, our research identiﬁes the performance bottleneck and closes the gap
between generative causal schemes and state-of-the-art alternatives.

This work promises to have positive societal impacts into the future. And with the best intention in
the world, the author(s) wish this research will be applied to progress the course of humanity for the
good. Areas stand most likely to beneﬁt from this research are personalized healthcare, public policy,
and transportation safety regulations. Variant of the proposed variational framework also promises
robustness against the algorithmic biases towards the minority populations, a major issue that draws
criticism for machine learning applications. This implies our model can be well suited for ensuring
social justice.

","Danni Lu, Chenyang Tao, Junya Chen, Fan Li, Feng Guo, Lawrence Carin",Reconsidering Generative Objectives For Counterfactual Reasoning,1.0,"{'Duke U', 'Virginia Tech', 'Duke University'}",,Causality,{'USA'},True,False,"This study presents a novel generative causal inference framework, called BV-NICE, that brings together ideas from both statistical and machine learning based causal modeling. By joining the strength of variational inference, R-learning, and Fenchel mini-max learning, the resulting procedure fully acknowledges the representation uncertainty and enables accurate, reliable direct estimation of individualized causal effect in a flexible, scalable manner. Importantly, while there has been growing consensus that generative causal modeling such as CE-VAE is more suited for many applications yet with suboptimal performance, our research identifies the performance bottleneck and closes the gap between generative causal schemes and state-of-the-art alternatives. This work promises to have positive societal impacts into the future. And with the best intention in the world, the author(s) wish this research will be applied to progress the course of humanity for the good. Areas stand most likely to benefit from this research are personalized healthcare, public policy, and transportation safety regulations. Variant of the proposed variational framework also promises robustness against the algorithmic biases towards the minority populations, a major issue that draws criticism for machine learning applications. This implies our model can be well suited for ensuring social justice.",Broader Impact,0.0,False,0.0,,"Danni Lu, Chenyang Tao, Junya Chen, Fan Li, Feng Guo, Lawrence Carin",f5cfbc876972bd0d031c8abc37344c28,https://proceedings.neurips.cc/paper/2020/file/f5cfbc876972bd0d031c8abc37344c28-Paper.pdf,Reconsidering Generative Objectives For Counterfactual Reasoning,Reconsidering Generative Objectives For Counterfactual Reasoning,Probabilistic Methods -> Causal Inference,Deep Learning -> Generative Models,8.0,"
This study presents a novel generative causal inference framework, called BV-NICE, that brings
together ideas from both statistical and machine learning based causal modeling. By joining the
strength of variational inference, R-learning, and Fenchel mini-max learning, the resulting procedure
fully acknowledges the representation uncertainty and enables accurate, reliable direct estimation of
individualized causal effect in a ﬂexible, scalable manner. Importantly, while there has been growing
consensus that generative causal modeling such as CE-VAE is more suited for many applications yet
with suboptimal performance, our research identiﬁes the performance bottleneck and closes the gap
between generative causal schemes and state-of-the-art alternatives.

This work promises to have positive societal impacts into the future. And with the best intention in
the world, the author(s) wish this research will be applied to progress the course of humanity for the
good. Areas stand most likely to beneﬁt from this research are personalized healthcare, public policy,
and transportation safety regulations. Variant of the proposed variational framework also promises
robustness against the algorithmic biases towards the minority populations, a major issue that draws
criticism for machine learning applications. This implies our model can be well suited for ensuring
social justice.

",293,Reconsidering Generative Objectives For Counterfactual Reasoning,https://papers.nips.cc/paper/2020/file/f5cfbc876972bd0d031c8abc37344c28-Paper.pdf,194.0
299,"
Our work is theoretical in nature, and as such the potential societal consequence are difficult to
foresee. We anticipate that deeper theoretical understanding of the functioning of machine learning
systems will lead to their improvement in the long term.","Stefano Sarao Mannelli, Eric Vanden-Eijnden, Lenka Zdeborová",Optimization and Generalization of Shallow Neural Networks with Quadratic Activation Functions,1.0,"{'CEA Saclay', 'New York University', 'Institut de Physique Théorique'}",False,Theory (including computational and statistical analyses),"{'France', 'USA'}",False,False,"Our work is theoretical in nature, and as such the potential societal consequence are difficult to foresee. We anticipate that deeper theoretical understanding of the functioning of machine learning systems will lead to their improvement in the long term.",Broader Impact,0.0,False,0.0,True,"Stefano Sarao Mannelli, Eric Vanden-Eijnden, Lenka Zdeborová",9b8b50fb590c590ffbf1295ce92258dc,https://proceedings.neurips.cc/paper/2020/file/9b8b50fb590c590ffbf1295ce92258dc-Paper.pdf,Optimization and Generalization of Shallow Neural Networks with Quadratic Activation Functions,Optimization and Generalization of Shallow Neural Networks with Quadratic Activation Functions,Theory -> Models of Learning and Generalization,Optimization -> Non-Convex Optimization; Theory -> Statistical Physics of Learning,2.0,"
Our work is theoretical in nature, and as such the potential societal consequence are difficult to
foresee. We anticipate that deeper theoretical understanding of the functioning of machine learning
systems will lead to their improvement in the long term.",299,Optimization and Generalization of Shallow Neural Networks with Quadratic Activation Functions,https://papers.nips.cc/paper/2020/file/9b8b50fb590c590ffbf1295ce92258dc-Paper.pdf,39.0
23,"In this paper, we have developed a self-supervised pre-trained GNN model—GROVER to extract the
useful implicit information from massive unlabelled molecules and the downstream tasks can largely
beneﬁt from this pre-trained GNN models. Below is the broader impact of our research:

- For machine learning community: This work demonstrates the success of pre-training
approach on Graph Neural Networks. It is expected that our research will open up a new
venue on an in-depth exploration of pre-trained GNNs for broader potential applications,
such as social networks and knowledge graphs.

- For the drug discovery community: Researchers from drug discovery can beneﬁt from
GROVER from two aspects. First, GROVER has encoded rich structural information of
molecules through the designing of self-supervision tasks. It can also produce feature vectors
of atoms and molecule ﬁngerprints, which can directly serve as inputs of downstream tasks.
Second, GROVER is designed based on Graph Neural Networks and all the parameters are
fully differentiable. So it is easy to ﬁne-tune GROVER in conjunction with speciﬁc drug
discovery tasks, in order to achieve better performance. We hope that GROVER can help
with boosting the performance of various drug discovery applications, such as molecular
property prediction and virtual screening.","Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying WEI, Wenbing Huang, Junzhou Huang",Self-Supervised Graph Transformer on Large-Scale Molecular Data,1.0,"{'University of Texas at Arlington / Tencent AI Lab', 'Tsinghua University', 'Tencent AI Lab'}",False,,"{'USA', 'China'}",True,False,"In this paper, we have developed a self-supervised pre-trained GNN model—GROVER to extract the useful implicit information from massive unlabelled molecules and the downstream tasks can largely benefit from this pre-trained GNN models. Below is the broader impact of our research: - For machine learning community: This work demonstrates the success of pre-training approach on Graph Neural Networks. It is expected that our research will open up a new venue on an in-depth exploration of pre-trained GNNs for broader potential applications, such as social networks and knowledge graphs. - For the drug discovery community: Researchers from drug discovery can benefit from GROVER from two aspects. First, GROVER has encoded rich structural information of molecules through the designing of self-supervision tasks. It can also produce feature vectors of atoms and molecule fingerprints, which can directly serve as inputs of downstream tasks. Second, GROVER is designed based on Graph Neural Networks and all the parameters are fully differentiable. So it is easy to fine-tune GROVER in conjunction with specific drug discovery tasks, in order to achieve better performance. We hope that GROVER can help with boosting the performance of various drug discovery applications, such as molecular property prediction and virtual screening.",Broader Impact,1.0,True,1.0,False,"Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying WEI, Wenbing Huang, Junzhou Huang",94aef38441efa3380a3bed3faf1f9d5d,https://proceedings.neurips.cc/paper/2020/file/94aef38441efa3380a3bed3faf1f9d5d-Paper.pdf,Self-Supervised Graph Transformer on Large-Scale Molecular Data,Self-Supervised Graph Transformer on Large-Scale Molecular Data,Algorithms -> Representation Learning,Applications -> Computational Biology and Bioinformatics; Deep Learning -> Embedding Approaches,9.0,"In this paper, we have developed a self-supervised pre-trained GNN model—GROVER to extract the
useful implicit information from massive unlabelled molecules and the downstream tasks can largely
beneﬁt from this pre-trained GNN models. Below is the broader impact of our research:

- For machine learning community: This work demonstrates the success of pre-training
approach on Graph Neural Networks. It is expected that our research will open up a new
venue on an in-depth exploration of pre-trained GNNs for broader potential applications,
such as social networks and knowledge graphs.

- For the drug discovery community: Researchers from drug discovery can beneﬁt from
GROVER from two aspects. First, GROVER has encoded rich structural information of
molecules through the designing of self-supervision tasks. It can also produce feature vectors
of atoms and molecule ﬁngerprints, which can directly serve as inputs of downstream tasks.
Second, GROVER is designed based on Graph Neural Networks and all the parameters are
fully differentiable. So it is easy to ﬁne-tune GROVER in conjunction with speciﬁc drug
discovery tasks, in order to achieve better performance. We hope that GROVER can help
with boosting the performance of various drug discovery applications, such as molecular
property prediction and virtual screening.",23,Self-Supervised Graph Transformer on Large-Scale Molecular Data,https://papers.nips.cc/paper/2020/file/94aef38441efa3380a3bed3faf1f9d5d-Paper.pdf,199.0
63,"
Robotics systems that utilize fully automated policies for different tasks have already been applied
to many manufacturing, assembly lines, and warehouses processes. Our work demonstrates the
potential to take this automation one step further. Our algorithm can automatically learn complex
control policies from expert demonstrations, which could potentially allow robots to augment their
existing control designs and further optimize their workﬂows. Implementing learned policies in
safety-critical environments such as large-scale assembly lines can be risky as these algorithms do not
have guaranteed precision. Improved theoretical understanding and interpretability of model policies
could potentially mitigate these risks.

","Fan Xie, Alexander Chowdhury, M. Clara De Paolis Kaluza, Linfeng Zhao, Lawson Wong, Rose Yu",Deep Imitation Learning for Bimanual Robotic Manipulation,1.0,"{'University of California, San Diego', 'Northeastern University'}",,"Other applications (e.g., robotics, biology, climate, finance)",{'USA'},False,False,"Robotics systems that utilize fully automated policies for different tasks have already been applied to many manufacturing, assembly lines, and warehouses processes. Our work demonstrates the potential to take this automation one step further. Our algorithm can automatically learn complex control policies from expert demonstrations, which could potentially allow robots to augment their existing control designs and further optimize their workflows. Implementing learned policies in safety-critical environments such as large-scale assembly lines can be risky as these algorithms do not have guaranteed precision. Improved theoretical understanding and interpretability of model policies could potentially mitigate these risks.",Broader Impact,0.0,False,0.0,,"Fan Xie, Alexander Chowdhury, M. Clara De Paolis Kaluza, Linfeng Zhao, Lawson Wong, Rose Yu",18a010d2a9813e91907ce88cd9143fdf,https://proceedings.neurips.cc/paper/2020/file/18a010d2a9813e91907ce88cd9143fdf-Paper.pdf,Deep Imitation Learning for Bimanual Robotic Manipulation,Deep Imitation Learning for Bimanual Robotic Manipulation,Applications -> Robotics,Algorithms -> Relational Learning; Algorithms -> Representation Learning; Deep Learning -> Interaction-Based Deep Networks; Deep Learning -> Recurrent Networks; Reinforcement Learning and Planning -> Hierarchical RL; Reinforcement Learning and Planning -> Model-Based RL; Reinforcement Learning and Planning -> Planning,5.0,"
Robotics systems that utilize fully automated policies for different tasks have already been applied
to many manufacturing, assembly lines, and warehouses processes. Our work demonstrates the
potential to take this automation one step further. Our algorithm can automatically learn complex
control policies from expert demonstrations, which could potentially allow robots to augment their
existing control designs and further optimize their workﬂows. Implementing learned policies in
safety-critical environments such as large-scale assembly lines can be risky as these algorithms do not
have guaranteed precision. Improved theoretical understanding and interpretability of model policies
could potentially mitigate these risks.

",63,Deep Imitation Learning for Bimanual Robotic Manipulation,https://papers.nips.cc/paper/2020/file/18a010d2a9813e91907ce88cd9143fdf-Paper.pdf,96.0
92,"
Our paper belongs to the cluster of works focusing on efﬁcient and resource-aware deep learning.
There are numerous positive impacts of these works, including the reduction of memory footprint
and computational time, so that deep neural networks can be deployed on devices equipped with less
capable computing units, e.g. the microcontroller units. In addition, we help facilitate on-device deep
learning, which could replace traditional cloud computation and foster the protection of privacy.

Popularization of deep learning, which our research helps facilitate, may result in some negative
societal consequences. For example, the unemployment may increase due to the increased automation
enabled by the deep learning.

","Shih-Kang Chao, Zhanyu Wang, Yue Xing, Guang Cheng",Directional Pruning of Deep Neural Networks,1.0,"{'University of Missouri', 'Purdue University'}",,Deep learning,{'USA'},False,False,"Our paper belongs to the cluster of works focusing on efficient and resource-aware deep learning. There are numerous positive impacts of these works, including the reduction of memory footprint and computational time, so that deep neural networks can be deployed on devices equipped with less capable computing units, e.g. the microcontroller units. In addition, we help facilitate on-device deep learning, which could replace traditional cloud computation and foster the protection of privacy. Popularization of deep learning, which our research helps facilitate, may result in some negative societal consequences. For example, the unemployment may increase due to the increased automation enabled by the deep learning.",Broader Impact,0.0,False,0.0,,"Shih-Kang Chao, Zhanyu Wang, Yue Xing, Guang Cheng",a09e75c5c86a7bf6582d2b4d75aad615,https://proceedings.neurips.cc/paper/2020/file/a09e75c5c86a7bf6582d2b4d75aad615-Paper.pdf,Directional Pruning of Deep Neural Networks,Directional Pruning of Deep Neural Networks,Deep Learning -> Efficient Training Methods,Deep Learning -> Efficient Inference Methods; Deep Learning -> Optimization for Deep Networks; Deep Learning -> Supervised Deep Networks,5.0,"
Our paper belongs to the cluster of works focusing on efﬁcient and resource-aware deep learning.
There are numerous positive impacts of these works, including the reduction of memory footprint
and computational time, so that deep neural networks can be deployed on devices equipped with less
capable computing units, e.g. the microcontroller units. In addition, we help facilitate on-device deep
learning, which could replace traditional cloud computation and foster the protection of privacy.

Popularization of deep learning, which our research helps facilitate, may result in some negative
societal consequences. For example, the unemployment may increase due to the increased automation
enabled by the deep learning.

",92,Directional Pruning of Deep Neural Networks,https://papers.nips.cc/paper/2020/file/a09e75c5c86a7bf6582d2b4d75aad615-Paper.pdf,104.0
113,"
Real-world adoption of our proposed methodology may have a number of ethical and societal
consequences. Our method is well-suited to decision support settings, including high-stakes decisions
such as public assistance, parole and bail decisions in criminal justice, and treatment prioritization
in healthcare. Our proposed method has the potential to improve decision-making in settings with
runtime confounding where, as demonstrated in this paper, standard methods produce biased results.
Beyond the statistical bias of simply failing to target the right counterfactual quantity, if decisions are
made based on such predictions, it may disadvantage certain demographic groups in cases such as
where group membership is a confounding factor in observed decisions [10]. This is a signiﬁcant
concern because group membership is often an impermissible input to decision-support tools at
runtime, while also being a factor that inﬂuences discriminatory decision-making in observed data.
Using our methods in these settings can improve predictions and the decisions they ultimately inform.

However, our proposed approach is valid only in the setting where our assumed Conditions 2.1 hold,
and is not offered as a panacea for generally confounded data. The assumption that training data
is unconfounded (§ 2.1.1) deserves considerable scrutiny any time the methods are applied. This
assumption cannot be veriﬁed empirically and must instead be evaluated by domain experts who
have detailed knowledge of the historical decision-making process. We encourage practitioners to
carefully consider the validity of this assumption for their setting. Further data collection may be
required to ensure that the data available for training does contain all factors that may have been
relevant to historical decision-making, even if it is not information that is desirable or permissible to
be used at runtime.

To illustrate the potential beneﬁts as well as possible misuses of our method, we consider how our
method could inform parole decisions. Parole boards determine whether and under what conditions
to release a person from incarceration. Recidivism risk assessment models are widely adopted by
probation and parole departments around the US. It is often of interest to assess the likelihood of
success under different possible supervision conditions. Runtime confounding occurs in the setting
when, for instance, the parole board makes a recommendation after reviewing documents and hearing
spoken testimony, but the board would like to see the predictions of a risk and needs assessment
tool prior to the hearing. The testimony may provide information that both inﬂuences the board’s
decision and reveals drivers of the offender’s likelihood to succeed if released, but this information
is unavailable at prediction time, leading to runtime confounding. Moreover, we may be concerned
that parole boards implicitly used race to make decisions and would like to account for this without
requiring the use of race as a model input. Our method would allow us to do so. Our method can
handle some of the challenging aspects of this setting, but there may be other problems that are not
addressed by our method. For instance, while our method can help account for racial bias in historical
parole decisions, it cannot correct for racial bias in the downstream outcomes. Since research suggests
that people of color are disproportionately arrested relative to true crime rates [1], one should be wary
of using these outcomes. Predictive models trained on such outcomes could perpetuate or exacerbate
racial disparities in criminal justice. Additionally, if Conditions 2.1 do not hold because e.g. the
spoken testimony is not accurately recorded, then our method may lead to unreliable predictions.

The appropriate use of our method in high-stakes real-world settings would include careful consider-
ation of the validity of Conditions 2.1 as well as other potential biases in the data used for model
training. If deployed in the appropriate setting, our method has the potential to help decision-makers
make better decisions that can improve efﬁciency and fairness.

","Amanda Coston, Edward Kennedy, Alexandra Chouldechova",Counterfactual Predictions under Runtime Confounding,1.0,"{'CMU', 'Carnegie Mellon University'}",,,{'USA'},False,False,"Real-world adoption of our proposed methodology may have a number of ethical and societal consequences. Our method is well-suited to decision support settings, including high-stakes decisions such as public assistance, parole and bail decisions in criminal justice, and treatment prioritization in healthcare. Our proposed method has the potential to improve decision-making in settings with runtime confounding where, as demonstrated in this paper, standard methods produce biased results. Beyond the statistical bias of simply failing to target the right counterfactual quantity, if decisions are made based on such predictions, it may disadvantage certain demographic groups in cases such as where group membership is a confounding factor in observed decisions [10]. This is a significant concern because group membership is often an impermissible input to decision-support tools at runtime, while also being a factor that influences discriminatory decision-making in observed data. Using our methods in these settings can improve predictions and the decisions they ultimately inform. However, our proposed approach is valid only in the setting where our assumed Conditions 2.1 hold, and is not offered as a panacea for generally confounded data. The assumption that training data is unconfounded (§ 2.1.1) deserves considerable scrutiny any time the methods are applied. This assumption cannot be verified empirically and must instead be evaluated by domain experts who have detailed knowledge of the historical decision-making process. We encourage practitioners to carefully consider the validity of this assumption for their setting. Further data collection may be required to ensure that the data available for training does contain all factors that may have been relevant to historical decision-making, even if it is not information that is desirable or permissible to be used at runtime. To illustrate the potential benefits as well as possible misuses of our method, we consider how our method could inform parole decisions. Parole boards determine whether and under what conditions to release a person from incarceration. Recidivism risk assessment models are widely adopted by probation and parole departments around the US. It is often of interest to assess the likelihood of success under different possible supervision conditions. Runtime confounding occurs in the setting when, for instance, the parole board makes a recommendation after reviewing documents and hearing spoken testimony, but the board would like to see the predictions of a risk and needs assessment tool prior to the hearing. The testimony may provide information that both influences the board’s decision and reveals drivers of the offender’s likelihood to succeed if released, but this information is unavailable at prediction time, leading to runtime confounding. Moreover, we may be concerned that parole boards implicitly used race to make decisions and would like to account for this without requiring the use of race as a model input. Our method would allow us to do so. Our method can handle some of the challenging aspects of this setting, but there may be other problems that are not addressed by our method. For instance, while our method can help account for racial bias in historical parole decisions, it cannot correct for racial bias in the downstream outcomes. Since research suggests that people of color are disproportionately arrested relative to true crime rates [1], one should be wary of using these outcomes. Predictive models trained on such outcomes could perpetuate or exacerbate racial disparities in criminal justice. Additionally, if Conditions 2.1 do not hold because e.g. the spoken testimony is not accurately recorded, then our method may lead to unreliable predictions. The appropriate use of our method in high-stakes real-world settings would include careful consider- ation of the validity of Conditions 2.1 as well as other potential biases in the data used for model training. If deployed in the appropriate setting, our method has the potential to help decision-makers make better decisions that can improve efficiency and fairness.",Broader Impact,0.0,False,0.0,,"Amanda Coston, Edward Kennedy, Alexandra Chouldechova",2b64c2f19d868305aa8bbc2d72902cc5,https://proceedings.neurips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf,Counterfactual Predictions under Runtime Confounding,Counterfactual Predictions under Runtime Confounding,Probabilistic Methods -> Causal Inference,Algorithms -> Classification; Algorithms -> Regression,26.0,"
Real-world adoption of our proposed methodology may have a number of ethical and societal
consequences. Our method is well-suited to decision support settings, including high-stakes decisions
such as public assistance, parole and bail decisions in criminal justice, and treatment prioritization
in healthcare. Our proposed method has the potential to improve decision-making in settings with
runtime confounding where, as demonstrated in this paper, standard methods produce biased results.
Beyond the statistical bias of simply failing to target the right counterfactual quantity, if decisions are
made based on such predictions, it may disadvantage certain demographic groups in cases such as
where group membership is a confounding factor in observed decisions [10]. This is a signiﬁcant
concern because group membership is often an impermissible input to decision-support tools at
runtime, while also being a factor that inﬂuences discriminatory decision-making in observed data.
Using our methods in these settings can improve predictions and the decisions they ultimately inform.

However, our proposed approach is valid only in the setting where our assumed Conditions 2.1 hold,
and is not offered as a panacea for generally confounded data. The assumption that training data
is unconfounded (§ 2.1.1) deserves considerable scrutiny any time the methods are applied. This
assumption cannot be veriﬁed empirically and must instead be evaluated by domain experts who
have detailed knowledge of the historical decision-making process. We encourage practitioners to
carefully consider the validity of this assumption for their setting. Further data collection may be
required to ensure that the data available for training does contain all factors that may have been
relevant to historical decision-making, even if it is not information that is desirable or permissible to
be used at runtime.

To illustrate the potential beneﬁts as well as possible misuses of our method, we consider how our
method could inform parole decisions. Parole boards determine whether and under what conditions
to release a person from incarceration. Recidivism risk assessment models are widely adopted by
probation and parole departments around the US. It is often of interest to assess the likelihood of
success under different possible supervision conditions. Runtime confounding occurs in the setting
when, for instance, the parole board makes a recommendation after reviewing documents and hearing
spoken testimony, but the board would like to see the predictions of a risk and needs assessment
tool prior to the hearing. The testimony may provide information that both inﬂuences the board’s
decision and reveals drivers of the offender’s likelihood to succeed if released, but this information
is unavailable at prediction time, leading to runtime confounding. Moreover, we may be concerned
that parole boards implicitly used race to make decisions and would like to account for this without
requiring the use of race as a model input. Our method would allow us to do so. Our method can
handle some of the challenging aspects of this setting, but there may be other problems that are not
addressed by our method. For instance, while our method can help account for racial bias in historical
parole decisions, it cannot correct for racial bias in the downstream outcomes. Since research suggests
that people of color are disproportionately arrested relative to true crime rates [1], one should be wary
of using these outcomes. Predictive models trained on such outcomes could perpetuate or exacerbate
racial disparities in criminal justice. Additionally, if Conditions 2.1 do not hold because e.g. the
spoken testimony is not accurately recorded, then our method may lead to unreliable predictions.

The appropriate use of our method in high-stakes real-world settings would include careful consider-
ation of the validity of Conditions 2.1 as well as other potential biases in the data used for model
training. If deployed in the appropriate setting, our method has the potential to help decision-makers
make better decisions that can improve efﬁciency and fairness.

",113,Counterfactual Predictions under Runtime Confounding,https://papers.nips.cc/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf,630.0
119,"
We proposed a method for fast and stable Neural ODEs training. This method can be applied to any
domain, where it is possible to use Neural ODEs. Since the IRDM reduces the time needed to train
Neural ODEs, it has the potential to reduce the carbon footprint of building AI models.","Talgat Daulbaev, Alexandr Katrutsa, Larisa Markeeva, Julia Gusak, Andrzej Cichocki, Ivan Oseledets",Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs,1.0,"{'Skolkovo Institute of Science and Technology', 'Skoltech'}",False,Neural ODEs,{'Russia'},False,False,"We proposed a method for fast and stable Neural ODEs training. This method can be applied to any domain, where it is possible to use Neural ODEs. Since the IRDM reduces the time needed to train Neural ODEs, it has the potential to reduce the carbon footprint of building AI models.",Broader Impact,0.0,False,0.0,False,"Talgat Daulbaev, Alexandr Katrutsa, Larisa Markeeva, Julia Gusak, Andrzej Cichocki, Ivan Oseledets",c24c65259d90ed4a19ab37b6fd6fe716,https://proceedings.neurips.cc/paper/2020/file/c24c65259d90ed4a19ab37b6fd6fe716-Paper.pdf,Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs,Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs,Algorithms -> Dynamical Systems,,3.0,"
We proposed a method for fast and stable Neural ODEs training. This method can be applied to any
domain, where it is possible to use Neural ODEs. Since the IRDM reduces the time needed to train
Neural ODEs, it has the potential to reduce the carbon footprint of building AI models.",119,Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs,https://papers.nips.cc/paper/2020/file/c24c65259d90ed4a19ab37b6fd6fe716-Paper.pdf,51.0
162,"In this paper, we introduce Noise2Same, a self-supervised framework for deep image denoising. As
Noise2Same does not need paired clean data, paired noisy data, nor the noise model, its application
scenarios could be much broader than both traditional supervised and existing self-supervised
denoising frameworks. The most direct application of Noise2Same is to perform denoising on digital
images captured under poor conditions. Individuals and corporations related to photography may
benefit from our work. Besides, Noise2Same could be applied as a pre-processing step for computer
vision tasks such as object detection and segmentation [18], making the downstream algorithms more
robust to noisy images. Also, specific research communities could benefit from the development of
Noise2Same as well. For example, the capture of high-quality microscopy data of live cells, tissue,
or nanomaterials is expensive in terms of budget and time [27]. Proper denoising algorithms allow
researchers to obtain high-quality data from low-quality data and hence remove the need to capture
high-quality data directly. In addition to image denoising applications, the self-supervised denoising
framework could be extended to other domains such as audio noise reduction and single-cell [1]. On
the negative aspect, as many imaging-based research tasks and computer vision applications may be
built upon the denoising algorithms, the failure of Noise2Same could potentially lead to biases or
failures in these tasks and applications.","Yaochen Xie, Zhengyang Wang, Shuiwang Ji",Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising,1.0,{'Texas A&M University'},,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"In this paper, we introduce Noise2Same, a self-supervised framework for deep image denoising. As Noise2Same does not need paired clean data, paired noisy data, nor the noise model, its application scenarios could be much broader than both traditional supervised and existing self-supervised denoising frameworks. The most direct application of Noise2Same is to perform denoising on digital images captured under poor conditions. Individuals and corporations related to photography may benefit from our work. Besides, Noise2Same could be applied as a pre-processing step for computer vision tasks such as object detection and segmentation [ 18], making the downstream algorithms more robust to noisy images. Also, specific research communities could benefit from the development of Noise2Same as well. For example, the capture of high-quality microscopy data of live cells, tissue, or nanomaterials is expensive in terms of budget and time [27]. Proper denoising algorithms allow researchers to obtain high-quality data from low-quality data and hence remove the need to capture high-quality data directly. In addition to image denoising applications, the self-supervised denoising framework could be extended to other domains such as audio noise reduction and single-cell [1]. On the negative aspect, as many imaging-based research tasks and computer vision applications may be built upon the denoising algorithms, the failure of Noise2Same could potentially lead to biases or failures in these tasks and applications.",Broader Impact,0.0,False,0.0,,"Yaochen Xie, Zhengyang Wang, Shuiwang Ji",ea6b2efbdd4255a9f1b3bbc6399b58f4,https://proceedings.neurips.cc/paper/2020/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising,Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising,Applications -> Denoising,Algorithms -> Unsupervised Learning,10.0,"In this paper, we introduce Noise2Same, a self-supervised framework for deep image denoising. As
Noise2Same does not need paired clean data, paired noisy data, nor the noise model, its application
scenarios could be much broader than both traditional supervised and existing self-supervised
denoising frameworks. The most direct application of Noise2Same is to perform denoising on digital
images captured under poor conditions. Individuals and corporations related to photography may
benefit from our work. Besides, Noise2Same could be applied as a pre-processing step for computer
vision tasks such as object detection and segmentation [18], making the downstream algorithms more
robust to noisy images. Also, specific research communities could benefit from the development of
Noise2Same as well. For example, the capture of high-quality microscopy data of live cells, tissue,
or nanomaterials is expensive in terms of budget and time [27]. Proper denoising algorithms allow
researchers to obtain high-quality data from low-quality data and hence remove the need to capture
high-quality data directly. In addition to image denoising applications, the self-supervised denoising
framework could be extended to other domains such as audio noise reduction and single-cell [1]. On
the negative aspect, as many imaging-based research tasks and computer vision applications may be
built upon the denoising algorithms, the failure of Noise2Same could potentially lead to biases or
failures in these tasks and applications.",162,Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising,https://papers.nips.cc/paper/2020/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,220.0
215,"
Our approach is motivated by sequential decision-making problems that arise in several domains such
as road trafﬁc, markets, and security applications with potentially signiﬁcant societal beneﬁts. In such
domains, it is important to predict how the system responds to any given decision and take this into
account to achieve the desired performance. The methods proposed in this paper require to observe
and quantify (via suitable indicators) the response of the system and to dispose of computational
resources to process the observed data. Moreover, it is important that the integrity and the reliability
of such data are veriﬁed, and that the used algorithms are complemented with suitable measures that
ensure the safety of the system at any point in time.

","Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, Andreas Krause",Learning to Play Sequential Games versus Unknown Opponents,1.0,"{'ETH Zurich', 'ETH Zürich'}",,Reinforcement learning and planning,{'Switzerland'},False,False,"Our approach is motivated by sequential decision-making problems that arise in several domains such as road traffic, markets, and security applications with potentially significant societal benefits. In such domains, it is important to predict how the system responds to any given decision and take this into account to achieve the desired performance. The methods proposed in this paper require to observe and quantify (via suitable indicators) the response of the system and to dispose of computational resources to process the observed data. Moreover, it is important that the integrity and the reliability of such data are verified, and that the used algorithms are complemented with suitable measures that ensure the safety of the system at any point in time.",Broader Impact,0.0,False,0.0,,"Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, Andreas Krause",65cf25ef90de99d93fa96dc49d0d8b3c,https://proceedings.neurips.cc/paper/2020/file/65cf25ef90de99d93fa96dc49d0d8b3c-Paper.pdf,Learning to Play Sequential Games versus Unknown Opponents,Learning to Play Sequential Games versus Unknown Opponents,Applications,Algorithms -> Bandit Algorithms; Algorithms -> Online Learning; Probabilistic Methods -> Gaussian Processes; Reinforcement Learning and Planning -> Planning,4.0,"
Our approach is motivated by sequential decision-making problems that arise in several domains such
as road trafﬁc, markets, and security applications with potentially signiﬁcant societal beneﬁts. In such
domains, it is important to predict how the system responds to any given decision and take this into
account to achieve the desired performance. The methods proposed in this paper require to observe
and quantify (via suitable indicators) the response of the system and to dispose of computational
resources to process the observed data. Moreover, it is important that the integrity and the reliability
of such data are veriﬁed, and that the used algorithms are complemented with suitable measures that
ensure the safety of the system at any point in time.

",215,Learning to Play Sequential Games versus Unknown Opponents,https://papers.nips.cc/paper/2020/file/65cf25ef90de99d93fa96dc49d0d8b3c-Paper.pdf,119.0
257,"
Product design (e.g., furniture ) is labor extensive and requires expertise in computer graphics. With
the increasing number and diversity of 3D CAD models in online repositories, there is a growing need
for leverage them to facilitate future product development due to their similarities in function and
shape. Towards this goal, our proposed method provide a novel unsupervised paradigm to establish
dense correspondence for topology-varying objects, which is a prerequisite for shape analysis and
synthesis. Furthermore, as our approach is designed for generic objects, its application space can be
extremely wide.
","Feng Liu, Xiaoming Liu",Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence,1.0,{'Michigan State University'},,Vision,{'USA'},False,False,"Product design ( e.g., furniture ) is labor extensive and requires expertise in computer graphics. With the increasing number and diversity of 3 D CAD models in online repositories, there is a growing need for leverage them to facilitate future product development due to their similarities in function and shape. Towards this goal, our proposed method provide a novel unsupervised paradigm to establish dense correspondence for topology-varying objects, which is a prerequisite for shape analysis and synthesis. Furthermore, as our approach is designed for generic objects, its application space can be extremely wide.",Broader Impact,0.0,False,0.0,,"Feng Liu, Xiaoming Liu",335cd1b90bfa4ee70b39d08a4ae0cf2d,https://proceedings.neurips.cc/paper/2020/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence,Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence,Applications,Applications -> Computer Vision,4.0,"
Product design (e.g., furniture ) is labor extensive and requires expertise in computer graphics. With
the increasing number and diversity of 3D CAD models in online repositories, there is a growing need
for leverage them to facilitate future product development due to their similarities in function and
shape. Towards this goal, our proposed method provide a novel unsupervised paradigm to establish
dense correspondence for topology-varying objects, which is a prerequisite for shape analysis and
synthesis. Furthermore, as our approach is designed for generic objects, its application space can be
extremely wide.
",257,Learning Implicit Functions for Topology-Varying Dense 3D Shape Correspondence,https://papers.nips.cc/paper/2020/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,93.0
269,"
In this paper, we introduced a plug-in framework to explicitly model domain shifts in CNNs. With
the proposed architecture, we need only a small set of dictionary atoms to model each additional
domain, which brings a negligible amount of additional parameters, typically a few hundred. We
consider our plug-and-play method a general contribution to deep learning, assuming no particular
application.

","Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu",A Dictionary Approach to Domain-Invariant Learning in Deep Networks,1.0,"{'Purdue University', 'Duke University'}",True,Vision,{'USA'},False,False,"In this paper, we introduced a plug-in framework to explicitly model domain shifts in CNNs. With the proposed architecture, we need only a small set of dictionary atoms to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred. We consider our plug-and-play method a general contribution to deep learning, assuming no particular application.",7 Broader Impact,0.0,False,0.0,False,"Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu",490640b43519c77281cb2f8471e61a71,https://proceedings.neurips.cc/paper/2020/file/490640b43519c77281cb2f8471e61a71-Paper.pdf,A Dictionary Approach to Domain-Invariant Learning in Deep Networks,A Dictionary Approach to Domain-Invariant Learning in Deep Networks,Deep Learning,Deep Learning -> CNN Architectures,3.0,"
In this paper, we introduced a plug-in framework to explicitly model domain shifts in CNNs. With
the proposed architecture, we need only a small set of dictionary atoms to model each additional
domain, which brings a negligible amount of additional parameters, typically a few hundred. We
consider our plug-and-play method a general contribution to deep learning, assuming no particular
application.

",269,A Dictionary Approach to Domain-Invariant Learning in Deep Networks,https://papers.nips.cc/paper/2020/file/490640b43519c77281cb2f8471e61a71-Paper.pdf,60.0
284,"
Processing of 3D data, in the form of point-cloud, voxel-grids, or meshes ﬁnds important applications
in several ﬁelds, including computer graphics, vision, and robotics. Further developments of this
work, which proposes a novel framework for mesh processing based on deep leaning, could have
a beneﬁt on several real-world applications, including augmented and virtual reality, robotics, and
spatial 3D scene understanding of environments. These applications can have profound positive
implications for the future of our society, by, for example, improving the quality of virtual social
interactions, or increasing the spatial-awareness of current robotic systems to integrate them in our
everyday life.

","Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, Luca Carlone",Primal-Dual Mesh Convolutional Neural Networks,1.0,"{'Massachusetts Institute of Technology', 'ETH Zurich', 'ETH / University of Zurich', 'MIT'}",,Deep learning,"{'USA', 'Switzerland', 'Germany'}",False,False,"Processing of 3D data, in the form of point-cloud, voxel-grids, or meshes finds important applications in several fields, including computer graphics, vision, and robotics. Further developments of this work, which proposes a novel framework for mesh processing based on deep leaning, could have a benefit on several real-world applications, including augmented and virtual reality, robotics, and spatial 3D scene understanding of environments. These applications can have profound positive implications for the future of our society, by, for example, improving the quality of virtual social interactions, or increasing the spatial-awareness of current robotic systems to integrate them in our everyday life.",Broader Impact,0.0,False,0.0,,"Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, Luca Carlone",0a656cc19f3f5b41530182a9e03982a4,https://proceedings.neurips.cc/paper/2020/file/0a656cc19f3f5b41530182a9e03982a4-Paper.pdf,Primal-Dual Mesh Convolutional Neural Networks,Primal-Dual Mesh Convolutional Neural Networks,Deep Learning -> CNN Architectures,Applications -> Computer Vision; Deep Learning -> Attention Models,3.0,"
Processing of 3D data, in the form of point-cloud, voxel-grids, or meshes ﬁnds important applications
in several ﬁelds, including computer graphics, vision, and robotics. Further developments of this
work, which proposes a novel framework for mesh processing based on deep leaning, could have
a beneﬁt on several real-world applications, including augmented and virtual reality, robotics, and
spatial 3D scene understanding of environments. These applications can have profound positive
implications for the future of our society, by, for example, improving the quality of virtual social
interactions, or increasing the spatial-awareness of current robotic systems to integrate them in our
everyday life.

",284,Primal-Dual Mesh Convolutional Neural Networks,https://papers.nips.cc/paper/2020/file/0a656cc19f3f5b41530182a9e03982a4-Paper.pdf,100.0
291,"
In this paper, we aim to address a fundamental problem in many aspects of data science: how to select
a representative and diverse subset of data points. An elegant and intuitive way to score diversity
of a subset is through a determinantal point process where diversity is measured via the geometric
embedding of the data points. Our paper provides a rigorous and scalable method for maximizing
diversity in such probabilistic models.

","Aditya Bhaskara, Amin Karbasi, Silvio Lattanzi, Morteza Zadimoghaddam",Online MAP Inference of Determinantal Point Processes,1.0,"{'Yale', 'University of Utah', 'Google Research'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'USA'},False,False,"In this paper, we aim to address a fundamental problem in many aspects of data science: how to select a representative and diverse subset of data points. An elegant and intuitive way to score diversity of a subset is through a determinantal point process where diversity is measured via the geometric embedding of the data points. Our paper provides a rigorous and scalable method for maximizing diversity in such probabilistic models.",Broader Impact,1.0,False,1.0,,"Aditya Bhaskara, Amin Karbasi, Silvio Lattanzi, Morteza Zadimoghaddam",23378a2d0a25c6ade2c1da1c06c5213f,https://proceedings.neurips.cc/paper/2020/file/23378a2d0a25c6ade2c1da1c06c5213f-Paper.pdf,Online MAP Inference of Determinantal Point Processes,Online MAP Inference of Determinantal Point Processes,Optimization -> Discrete Optimization,Algorithms -> Large Scale Learning; Algorithms -> Online Learning; Optimization -> Submodular Optimization,3.0,"
In this paper, we aim to address a fundamental problem in many aspects of data science: how to select
a representative and diverse subset of data points. An elegant and intuitive way to score diversity
of a subset is through a determinantal point process where diversity is measured via the geometric
embedding of the data points. Our paper provides a rigorous and scalable method for maximizing
diversity in such probabilistic models.

",291,Online MAP Inference of Determinantal Point Processes,https://papers.nips.cc/paper/2020/file/23378a2d0a25c6ade2c1da1c06c5213f-Paper.pdf,71.0
292,"
This work lays the foundations for using the information-theoretic notion of common entropy within
the context of discovering causal relations from data.

Problems that require discovering causal relations from observational data are prominent across many
different ﬁelds. Causality is also central to the development of AI. Therefore, we expect this work to
have a positive impact by providing a new methodology and identifying settings in which causality
can be inferred using this framework.

In terms of the negative effects, we do not foresee an immediate negative effect that may arise because
of this work. The only risks would be due to the risks associated with having stronger machine
learning models, and better AI that could be misused or exploited.

","Murat Kocaoglu, Sanjay Shakkottai, Alexandros G. Dimakis, Constantine Caramanis, Sriram Vishwanath",Applications of Common Entropy for Causal Inference,1.0,"{'University of Texas at Austin', 'University of Texas, Austin', 'IBM Research', 'UT Austin'}",,Causality,{'USA'},False,False,"This work lays the foundations for using the information-theoretic notion of common entropy within the context of discovering causal relations from data. Problems that require discovering causal relations from observational data are prominent across many different fields. Causality is also central to the development of AI. Therefore, we expect this work to have a positive impact by providing a new methodology and identifying settings in which causality can be inferred using this framework. In terms of the negative effects, we do not foresee an immediate negative effect that may arise because of this work. The only risks would be due to the risks associated with having stronger machine learning models, and better AI that could be misused or exploited.",Broader Impact,1.0,False,1.0,,"Murat Kocaoglu, Sanjay Shakkottai, Alexandros G. Dimakis, Constantine Caramanis, Sriram Vishwanath",cae7115f44837c806c9b23ed00a1a28a,https://proceedings.neurips.cc/paper/2020/file/cae7115f44837c806c9b23ed00a1a28a-Paper.pdf,Applications of Common Entropy for Causal Inference,Applications of Common Entropy for Causal Inference,Probabilistic Methods -> Causal Inference,Theory -> Information Theory,6.0,"
This work lays the foundations for using the information-theoretic notion of common entropy within
the context of discovering causal relations from data.

Problems that require discovering causal relations from observational data are prominent across many
different ﬁelds. Causality is also central to the development of AI. Therefore, we expect this work to
have a positive impact by providing a new methodology and identifying settings in which causality
can be inferred using this framework.

In terms of the negative effects, we do not foresee an immediate negative effect that may arise because
of this work. The only risks would be due to the risks associated with having stronger machine
learning models, and better AI that could be misused or exploited.

",292,Applications of Common Entropy for Causal Inference,https://papers.nips.cc/paper/2020/file/cae7115f44837c806c9b23ed00a1a28a-Paper.pdf,119.0
4,"Our work provides a family of simple and efﬁcient algorithms for some classes of minimax optimiza-
tion. We believe our theoretical results advance many applications in ML which requires minimax
optimization. Of particular interests are deep learning and fair machine learning.

Deep learning is used in many safety-critical environments, including self-driving car, biometric
authentication, and so on. There is growing evidence that shows deep neural networks are vulnerable
to adversarial attacks. Since adversarial attacks and defenses are often considered as two-player games,
progress in minimax optimization will deﬁnitely empower both. Furthermore, minimax optimization
problems provide insights and understanding into the balance and equilibrium between attacks and
defenses. As a consequence, making good use of those techniques will boost the robustness of deep
learning models and strengthen the security of its applications.

Fairness in machine learning has attracted much attention, because it is directly relevant to policy
design and social welfare. For example, courts use COMPAS for recidivism prediction. Researchers
have shown that bias is introduced into many machine learning systems through skewed data, limited
features, etc. One approach to mitigate this is adding constraints into the system, which naturally
gives rise to minimax problems.
","Junchi Yang, Siqi Zhang, Negar Kiyavash, Niao He",A Catalyst Framework for Minimax Optimization,1.0,"{'University of Illinois', 'UIUC', 'University of Illinois at Urbana-Champaign', 'École Polytechnique Fédérale de Lausanne'}",,Optimization Methods (continuous or discrete),"{'USA', 'Switzerland'}",False,False,"Our work provides a family of simple and efficient algorithms for some classes of minimax optimization. We believe our theoretical results advance many applications in ML which requires minimax optimization. Of particular interests are deep learning and fair machine learning. Deep learning is used in many safety-critical environments, including self-driving car, biometric authentication, and so on. There is growing evidence that shows deep neural networks are vulnerable to adversarial attacks. Since adversarial attacks and defenses are often considered as two-player games, progress in minimax optimization will definitely empower both. Furthermore, minimax optimization problems provide insights and understanding into the balance and equilibrium between attacks and defenses. As a consequence, making good use of those techniques will boost the robustness of deep learning models and strengthen the security of its applications. Fairness in machine learning has attracted much attention, because it is directly relevant to policy design and social welfare. For example, courts use COMPAS for recidivism prediction. Researchers have shown that bias is introduced into many machine learning systems through skewed data, limited features, etc. One approach to mitigate this is adding constraints into the system, which naturally gives rise to minimax problems.",Broader Impact,0.0,False,0.0,,"Junchi Yang, Siqi Zhang, Negar Kiyavash, Niao He",3db54f5573cd617a0112d35dd1e6b1ef,https://proceedings.neurips.cc/paper/2020/file/3db54f5573cd617a0112d35dd1e6b1ef-Paper.pdf,A Catalyst Framework for Minimax Optimization,A Catalyst Framework for Minimax Optimization,Optimization -> Convex Optimization,Optimization; Optimization -> Non-Convex Optimization; Optimization -> Stochastic Optimization,12.0,"Our work provides a family of simple and efﬁcient algorithms for some classes of minimax optimiza-
tion. We believe our theoretical results advance many applications in ML which requires minimax
optimization. Of particular interests are deep learning and fair machine learning.

Deep learning is used in many safety-critical environments, including self-driving car, biometric
authentication, and so on. There is growing evidence that shows deep neural networks are vulnerable
to adversarial attacks. Since adversarial attacks and defenses are often considered as two-player games,
progress in minimax optimization will deﬁnitely empower both. Furthermore, minimax optimization
problems provide insights and understanding into the balance and equilibrium between attacks and
defenses. As a consequence, making good use of those techniques will boost the robustness of deep
learning models and strengthen the security of its applications.

Fairness in machine learning has attracted much attention, because it is directly relevant to policy
design and social welfare. For example, courts use COMPAS for recidivism prediction. Researchers
have shown that bias is introduced into many machine learning systems through skewed data, limited
features, etc. One approach to mitigate this is adding constraints into the system, which naturally
gives rise to minimax problems.
",4,A Catalyst Framework for Minimax Optimization,https://papers.nips.cc/paper/2020/file/3db54f5573cd617a0112d35dd1e6b1ef-Paper.pdf,193.0
6,"
Beneﬁts

Our conditional chain model addresses the problems where one input sequence is mapped to multiple
sequences by taking advantage of the intrinsic interaction between the output sequences. There are a
variety of applications that can beneﬁt from the use of the conditional information, such as the text
generation tasks. Another important application is the cocktail party problem in speech processing.
With the parallel mapping models, which are the dominant method at present, the model cannot handle
the variable number of speakers ﬂexibly due to the limitation of the model structure. In such models,
the solution to label permutation problems is to exhaustively compute all the permutations with the
computation cost of N !, which cannot be neglected when the number of speakers are more than 3.
However, using the conditional model can avoid this problem. It also proves the effectiveness of
our model which achieves relatively good performance in both separation and recognition tasks. We
make a further step towards attacking cocktail party problem. This will improve the communication
quality of human-computer interaction. And our method can also be applied in meeting transcription
system to provide better performance. We would like to make our code available latter to facilitate
the study applied to other tasks.

Drawbacks

There is no doubt that the improvement of artiﬁcial intelligence can potentially revolutionise our
societies in many ways. However, it also bring some risks to human’s privacy. With the abusing use
of speech separation and recognition techniques, hackers can easily monitor people’s daily life, while
a strong NLP system can also be applied to Internet fraud. We think the community should not only
focus the development of techniques, but also concern the privacy issue. Besides, the widely use of
artiﬁcial intelligence techniques may also lead to mass-scale unemployment problems, such as call
center.","Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, Lei Xie",Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals,1.0,"{'Northwestern Polytechnical University', 'Johns Hopkins University', 'Hitachi', 'Institute of Automation Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences'}",,Audio / Music / Speech,"{'Japan', 'USA', 'China'}",True,False,"Benefits Our conditional chain model addresses the problems where one input sequence is mapped to multiple sequences by taking advantage of the intrinsic interaction between the output sequences. There are a variety of applications that can benefit from the use of the conditional information, such as the text generation tasks. Another important application is the cocktail party problem in speech processing. With the parallel mapping models, which are the dominant method at present, the model cannot handle the variable number of speakers flexibly due to the limitation of the model structure. In such models, the solution to label permutation problems is to exhaustively compute all the permutations with the computation cost of N!, which cannot be neglected when the number of speakers are more than 3. However, using the conditional model can avoid this problem. It also proves the effectiveness of our model which achieves relatively good performance in both separation and recognition tasks. We make a further step towards attacking cocktail party problem. This will improve the communication quality of human-computer interaction. And our method can also be applied in meeting transcription system to provide better performance. We would like to make our code available latter to facilitate the study applied to other tasks. Drawbacks There is no doubt that the improvement of artificial intelligence can potentially revolutionise our societies in many ways. However, it also bring some risks to human’s privacy. With the abusing use of speech separation and recognition techniques, hackers can easily monitor people’s daily life, while a strong NLP system can also be applied to Internet fraud. We think the community should not only focus the development of techniques, but also concern the privacy issue. Besides, the widely use of artificial intelligence techniques may also lead to mass-scale unemployment problems, such as call center.",Broader impact,1.0,True,1.0,,"Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, Lei Xie",27059a11c58ade9b03bde05c2ca7c285,https://proceedings.neurips.cc/paper/2020/file/27059a11c58ade9b03bde05c2ca7c285-Paper.pdf,Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals,Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals,Applications -> Audio and Speech Processing,Applications -> Speech Recognition,16.0,"
Beneﬁts

Our conditional chain model addresses the problems where one input sequence is mapped to multiple
sequences by taking advantage of the intrinsic interaction between the output sequences. There are a
variety of applications that can beneﬁt from the use of the conditional information, such as the text
generation tasks. Another important application is the cocktail party problem in speech processing.
With the parallel mapping models, which are the dominant method at present, the model cannot handle
the variable number of speakers ﬂexibly due to the limitation of the model structure. In such models,
the solution to label permutation problems is to exhaustively compute all the permutations with the
computation cost of N !, which cannot be neglected when the number of speakers are more than 3.
However, using the conditional model can avoid this problem. It also proves the effectiveness of
our model which achieves relatively good performance in both separation and recognition tasks. We
make a further step towards attacking cocktail party problem. This will improve the communication
quality of human-computer interaction. And our method can also be applied in meeting transcription
system to provide better performance. We would like to make our code available latter to facilitate
the study applied to other tasks.

Drawbacks

There is no doubt that the improvement of artiﬁcial intelligence can potentially revolutionise our
societies in many ways. However, it also bring some risks to human’s privacy. With the abusing use
of speech separation and recognition techniques, hackers can easily monitor people’s daily life, while
a strong NLP system can also be applied to Internet fraud. We think the community should not only
focus the development of techniques, but also concern the privacy issue. Besides, the widely use of
artiﬁcial intelligence techniques may also lead to mass-scale unemployment problems, such as call
center.",6,Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals,https://papers.nips.cc/paper/2020/file/27059a11c58ade9b03bde05c2ca7c285-Paper.pdf,299.0
13,"
Our method improves the ability of machines to understand human body poses in images and videos.
Understanding people automatically may arguably be misused by bad actors. However, importantly,
our method is not a form of biometric as it does not allow the identiﬁcation of people. Rather,
only their overall body shape and pose is reconstructed, but these details are insufﬁcient for unique
identiﬁcation. In particular, individual facial features are not reconstructed at all.

Furthermore, our method is an improvement of existing capabilities, but does not introduce a rad-
ical new capability in machine learning. Thus our contribution is unlikely to facilitate misuse of
technology which is already available to anyone.

Finally, any potential negative use of a technology should be balanced against positive uses. Un-
derstanding body poses has many legitimate applications in VR and AR, medical, assistance to the
elderly, assistance to the visual impaired, autonomous driving, human-machine interactions, image
and video categorization, platform integrity, etc.","Benjamin Biggs, David Novotny, Sebastien Ehrhardt, Hanbyul Joo, Ben Graham, Andrea Vedaldi",3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data,1.0,"{'Facebook Research', 'University of Oxford / Facebook AI Research', 'Facebook AI Research', 'University of Cambridge', 'FAIR', 'University of Oxford'}",,Vision,"{'UK', 'USA'}",False,False,"Our method improves the ability of machines to understand human body poses in images and videos. Understanding people automatically may arguably be misused by bad actors. However, importantly, our method is not a form of biometric as it does not allow the identification of people. Rather, only their overall body shape and pose is reconstructed, but these details are insufficient for unique identification. In particular, individual facial features are not reconstructed at all. Furthermore, our method is an improvement of existing capabilities, but does not introduce a rad- ical new capability in machine learning. Thus our contribution is unlikely to facilitate misuse of technology which is already available to anyone. Finally, any potential negative use of a technology should be balanced against positive uses. Un- derstanding body poses has many legitimate applications in VR and AR, medical, assistance to the elderly, assistance to the visual impaired, autonomous driving, human-machine interactions, image and video categorization, platform integrity, etc.",Broader impact,1.0,False,1.0,,"Benjamin Biggs, David Novotny, Sebastien Ehrhardt, Hanbyul Joo, Ben Graham, Andrea Vedaldi",ebf99bb5df6533b6dd9180a59034698d,https://proceedings.neurips.cc/paper/2020/file/ebf99bb5df6533b6dd9180a59034698d-Paper.pdf,3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data,3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data,Applications -> Computer Vision,,9.0,"
Our method improves the ability of machines to understand human body poses in images and videos.
Understanding people automatically may arguably be misused by bad actors. However, importantly,
our method is not a form of biometric as it does not allow the identiﬁcation of people. Rather,
only their overall body shape and pose is reconstructed, but these details are insufﬁcient for unique
identiﬁcation. In particular, individual facial features are not reconstructed at all.

Furthermore, our method is an improvement of existing capabilities, but does not introduce a rad-
ical new capability in machine learning. Thus our contribution is unlikely to facilitate misuse of
technology which is already available to anyone.

Finally, any potential negative use of a technology should be balanced against positive uses. Un-
derstanding body poses has many legitimate applications in VR and AR, medical, assistance to the
elderly, assistance to the visual impaired, autonomous driving, human-machine interactions, image
and video categorization, platform integrity, etc.",13,3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data,https://papers.nips.cc/paper/2020/file/ebf99bb5df6533b6dd9180a59034698d-Paper.pdf,157.0
28,"
One of the main contribution of this paper is to protect meta-learning approaches against data
poisoning attacks. Such robustness encourages participation from data contributors, as they can
collaborate without necessarily trusting the other data contributors. This facilitates participation of
minor contributors who suffer from data scarcity. This fosters democratization of machine learning
by allowing minor contributors to enjoy the beneﬁt of big data through collaboration. Such ecosystem
will also encourage data sharing, thus improving transparency.

The adaptive guarantee we provide in Theorem 1 is fair, in the sense that a group that provides low
noise data will receive a model with better accuracy. However, one potential risk in fairness is that
meta-learning might result in varying accuracy across the groups. This can be problematic as an
under-represented group in training data could suffer from inaccurate prediction for that population.
This is an active area of research in the fairness community, but there is no strong experimental
evidence that this can be mitigated with algorithmic innovations that do not involve collecting more
data from the under-represented population.

Another concern in meta-learning with data sharing is privacy. Without proper system to regulate the
usage of shared data, sensitive information could be leaked or protected features could be inferred.
One silver lining is that robust methods are naturally private, as the trained model is by deﬁnition not
sensitive to any one particular data point. On the other hand, if the system relies on the participation of
various individuals, then either a technological solution needs to be implemented with cryptographic
or privacy preserving primitives, or a proper regulation must be enforced.","Weihao Kong, Raghav Somani, Sham Kakade, Sewoong Oh",Robust Meta-learning for Mixed Linear Regression with Small Batches,1.0,"{'Stanford University', 'University of Washington'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"One of the main contribution of this paper is to protect meta-learning approaches against data poisoning attacks. Such robustness encourages participation from data contributors, as they can collaborate without necessarily trusting the other data contributors. This facilitates participation of minor contributors who suffer from data scarcity. This fosters democratization of machine learning by allowing minor contributors to enjoy the benefit of big data through collaboration. Such ecosystem will also encourage data sharing, thus improving transparency. The adaptive guarantee we provide in Theorem 1 is fair, in the sense that a group that provides low noise data will receive a model with better accuracy. However, one potential risk in fairness is that meta-learning might result in varying accuracy across the groups. This can be problematic as an under-represented group in training data could suffer from inaccurate prediction for that population. This is an active area of research in the fairness community, but there is no strong experimental evidence that this can be mitigated with algorithmic innovations that do not involve collecting more data from the under-represented population. Another concern in meta-learning with data sharing is privacy. Without proper system to regulate the usage of shared data, sensitive information could be leaked or protected features could be inferred. One silver lining is that robust methods are naturally private, as the trained model is by definition not sensitive to any one particular data point. On the other hand, if the system relies on the participation of various individuals, then either a technological solution needs to be implemented with cryptographic or privacy preserving primitives, or a proper regulation must be enforced.",Broader Impact,0.0,False,0.0,,"Weihao Kong, Raghav Somani, Sham Kakade, Sewoong Oh",3214a6d842cc69597f9edf26df552e43,https://proceedings.neurips.cc/paper/2020/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf,Robust Meta-learning for Mixed Linear Regression with Small Batches,Robust Meta-learning for Mixed Linear Regression with Small Batches,Algorithms -> Spectral Methods,Theory -> High-Dimensional Inference,13.0,"
One of the main contribution of this paper is to protect meta-learning approaches against data
poisoning attacks. Such robustness encourages participation from data contributors, as they can
collaborate without necessarily trusting the other data contributors. This facilitates participation of
minor contributors who suffer from data scarcity. This fosters democratization of machine learning
by allowing minor contributors to enjoy the beneﬁt of big data through collaboration. Such ecosystem
will also encourage data sharing, thus improving transparency.

The adaptive guarantee we provide in Theorem 1 is fair, in the sense that a group that provides low
noise data will receive a model with better accuracy. However, one potential risk in fairness is that
meta-learning might result in varying accuracy across the groups. This can be problematic as an
under-represented group in training data could suffer from inaccurate prediction for that population.
This is an active area of research in the fairness community, but there is no strong experimental
evidence that this can be mitigated with algorithmic innovations that do not involve collecting more
data from the under-represented population.

Another concern in meta-learning with data sharing is privacy. Without proper system to regulate the
usage of shared data, sensitive information could be leaked or protected features could be inferred.
One silver lining is that robust methods are naturally private, as the trained model is by deﬁnition not
sensitive to any one particular data point. On the other hand, if the system relies on the participation of
various individuals, then either a technological solution needs to be implemented with cryptographic
or privacy preserving primitives, or a proper regulation must be enforced.",28,Robust Meta-learning for Mixed Linear Regression with Small Batches,https://papers.nips.cc/paper/2020/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf,267.0
36,"
Robust learning of structured signals from high-dimensional data has a wide range of applications,
including imaging processing, computer vision, recommender systems, generative models and many
more. In this work, we presented a new type of practical methods and provided improved under-
standings of solving these problems via over-parameterized models. In particular, our method ex-
ploits the implicit bias introduced by the learning algorithm, with the underlying driving force being
the intrinsic structure of the data itself rather than human handcrafting. Such a design methodology
helps to eliminate human bias in the design process, hence provides the basis for developing truly
fair machine learning systems.

","Chong You, Zhihui Zhu, Qing Qu, Yi Ma",Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization,1.0,"{'UC Berkeley', 'University of California, Berkeley', 'New York University', 'Johns Hopkins University'}",,Deep learning,{'USA'},False,False,"Robust learning of structured signals from high-dimensional data has a wide range of applications, including imaging processing, computer vision, recommender systems, generative models and many more. In this work, we presented a new type of practical methods and provided improved understandings of solving these problems via over-parameterized models. In particular, our method ex- ploits the implicit bias introduced by the learning algorithm, with the underlying driving force being the intrinsic structure of the data itself rather than human handcrafting. Such a design methodology helps to eliminate human bias in the design process, hence provides the basis for developing truly fair machine learning systems.",Broader Impact,0.0,False,0.0,,"Chong You, Zhihui Zhu, Qing Qu, Yi Ma",cd42c963390a9cd025d007dacfa99351,https://proceedings.neurips.cc/paper/2020/file/cd42c963390a9cd025d007dacfa99351-Paper.pdf,Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization,Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization,Algorithms -> Sparsity and Compressed Sensing,"Algorithms -> Components Analysis (e.g., CCA, ICA, LDA, PCA); Applications -> Computer Vision; Applications -> Matrix and Tensor Factorization; Optimization -> Non-Convex Optimization; Theory -> Models of Learning and Generalization",4.0,"
Robust learning of structured signals from high-dimensional data has a wide range of applications,
including imaging processing, computer vision, recommender systems, generative models and many
more. In this work, we presented a new type of practical methods and provided improved under-
standings of solving these problems via over-parameterized models. In particular, our method ex-
ploits the implicit bias introduced by the learning algorithm, with the underlying driving force being
the intrinsic structure of the data itself rather than human handcrafting. Such a design methodology
helps to eliminate human bias in the design process, hence provides the basis for developing truly
fair machine learning systems.

",36,Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization,https://papers.nips.cc/paper/2020/file/cd42c963390a9cd025d007dacfa99351-Paper.pdf,103.0
38,"This work mainly contributes to AutoML in the aspect of discovering better learning rules or
optimization algorithms from data. As a fundamental technique, it seems to pose no substantial
societal risk. This paper proposes several improved training techniques to tackle the dilemma of
training instability and poor generalization in learned optimizers. In general, learning to optimize
(L2O) prevents laborious problem-specific optimizer design, and potentially can largely reduce the
cost (including time, energy and expense) of model training or tuning hyperparameters.","Tianlong Chen, Weiyi Zhang, Zhou Jingyang, Shiyu Chang, Sijia Liu, Lisa Amini, Zhangyang Wang",Training Stronger Baselines for Learning to Optimize,1.0,"{'MIT-IBM Watson AI Lab, IBM Research AI', 'University of Texas at Austin', 'Unversity of Texas at Austin', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'IBM Research'}",,AutoML,"{'USA', 'China'}",False,False,"This work mainly contributes to AutoML in the aspect of discovering better learning rules or optimization algorithms from data. As a fundamental technique, it seems to pose no substantial societal risk. This paper proposes several improved training techniques to tackle the dilemma of training instability and poor generalization in learned optimizers. In general, learning to optimize (L2O) prevents laborious problem-specific optimizer design, and potentially can largely reduce the cost (including time, energy and expense) of model training or tuning hyperparameters.",Broader Impact,1.0,False,1.0,,"Tianlong Chen, Weiyi Zhang, Zhou Jingyang, Shiyu Chang, Sijia Liu, Lisa Amini, Zhangyang Wang",51f4efbfb3e18f4ea053c4d3d282c4e2,https://proceedings.neurips.cc/paper/2020/file/51f4efbfb3e18f4ea053c4d3d282c4e2-Paper.pdf,Training Stronger Baselines for Learning to Optimize,Training Stronger Baselines for Learning to Optimize,Algorithms -> Meta-Learning,Algorithms -> AutoML,4.0,"This work mainly contributes to AutoML in the aspect of discovering better learning rules or
optimization algorithms from data. As a fundamental technique, it seems to pose no substantial
societal risk. This paper proposes several improved training techniques to tackle the dilemma of
training instability and poor generalization in learned optimizers. In general, learning to optimize
(L2O) prevents laborious problem-specific optimizer design, and potentially can largely reduce the
cost (including time, energy and expense) of model training or tuning hyperparameters.",38,Training Stronger Baselines for Learning to Optimize,https://papers.nips.cc/paper/2020/file/51f4efbfb3e18f4ea053c4d3d282c4e2-Paper.pdf,80.0
68,"
Who may beneﬁt from this research. This is a theory paper primarily targeted at the research
community. The signal recovery techniques studied could potentially be useful for practitioners in
areas such as image processing, audio processing, and medical imaging.

Who may be put at disadvantage from this research. We are not aware of any signiﬁcant/imminent
risks of placing anyone at a disadvantage.

Consequences of failure of the system. We believe that most failures should be immediately
evident and detectable due to visibly poor reconstruction performance, and any such outputs could be
discarded as needed. However, some more subtle issues could arise, such as the reconstruction missing
important details in the signal due to the generative model not capturing them. As a result, care is
advised in the choice of generative model, particularly in applications for which the reconstruction of
ﬁne details is crucial.

Potential biases. The signal recovery algorithm that we consider takes as input an arbitrary pre-
trained generative model. If such a pre-trained model has inherent biases, they could be transferred to
the signal recovery algorithm.

","Zhaoqiang Liu, Jonathan Scarlett",The Generalized Lasso with Nonlinear Observations and Generative Priors,1.0,{'National University of Singapore'},,Theory (including computational and statistical analyses),{'Singapore'},False,False,"Who may benefit from this research. This is a theory paper primarily targeted at the research community. The signal recovery techniques studied could potentially be useful for practitioners in areas such as image processing, audio processing, and medical imaging. Who may be put at disadvantage from this research. We are not aware of any significant/imminent risks of placing anyone at a disadvantage. Consequences of failure of the system. We believe that most failures should be immediately evident and detectable due to visibly poor reconstruction performance, and any such outputs could be discarded as needed. However, some more subtle issues could arise, such as the reconstruction missing important details in the signal due to the generative model not capturing them. As a result, care is advised in the choice of generative model, particularly in applications for which the reconstruction of fine details is crucial. Potential biases. The signal recovery algorithm that we consider takes as input an arbitrary pre- trained generative model. If such a pre-trained model has inherent biases, they could be transferred to the signal recovery algorithm.",Broader Impact,0.0,False,0.0,,"Zhaoqiang Liu, Jonathan Scarlett",dd45045f8c68db9f54e70c67048d32e8,https://proceedings.neurips.cc/paper/2020/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf,The Generalized Lasso with Nonlinear Observations and Generative Priors,The Generalized Lasso with Nonlinear Observations and Generative Priors,Algorithms -> Sparsity and Compressed Sensing,Theory -> High-Dimensional Inference,12.0,"
Who may beneﬁt from this research. This is a theory paper primarily targeted at the research
community. The signal recovery techniques studied could potentially be useful for practitioners in
areas such as image processing, audio processing, and medical imaging.

Who may be put at disadvantage from this research. We are not aware of any signiﬁcant/imminent
risks of placing anyone at a disadvantage.

Consequences of failure of the system. We believe that most failures should be immediately
evident and detectable due to visibly poor reconstruction performance, and any such outputs could be
discarded as needed. However, some more subtle issues could arise, such as the reconstruction missing
important details in the signal due to the generative model not capturing them. As a result, care is
advised in the choice of generative model, particularly in applications for which the reconstruction of
ﬁne details is crucial.

Potential biases. The signal recovery algorithm that we consider takes as input an arbitrary pre-
trained generative model. If such a pre-trained model has inherent biases, they could be transferred to
the signal recovery algorithm.

",68,The Generalized Lasso with Nonlinear Observations and Generative Priors,https://papers.nips.cc/paper/2020/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf,178.0
83,"Decision-making problems are ubiquitous, arising in applications such as tracking an oil spill using a
marine robot, selecting an effective drug schedule in personalized medicine, or allocating irrigation
resources based on seasonal weather forecasts. In each of these important application areas, system
dynamics are represented by complex and potentially learned models and the decision-making agent
can only observe the state through limited sensors. Many current planning and reinforcement learning
algorithms focus on fully-observable domains and generate learned policies without performance
guarantees. However, uncertainty and formal guarantees must play a role in robust decision-making
for high-stakes domains. VoI macro-action generation contributes to fundamental research in robust
and efﬁcient model-based planning under uncertainty. As with all formal results, however, the
bounds we derive only hold under the assumptions that we describe in the text. When performing
decision-making in high-stakes applications, understanding these conditions, the extent to which they
hold, and how algorithm performance degrades as assumptions are violated is critical.","Genevieve Flaspohler, Nicholas A. Roy, John W. Fisher III",Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information,1.0,"{'Massachusetts Institute of Technology', 'MIT'}",,,{'USA'},False,False,"Decision-making problems are ubiquitous, arising in applications such as tracking an oil spill using a marine robot, selecting an effective drug schedule in personalized medicine, or allocating irrigation resources based on seasonal weather forecasts. In each of these important application areas, system dynamics are represented by complex and potentially learned models and the decision-making agent can only observe the state through limited sensors. Many current planning and reinforcement learning algorithms focus on fully-observable domains and generate learned policies without performance guarantees. However, uncertainty and formal guarantees must play a role in robust decision-making for high-stakes domains. VoI macro-action generation contributes to fundamental research in robust and efficient model-based planning under uncertainty. As with all formal results, however, the bounds we derive only hold under the assumptions that we describe in the text. When performing decision-making in high-stakes applications, understanding these conditions, the extent to which they hold, and how algorithm performance degrades as assumptions are violated is critical.",Broader Impact,0.0,False,0.0,,"Genevieve Flaspohler, Nicholas A. Roy, John W. Fisher III",7f2be1b45d278ac18804b79207a24c53,https://proceedings.neurips.cc/paper/2020/file/7f2be1b45d278ac18804b79207a24c53-Paper.pdf,Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information,Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information,Reinforcement Learning and Planning -> Planning,Applications -> Robotics; Reinforcement Learning and Planning -> Exploration; Reinforcement Learning and Planning -> Markov Decision Processes,7.0,"Decision-making problems are ubiquitous, arising in applications such as tracking an oil spill using a
marine robot, selecting an effective drug schedule in personalized medicine, or allocating irrigation
resources based on seasonal weather forecasts. In each of these important application areas, system
dynamics are represented by complex and potentially learned models and the decision-making agent
can only observe the state through limited sensors. Many current planning and reinforcement learning
algorithms focus on fully-observable domains and generate learned policies without performance
guarantees. However, uncertainty and formal guarantees must play a role in robust decision-making
for high-stakes domains. VoI macro-action generation contributes to fundamental research in robust
and efﬁcient model-based planning under uncertainty. As with all formal results, however, the
bounds we derive only hold under the assumptions that we describe in the text. When performing
decision-making in high-stakes applications, understanding these conditions, the extent to which they
hold, and how algorithm performance degrades as assumptions are violated is critical.",83,Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information,https://papers.nips.cc/paper/2020/file/7f2be1b45d278ac18804b79207a24c53-Paper.pdf,158.0
93,"
In adversarial learning, this work can beneﬁt users when adversarial examples are correctly identiﬁed.
It can harm users by misidentifying such examples, and the misidentiﬁcations of examples as
suspicious could have negative consequences just like misclassiﬁcations. This work ideally could
beneﬁt groups who are underrepresented in training data, by abstaining rather than performing
harmful incorrect classiﬁcation. However, it could also harm such groups: (a) by providing system
designers an alternative to collecting fully representative data if possible; (b) by harmfully abstaining
at different rates for different groups; (c) when those labels would have otherwise been correct but are
instead being withheld; and (d) by identifying them when they would prefer to remain anonymous.

Our experiments on handwriting recognition have few ethical concerns but also have less ecological
validity than real-world experiments on classifying explicit images or medical scans.

A note of caution.
Inequities may be caused by using training data that differs from the test
distribution on which the classiﬁer is used. For instance, in classifying a person’s gender from a
facial image, Buolamwini and Gebru [2018] have demonstrated that commercial classiﬁers are highly
inaccurate on dark-skinned faces, likely because they were trained on light-skinned faces. In such
cases, it is preferable to collect a more diverse training sample even if it comes at greater expense, or
in some cases to abstain from using machine learning altogether. In such cases, 𝑃 𝑄 learning should
not be used, as an unbalanced distribution of rejections can also be harmful.4

","Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, Omar Montasser",Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples,1.0,"{'Toyota Technological Institute at Chicago', 'Microsoft Research', 'The Simons Institute for the Theory of Computing', 'Micr'}",,Theory (including computational and statistical analyses),{'USA'},True,False,"In adversarial learning, this work can benefit users when adversarial examples are correctly identified. It can harm users by misidentifying such examples, and the misidentifications of examples as suspicious could have negative consequences just like misclassifications. This work ideally could benefit groups who are underrepresented in training data, by abstaining rather than performing harmful incorrect classification. However, it could also harm such groups: (a) by providing system designers an alternative to collecting fully representative data if possible; (b) by harmfully abstaining at different rates for different groups; (c) when those labels would have otherwise been correct but are instead being withheld; and (d) by identifying them when they would prefer to remain anonymous. Our experiments on handwriting recognition have few ethical concerns but also have less ecological validity than real-world experiments on classifying explicit images or medical scans. A note of caution. Inequities may be caused by using training data that differs from the test distribution on which the classifier is used. For instance, in classifying a person’s gender from a facial image, Buolamwini and Gebru [2018] have demonstrated that commercial classifiers are highly inaccurate on dark-skinned faces, likely because they were trained on light-skinned faces. In such cases, it is preferable to collect a more diverse training sample even if it comes at greater expense, or in some cases to abstain from using machine learning altogether. In such cases, PQ learning should not be used, as an unbalanced distribution of rejections can also be harmful. 4 4We are grateful to an anonymous reviewer who pointed out that gender classification is an example of when not to use PQ learning.",Broader Impact,1.0,True,1.0,,"Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, Omar Montasser",b6c8cf4c587f2ead0c08955ee6e2502b,https://proceedings.neurips.cc/paper/2020/file/b6c8cf4c587f2ead0c08955ee6e2502b-Paper.pdf,Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples,Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples,Theory -> Computational Learning Theory,Algorithms -> Adversarial Learning; Theory -> Statistical Learning Theory,11.0,"
In adversarial learning, this work can beneﬁt users when adversarial examples are correctly identiﬁed.
It can harm users by misidentifying such examples, and the misidentiﬁcations of examples as
suspicious could have negative consequences just like misclassiﬁcations. This work ideally could
beneﬁt groups who are underrepresented in training data, by abstaining rather than performing
harmful incorrect classiﬁcation. However, it could also harm such groups: (a) by providing system
designers an alternative to collecting fully representative data if possible; (b) by harmfully abstaining
at different rates for different groups; (c) when those labels would have otherwise been correct but are
instead being withheld; and (d) by identifying them when they would prefer to remain anonymous.

Our experiments on handwriting recognition have few ethical concerns but also have less ecological
validity than real-world experiments on classifying explicit images or medical scans.

A note of caution.
Inequities may be caused by using training data that differs from the test
distribution on which the classiﬁer is used. For instance, in classifying a person’s gender from a
facial image, Buolamwini and Gebru [2018] have demonstrated that commercial classiﬁers are highly
inaccurate on dark-skinned faces, likely because they were trained on light-skinned faces. In such
cases, it is preferable to collect a more diverse training sample even if it comes at greater expense, or
in some cases to abstain from using machine learning altogether. In such cases, 𝑃 𝑄 learning should
not be used, as an unbalanced distribution of rejections can also be harmful.4

",93,Beyond Perturbations: Learning Guarantees with Arbitrary Adversarial Test Examples,https://papers.nips.cc/paper/2020/file/b6c8cf4c587f2ead0c08955ee6e2502b-Paper.pdf,270.0
101,"We have proposed topology design algorithms that can signiﬁcantly speed-up federated learning in a
cross-silo setting. Improving the efﬁciency of federated learning can foster its adoption, allowing
different entities to share datasets that otherwise would not be available for training.

Federated learning is intended to protect data privacy, as the data is not collected at a single point. At
the same time a federated learning system, as any Internet-scale distributed system, may be more
vulnerable to different attacks aiming to jeopardize training or to infer some characteristics of the
local dataset by looking at the different messages [26, 92]. Encryption [10, 80, 8] and differential
privacy [1] techniques may help preventing such attacks.

Federated learning is less efﬁcient than training in a highly-optimized computing cluster. It may
in particular increase energy training costs, due to a more discontinuous usage of local computing
resources and the additional cost of transmitting messages over long distance links. To the best of our
knowledge, energetic considerations for federated learning have not been adequately explored, but
for a few papers considering FL for mobile devices [42, 97].","Othmane MARFOQ, CHUAN XU, Giovanni Neglia, Richard Vidal",Throughput-Optimal Topology Design for Cross-Silo Federated Learning,1.0,"{'Inria Sophia Antipolis', 'Accenture', 'Inria'}",,Federated Learning,{'France'},False,False,"We have proposed topology design algorithms that can significantly speed-up federated learning in a cross-silo setting. Improving the efficiency of federated learning can foster its adoption, allowing different entities to share datasets that otherwise would not be available for training. Federated learning is intended to protect data privacy, as the data is not collected at a single point. At the same time a federated learning system, as any Internet-scale distributed system, may be more vulnerable to different attacks aiming to jeopardize training or to infer some characteristics of the local dataset by looking at the different messages [26, 92]. Encryption [10, 80, 8] and differential privacy [1] techniques may help preventing such attacks. Federated learning is less efficient than training in a highly-optimized computing cluster. It may in particular increase energy training costs, due to a more discontinuous usage of local computing resources and the additional cost of transmitting messages over long distance links. To the best of our knowledge, energetic considerations for federated learning have not been adequately explored, but for a few papers considering FL for mobile devices [42, 97].",6 Broader Impact,1.0,False,1.0,,"Othmane MARFOQ, CHUAN XU, Giovanni Neglia, Richard Vidal",e29b722e35040b88678e25a1ec032a21,https://proceedings.neurips.cc/paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf,Throughput-Optimal Topology Design for Cross-Silo Federated Learning,Throughput-Optimal Topology Design for Cross-Silo Federated Learning,Deep Learning -> Efficient Training Methods,Algorithms -> Large Scale Learning,8.0,"We have proposed topology design algorithms that can signiﬁcantly speed-up federated learning in a
cross-silo setting. Improving the efﬁciency of federated learning can foster its adoption, allowing
different entities to share datasets that otherwise would not be available for training.

Federated learning is intended to protect data privacy, as the data is not collected at a single point. At
the same time a federated learning system, as any Internet-scale distributed system, may be more
vulnerable to different attacks aiming to jeopardize training or to infer some characteristics of the
local dataset by looking at the different messages [26, 92]. Encryption [10, 80, 8] and differential
privacy [1] techniques may help preventing such attacks.

Federated learning is less efﬁcient than training in a highly-optimized computing cluster. It may
in particular increase energy training costs, due to a more discontinuous usage of local computing
resources and the additional cost of transmitting messages over long distance links. To the best of our
knowledge, energetic considerations for federated learning have not been adequately explored, but
for a few papers considering FL for mobile devices [42, 97].",101,Throughput-Optimal Topology Design for Cross-Silo Federated Learning,https://papers.nips.cc/paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf,182.0
105,"At a high level, this work aims to provide a way to characterize adversarial attacks and defenses that
might be best for each other, in a game theoretic sense where the attacker cannot decrease the robust
accuracy further when the defense is ﬁxed, and the defender cannot increase the robust accuracy
further when the attack is ﬁxed. The technical contributions are novel geometry-ﬂavored proof
techniques that can be used to analyze provable attacks and defenses, and a game-theoretic framework
to study such equilibria. Machine learning systems are increasingly being used in security-critical
applications, like healthcare and automated driving: our work can be used to ﬁnd guarantees on the
worst accuracy a defended classiﬁer can have under any attack. This is a step towards safe machine
learning, where the ultimate goal is to be able to construct classiﬁers whose performance cannot be
degraded by an adversary on most data-points with high probability.","Ambar Pal, Rene Vidal",A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses,1.0,"{'Johns Hopkins University, USA', 'Johns Hopkins University'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'USA'},False,False,"At a high level, this work aims to provide a way to characterize adversarial attacks and defenses that might be best for each other, in a game theoretic sense where the attacker cannot decrease the robust accuracy further when the defense is fixed, and the defender cannot increase the robust accuracy further when the attack is fixed. The technical contributions are novel geometry-flavored proof techniques that can be used to analyze provable attacks and defenses, and a game-theoretic framework to study such equilibria. Machine learning systems are increasingly being used in security-critical applications, like healthcare and automated driving: our work can be used to find guarantees on the worst accuracy a defended classifier can have under any attack. This is a step towards safe machine learning, where the ultimate goal is to be able to construct classifiers whose performance cannot be degraded by an adversary on most data-points with high probability.",Broader Impact,0.0,False,0.0,,"Ambar Pal, Rene Vidal",0ea6f098a59fcf2462afc50d130ff034,https://proceedings.neurips.cc/paper/2020/file/0ea6f098a59fcf2462afc50d130ff034-Paper.pdf,A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses,A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses,Algorithms -> Adversarial Learning,Algorithms -> Classification; Theory -> Game Theory and Computational Economics,4.0,"At a high level, this work aims to provide a way to characterize adversarial attacks and defenses that
might be best for each other, in a game theoretic sense where the attacker cannot decrease the robust
accuracy further when the defense is ﬁxed, and the defender cannot increase the robust accuracy
further when the attack is ﬁxed. The technical contributions are novel geometry-ﬂavored proof
techniques that can be used to analyze provable attacks and defenses, and a game-theoretic framework
to study such equilibria. Machine learning systems are increasingly being used in security-critical
applications, like healthcare and automated driving: our work can be used to ﬁnd guarantees on the
worst accuracy a defended classiﬁer can have under any attack. This is a step towards safe machine
learning, where the ultimate goal is to be able to construct classiﬁers whose performance cannot be
degraded by an adversary on most data-points with high probability.",105,A Game Theoretic Analysis of Additive Adversarial Attacks and Defenses,https://papers.nips.cc/paper/2020/file/0ea6f098a59fcf2462afc50d130ff034-Paper.pdf,151.0
107,"
The developed method will make signiﬁcant contributions to both the 3D vision and endangered
species research. The method provides a way to study animals that can only be captured in the wild
as 2D videos, e.g., endangered animal species of birds and zebras. The broader impact includes
enhancing our understanding of such endangered animals simply from videos, as they can be
reconstructed and viewed in 3D. The method can also be applied to tasks such as bird watching,
motion analysis, shape analysis, to name a few. Furthermore, another important application is to
simplify an artists workﬂow, as an initial animated and textured 3D shape can be directly derived
from a video.

","Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz",Online Adaptation for Consistent Mesh Reconstruction in the Wild,1.0,"{'UCSD/UC Berkeley', 'Google / UC Merced', 'NVIDIA', 'University of California, Merced'}",,Vision,{'USA'},False,False,"The developed method will make significant contributions to both the 3D vision and endangered species research. The method provides a way to study animals that can only be captured in the wild as 2D videos, e.g., endangered animal species of birds and zebras. The broader impact includes enhancing our understanding of such endangered animals simply from videos, as they can be reconstructed and viewed in 3D. The method can also be applied to tasks such as bird watching, motion analysis, shape analysis, to name a few. Furthermore, another important application is to simplify an artists workflow, as an initial animated and textured 3D shape can be directly derived from a video.",Broader Impact,1.0,False,1.0,,"Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz",aba3b6fd5d186d28e06ff97135cade7f,https://proceedings.neurips.cc/paper/2020/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf,Online Adaptation for Consistent Mesh Reconstruction in the Wild,Online Adaptation for Consistent Mesh Reconstruction in the Wild,Applications -> Computer Vision,Applications -> Video Analysis,5.0,"
The developed method will make signiﬁcant contributions to both the 3D vision and endangered
species research. The method provides a way to study animals that can only be captured in the wild
as 2D videos, e.g., endangered animal species of birds and zebras. The broader impact includes
enhancing our understanding of such endangered animals simply from videos, as they can be
reconstructed and viewed in 3D. The method can also be applied to tasks such as bird watching,
motion analysis, shape analysis, to name a few. Furthermore, another important application is to
simplify an artists workﬂow, as an initial animated and textured 3D shape can be directly derived
from a video.

",107,Online Adaptation for Consistent Mesh Reconstruction in the Wild,https://papers.nips.cc/paper/2020/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf,111.0
108,"
Our work provides a basis as to what neuroscience experimental data should be collected in order
to infer plasticity rules. Therefore, the experimental neuroscience community may beneﬁt from
this research. The data in this research is collected from neural networks trained on the ImageNet,
Word-Speaker-Noise (WSN), and CIFAR-10 datasets. There is well-documented evidence of bias
emerging from deep neural networks trained on ﬁeld-standard large-scale image databases, with
many ethical harms stemming from use of the biased algorithms to make important decisions on
social policy or limit civil liberties. Rather than using network classiﬁcations in a real-world context,
our study uses observable measurements from within the network mechanism as simulated brain data.
This data is not interpretable to humans and is ultimately used to train and test a classiﬁer to separate
learning rules, which are a mathematical formulation and agnostic to any community. This study
therefore does not leverage any bias in the data. However, identifying neural network observables that
distinguish learning rules presents obvious privacy concerns which could be exploited, for example,
to generate adversarial attacks or even to recover training data. It is possible that the observable
statistics reﬂect the workings of a biased “in silico brain”, in which case the classiﬁer used to separate
learning rules may be biased in that it has only been trained and tested on networks whose training
data may be biased differently than the training data animals typically receive. This is a problem not
just for this study, but generally comes down to available network-training tasks, especially those of
the scale needed to provide neurally-plausible representations. Tasks are the most computationally
complex factor of variation to add values to, so diversifying the set of tasks is an ongoing effort, and
we hope to use balanced, ethically veriﬁed large-scale datasets as they become available. Furthermore,
given the limited number of subjects in neuroscience experiments, this type of bias is not new in our
experiment; the scalability of our approach may provide a way to have more representative data. The
only consequence of failure is that the experiment which is performed on the basis of the conclusions
we draw may not be as decisive in determining the learning rule.

","Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L. Yamins",Identifying Learning Rules From Neural Network Observables,1.0,"{'Stanford', 'Stanford University'}",,Neuroscience and cognitive science,{'USA'},False,False,"Our work provides a basis as to what neuroscience experimental data should be collected in order to infer plasticity rules. Therefore, the experimental neuroscience community may benefit from this research. The data in this research is collected from neural networks trained on the ImageNet, Word-Speaker-Noise (WSN), and CIFAR-10 datasets. There is well-documented evidence of bias emerging from deep neural networks trained on field-standard large-scale image databases, with many ethical harms stemming from use of the biased algorithms to make important decisions on social policy or limit civil liberties. Rather than using network classifications in a real-world context, our study uses observable measurements from within the network mechanism as simulated brain data. This data is not interpretable to humans and is ultimately used to train and test a classifier to separate learning rules, which are a mathematical formulation and agnostic to any community. This study therefore does not leverage any bias in the data. However, identifying neural network observables that distinguish learning rules presents obvious privacy concerns which could be exploited, for example, to generate adversarial attacks or even to recover training data. It is possible that the observable statistics reflect the workings of a biased “ in silico brain”, in which case the classifier used to separate learning rules may be biased in that it has only been trained and tested on networks whose training data may be biased differently than the training data animals typically receive. This is a problem not just for this study, but generally comes down to available network-training tasks, especially those of the scale needed to provide neurally-plausible representations. Tasks are the most computationally complex factor of variation to add values to, so diversifying the set of tasks is an ongoing effort, and we hope to use balanced, ethically verified large-scale datasets as they become available. Furthermore, given the limited number of subjects in neuroscience experiments, this type of bias is not new in our experiment; the scalability of our approach may provide a way to have more representative data. The only consequence of failure is that the experiment which is performed on the basis of the conclusions we draw may not be as decisive in determining the learning rule.",Broader Impact,0.0,False,0.0,,"Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L. Yamins",1ba922ac006a8e5f2b123684c2f4d65f,https://proceedings.neurips.cc/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf,Identifying Learning Rules From Neural Network Observables,Identifying Learning Rules From Neural Network Observables,Neuroscience and Cognitive Science,Deep Learning -> Biologically Plausible Deep Networks; Deep Learning -> CNN Architectures; Deep Learning -> Optimization for Deep Networks; Neuroscience and Cognitive Science -> Neuroscience,13.0,"
Our work provides a basis as to what neuroscience experimental data should be collected in order
to infer plasticity rules. Therefore, the experimental neuroscience community may beneﬁt from
this research. The data in this research is collected from neural networks trained on the ImageNet,
Word-Speaker-Noise (WSN), and CIFAR-10 datasets. There is well-documented evidence of bias
emerging from deep neural networks trained on ﬁeld-standard large-scale image databases, with
many ethical harms stemming from use of the biased algorithms to make important decisions on
social policy or limit civil liberties. Rather than using network classiﬁcations in a real-world context,
our study uses observable measurements from within the network mechanism as simulated brain data.
This data is not interpretable to humans and is ultimately used to train and test a classiﬁer to separate
learning rules, which are a mathematical formulation and agnostic to any community. This study
therefore does not leverage any bias in the data. However, identifying neural network observables that
distinguish learning rules presents obvious privacy concerns which could be exploited, for example,
to generate adversarial attacks or even to recover training data. It is possible that the observable
statistics reﬂect the workings of a biased “in silico brain”, in which case the classiﬁer used to separate
learning rules may be biased in that it has only been trained and tested on networks whose training
data may be biased differently than the training data animals typically receive. This is a problem not
just for this study, but generally comes down to available network-training tasks, especially those of
the scale needed to provide neurally-plausible representations. Tasks are the most computationally
complex factor of variation to add values to, so diversifying the set of tasks is an ongoing effort, and
we hope to use balanced, ethically veriﬁed large-scale datasets as they become available. Furthermore,
given the limited number of subjects in neuroscience experiments, this type of bias is not new in our
experiment; the scalability of our approach may provide a way to have more representative data. The
only consequence of failure is that the experiment which is performed on the basis of the conclusions
we draw may not be as decisive in determining the learning rule.

",108,Identifying Learning Rules From Neural Network Observables,https://papers.nips.cc/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf,365.0
118,"
The approach proposed in this work is only a prototype with limited direct consequences, but the
long-term goal of training huge models with volunteer computing can have a lasting effect on both
the research community and the general public.

Funding bias vs crowdsourcing bias
The main positive outcome we pursue is to let researchers harness volunteer computing and train
models on the scale currently available only to large corporations. Ideally, a deep learning researcher
with a promising idea will be able to amass the computation needed to realize this idea by involving
volunteers. However, the project’s appeal for volunteers depends on many factors such as subject
area, current societal trends, and even researcher’s personality.

For example, a project about teaching agents to play games [38] or ﬁghting global pandemics [67]
is likely to attract more resources than deep learning applied to soil science. In essence, volunteer
computing is biased towards exciting or socially relevant research the same way as traditional HPC is
biased towards the interests of those who fund it.

Alternative use and misuse
The proposed technology can be used with different economic models. If a deep learning system is
immediately useful (e.g. for machine translation, information retrieval, etc), the participants could
use it for their needs based on their contributions to training. This can take many forms: several
labs combining their hardware and training larger models; a web-service that lets people contribute
their compute instead of using ads/subscriptions; or simply a framework that someone can use to run
distributed training across two or more datacenters.

Unfortunately, this also allows several opportunities for malicious use. If a machine is hacked, the
attacker can use its compute unnoticed by the machine owner — much the same way that botnets are
currently used to mine cryptocurrencies. Furthermore, due to decentalized nature even legitimate
Learning@home projects can be hijacked by hackers.

Security
Using crowdsourced hardware makes Learning@home susceptible to attacks from malicious partici-
pants. There are multiple attack vectors already known in P2P community: denial of service attacks,
Sybil attacks, Eclipse attacks and more [68, 69, 70, 71]. Fortunately, there are variations of the DHT
protocol that make it resistant to said attacks: if a reader wishes to learn more about DHT security,
we recommend starting with [68].

Another source of vulnerability stems from the sequential nature of neural networks. If a single expert
were to return incorrect (e.g. NaN) outputs or gradients, it could compromise the outputs of the entire
network and even poison adjacent nodes through backpropagation. Recent studies expose similar
attack patterns on federated learning systems [72, 73].

The redundant nature of mixture-of-experts layers provides some degree of resistance against those
attacks. A single malicious expert will only affect a small fraction of inputs that pass through this
speciﬁc expert. Furthermore, a trainer with access to predictions from multiple experts could provide
a higher degree of robustness by using statistical techniques (e.g., by ignoring outlier gradients).
However, such techniques need to be carefully designed so as not to introduce harmful side effects.

The burden on the network
Finally, we would like to point out the potential harm that our approach can do to network infrastruc-
ture. The experiments we ran in Section 4.1 saturate with the bandwidth of 100 − 200Mbps, most of
which is tensors passed between experts and trainers.

This coincides with the typical home internet speed available in major cities of developed countries.
However, not all ISPs design their infrastructure for users who always use up all their bandwidth. If
too many Learning@home participants are located in one LAN or MAN, it can cause congestion or
even failures in the network infrastructure.

Similar situations frequently took place in late 2000s due to growing popularity of BitTorrent for ﬁle
sharing. Fortunately, the network infrastructure is continually improving, which leads us to believe
that this problem will eventually be solved. Until then, we describe several ways to reduce network
load of Learning@home in Appendix E.

10

","Maksim Ryabinin, Anton Gusev",Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,1.0,"{'Yandex, Higher School of Economics', 'none'}",,Deep learning,{'Russia'},True,False,"The approach proposed in this work is only a prototype with limited direct consequences, but the long-term goal of training huge models with volunteer computing can have a lasting effect on both the research community and the general public. Funding bias vs crowdsourcing bias The main positive outcome we pursue is to let researchers harness volunteer computing and train models on the scale currently available only to large corporations. Ideally, a deep learning researcher with a promising idea will be able to amass the computation needed to realize this idea by involving volunteers. However, the project’s appeal for volunteers depends on many factors such as subject area, current societal trends, and even researcher’s personality. For example, a project about teaching agents to play games [38] or fighting global pandemics [67] is likely to attract more resources than deep learning applied to soil science. In essence, volunteer computing is biased towards exciting or socially relevant research the same way as traditional HPC is biased towards the interests of those who fund it. Alternative use and misuse The proposed technology can be used with different economic models. If a deep learning system is immediately useful (e.g. for machine translation, information retrieval, etc), the participants could use it for their needs based on their contributions to training. This can take many forms: several labs combining their hardware and training larger models; a web-service that lets people contribute their compute instead of using ads/subscriptions; or simply a framework that someone can use to run distributed training across two or more datacenters. Unfortunately, this also allows several opportunities for malicious use. If a machine is hacked, the attacker can use its compute unnoticed by the machine owner — much the same way that botnets are currently used to mine cryptocurrencies. Furthermore, due to decentalized nature even legitimate Learning@home projects can be hijacked by hackers. Security Using crowdsourced hardware makes Learning@home susceptible to attacks from malicious participants. There are multiple attack vectors already known in P2P community: denial of service attacks, Sybil attacks, Eclipse attacks and more [68, 69, 70, 71]. Fortunately, there are variations of the DHT protocol that make it resistant to said attacks: if a reader wishes to learn more about DHT security, we recommend starting with [68]. Another source of vulnerability stems from the sequential nature of neural networks. If a single expert were to return incorrect (e.g. NaN) outputs or gradients, it could compromise the outputs of the entire network and even poison adjacent nodes through backpropagation. Recent studies expose similar attack patterns on federated learning systems [72, 73]. The redundant nature of mixture-of-experts layers provides some degree of resistance against those attacks. A single malicious expert will only affect a small fraction of inputs that pass through this specific expert. Furthermore, a trainer with access to predictions from multiple experts could provide a higher degree of robustness by using statistical techniques (e.g., by ignoring outlier gradients). However, such techniques need to be carefully designed so as not to introduce harmful side effects. The burden on the network Finally, we would like to point out the potential harm that our approach can do to network infrastructure. The experiments we ran in Section 4.1 saturate with the bandwidth of 100 − 200 Mbps, most of which is tensors passed between experts and trainers. This coincides with the typical home internet speed available in major cities of developed countries. However, not all ISPs design their infrastructure for users who always use up all their bandwidth. If too many Learning@home participants are located in one LAN or MAN, it can cause congestion or even failures in the network infrastructure. Similar situations frequently took place in late 2000s due to growing popularity of BitTorrent for file sharing. Fortunately, the network infrastructure is continually improving, which leads us to believe that this problem will eventually be solved. Until then, we describe several ways to reduce network load of Learning@home in Appendix E.",Broader Impact,1.0,False,1.0,,"Max Ryabinin, Anton Gusev",25ddc0f8c9d3e22e03d3076f98d83cb2,https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,Algorithms -> Large Scale Learning,Algorithms -> Communication- or Memory-Bounded Learning; Applications -> Hardware and Systems; Deep Learning,30.0,"
The approach proposed in this work is only a prototype with limited direct consequences, but the
long-term goal of training huge models with volunteer computing can have a lasting effect on both
the research community and the general public.

Funding bias vs crowdsourcing bias
The main positive outcome we pursue is to let researchers harness volunteer computing and train
models on the scale currently available only to large corporations. Ideally, a deep learning researcher
with a promising idea will be able to amass the computation needed to realize this idea by involving
volunteers. However, the project’s appeal for volunteers depends on many factors such as subject
area, current societal trends, and even researcher’s personality.

For example, a project about teaching agents to play games [38] or ﬁghting global pandemics [67]
is likely to attract more resources than deep learning applied to soil science. In essence, volunteer
computing is biased towards exciting or socially relevant research the same way as traditional HPC is
biased towards the interests of those who fund it.

Alternative use and misuse
The proposed technology can be used with different economic models. If a deep learning system is
immediately useful (e.g. for machine translation, information retrieval, etc), the participants could
use it for their needs based on their contributions to training. This can take many forms: several
labs combining their hardware and training larger models; a web-service that lets people contribute
their compute instead of using ads/subscriptions; or simply a framework that someone can use to run
distributed training across two or more datacenters.

Unfortunately, this also allows several opportunities for malicious use. If a machine is hacked, the
attacker can use its compute unnoticed by the machine owner — much the same way that botnets are
currently used to mine cryptocurrencies. Furthermore, due to decentalized nature even legitimate
Learning@home projects can be hijacked by hackers.

Security
Using crowdsourced hardware makes Learning@home susceptible to attacks from malicious partici-
pants. There are multiple attack vectors already known in P2P community: denial of service attacks,
Sybil attacks, Eclipse attacks and more [68, 69, 70, 71]. Fortunately, there are variations of the DHT
protocol that make it resistant to said attacks: if a reader wishes to learn more about DHT security,
we recommend starting with [68].

Another source of vulnerability stems from the sequential nature of neural networks. If a single expert
were to return incorrect (e.g. NaN) outputs or gradients, it could compromise the outputs of the entire
network and even poison adjacent nodes through backpropagation. Recent studies expose similar
attack patterns on federated learning systems [72, 73].

The redundant nature of mixture-of-experts layers provides some degree of resistance against those
attacks. A single malicious expert will only affect a small fraction of inputs that pass through this
speciﬁc expert. Furthermore, a trainer with access to predictions from multiple experts could provide
a higher degree of robustness by using statistical techniques (e.g., by ignoring outlier gradients).
However, such techniques need to be carefully designed so as not to introduce harmful side effects.

The burden on the network
Finally, we would like to point out the potential harm that our approach can do to network infrastruc-
ture. The experiments we ran in Section 4.1 saturate with the bandwidth of 100 − 200Mbps, most of
which is tensors passed between experts and trainers.

This coincides with the typical home internet speed available in major cities of developed countries.
However, not all ISPs design their infrastructure for users who always use up all their bandwidth. If
too many Learning@home participants are located in one LAN or MAN, it can cause congestion or
even failures in the network infrastructure.

Similar situations frequently took place in late 2000s due to growing popularity of BitTorrent for ﬁle
sharing. Fortunately, the network infrastructure is continually improving, which leads us to believe
that this problem will eventually be solved. Until then, we describe several ways to reduce network
load of Learning@home in Appendix E.

10

",118,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,https://papers.nips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf,656.0
123,"
This work has the following potential positive impact in the society: we hope that our work enables
algorithmic fairness for intersectional groups, which most existing works ignore. At the same time,
this work may have some negative consequences because it does not solve the problem of limited
statistical power for severely under-represented minorities. This work also assumes that algorithmic
fairness is appropriate, which may not always be the case. Furthermore, we should be cautious of the
result of failure of the system when the assumed parametric deﬁnition for the fairness violation is
unknown or not appropriate, yet generic metrics are used in its place.

","Forest Yang, Mouhamadou Cisse, Oluwasanmi O. Koyejo",Fairness with Overlapping Groups; a Probabilistic Perspective,,,,,,,,,,,,,,,,,,,,,,"
This work has the following potential positive impact in the society: we hope that our work enables
algorithmic fairness for intersectional groups, which most existing works ignore. At the same time,
this work may have some negative consequences because it does not solve the problem of limited
statistical power for severely under-represented minorities. This work also assumes that algorithmic
fairness is appropriate, which may not always be the case. Furthermore, we should be cautious of the
result of failure of the system when the assumed parametric deﬁnition for the fairness violation is
unknown or not appropriate, yet generic metrics are used in its place.

",123,,https://papers.nips.cc/paper/2020/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf,
130,"
This work is mainly theoretical in nature, introducing a new framework for distributed learning
algorithms where each agent affects all of its peers through its actions. Therefore there are no speciﬁc
ethical considerations relevant to this work. From a broader perspective, the work contributes to the
growing ﬁeld of multi-agent learning. Multi-agent learning can be thought of as the next wave of
machine learning, where the isolated black box machines will start interacting and learning from
each other to form a large machine learning network. This shift involves automating more decision
making processes, that will interact between themselves for our beneﬁt. This, however, does not
come without some application-speciﬁc ethical issues and concerns, as is already being discussed
today for autonomous vehicles, that is a special case of multi-agent learning.

","Ilai Bistritz, Nicholas Bambos",Cooperative Multi-player Bandit Optimization,1.0,"{'Stanford', 'Stanford University'}",,Theory (including computational and statistical analyses),{'USA'},False,False,"This work is mainly theoretical in nature, introducing a new framework for distributed learning algorithms where each agent affects all of its peers through its actions. Therefore there are no specific ethical considerations relevant to this work. From a broader perspective, the work contributes to the growing field of multi-agent learning. Multi-agent learning can be thought of as the next wave of machine learning, where the isolated black box machines will start interacting and learning from each other to form a large machine learning network. This shift involves automating more decision making processes, that will interact between themselves for our benefit. This, however, does not come without some application-specific ethical issues and concerns, as is already being discussed today for autonomous vehicles, that is a special case of multi-agent learning.",7 Broader Impact,0.0,False,0.0,,"Ilai Bistritz, Nicholas Bambos",15ae3b9d6286f1b2a489ea4f3f4abaed,https://proceedings.neurips.cc/paper/2020/file/15ae3b9d6286f1b2a489ea4f3f4abaed-Paper.pdf,Cooperative Multi-player Bandit Optimization,Cooperative Multi-player Bandit Optimization,Theory -> Game Theory and Computational Economics,Algorithms -> Bandit Algorithms,6.0,"
This work is mainly theoretical in nature, introducing a new framework for distributed learning
algorithms where each agent affects all of its peers through its actions. Therefore there are no speciﬁc
ethical considerations relevant to this work. From a broader perspective, the work contributes to the
growing ﬁeld of multi-agent learning. Multi-agent learning can be thought of as the next wave of
machine learning, where the isolated black box machines will start interacting and learning from
each other to form a large machine learning network. This shift involves automating more decision
making processes, that will interact between themselves for our beneﬁt. This, however, does not
come without some application-speciﬁc ethical issues and concerns, as is already being discussed
today for autonomous vehicles, that is a special case of multi-agent learning.

",130,Cooperative Multi-player Bandit Optimization,https://papers.nips.cc/paper/2020/file/15ae3b9d6286f1b2a489ea4f3f4abaed-Paper.pdf,130.0
140,"
This paper presents methods for estimating point-wise dependency between high-dimensional data
using neural networks. This work may beneﬁt the applications that require understanding instance-
level dependency. Take adversarial samples detection as an example: we can perform point-wise
dependency estimation between data and label, and the ones with low point-wise dependency can be
regarded as adversarial samples. We should also be aware of the malicious usage for our framework.
For instance, people with bad intentions can use our framework to detect samples that have a high
point-wise dependency with their of-interest private attributes. Then, these detected samples may be
used for malicious purposes.
","Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, Russ R. Salakhutdinov",Neural Methods for Point-wise Dependency Estimation,1.0,"{'Kyoto University/RIKEN AIP', 'Carnegie Mellon University'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)","{'Japan', 'USA'}",False,False,"This paper presents methods for estimating point-wise dependency between high-dimensional data using neural networks. This work may benefit the applications that require understanding instance- level dependency. Take adversarial samples detection as an example: we can perform point-wise dependency estimation between data and label, and the ones with low point-wise dependency can be regarded as adversarial samples. We should also be aware of the malicious usage for our framework. For instance, people with bad intentions can use our framework to detect samples that have a high point-wise dependency with their of-interest private attributes. Then, these detected samples may be used for malicious purposes.",Broader Impact,0.0,False,0.0,,"Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, Russ R. Salakhutdinov",00a03ec6533ca7f5c644d198d815329c,https://proceedings.neurips.cc/paper/2020/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf,Neural Methods for Point-wise Dependency Estimation,Neural Methods for Point-wise Dependency Estimation,Algorithms -> Relational Learning,Algorithms -> Representation Learning; Algorithms -> Similarity and Distance Learning; Deep Learning,6.0,"
This paper presents methods for estimating point-wise dependency between high-dimensional data
using neural networks. This work may beneﬁt the applications that require understanding instance-
level dependency. Take adversarial samples detection as an example: we can perform point-wise
dependency estimation between data and label, and the ones with low point-wise dependency can be
regarded as adversarial samples. We should also be aware of the malicious usage for our framework.
For instance, people with bad intentions can use our framework to detect samples that have a high
point-wise dependency with their of-interest private attributes. Then, these detected samples may be
used for malicious purposes.
",140,Neural Methods for Point-wise Dependency Estimation,https://papers.nips.cc/paper/2020/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf,102.0
143,"
Fluent and reliable Natural Language Generation can have signiﬁcant societal impacts. On the one
hand, we envision several applications beneﬁcial for business, research or education: from automatic
summarization of news, papers or books, to efﬁcient information access; from automatic and person-
alized student evaluation tests trough question generation, to responsive conversational interfaces. On
the other hand, malicious actors can use the same technology to build tools detrimental to society,
e.g. for creation and propagation of misleading (fake) news as discussed in [30], impersonation, and
deceit. Nonetheless, keeping this research open and under public scrutiny is arguably one of the best
ways to defend against such actors [53].
","Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano",ColdGANs: Taming Language GANs with Cautious Sampling Strategies,1.0,"{'LIP6, UPMC / CNRS, Paris, France', 'LIP6-UPMC', 'reciTAL'}",,Natural language processing,{'France'},False,False,"Fluent and reliable Natural Language Generation can have significant societal impacts. On the one hand, we envision several applications beneficial for business, research or education: from automatic summarization of news, papers or books, to efficient information access; from automatic and person- alized student evaluation tests trough question generation, to responsive conversational interfaces. On the other hand, malicious actors can use the same technology to build tools detrimental to society, e.g. for creation and propagation of misleading (fake) news as discussed in [30], impersonation, and deceit. Nonetheless, keeping this research open and under public scrutiny is arguably one of the best ways to defend against such actors [53].",Broader Impact,1.0,False,1.0,,"Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano",db261d4f615f0e982983be499e57ccda,https://proceedings.neurips.cc/paper/2020/file/db261d4f615f0e982983be499e57ccda-Paper.pdf,ColdGANs: Taming Language GANs with Cautious Sampling Strategies,ColdGANs: Taming Language GANs with Cautious Sampling Strategies,Applications -> Natural Language Processing,,4.0,"
Fluent and reliable Natural Language Generation can have signiﬁcant societal impacts. On the one
hand, we envision several applications beneﬁcial for business, research or education: from automatic
summarization of news, papers or books, to efﬁcient information access; from automatic and person-
alized student evaluation tests trough question generation, to responsive conversational interfaces. On
the other hand, malicious actors can use the same technology to build tools detrimental to society,
e.g. for creation and propagation of misleading (fake) news as discussed in [30], impersonation, and
deceit. Nonetheless, keeping this research open and under public scrutiny is arguably one of the best
ways to defend against such actors [53].
",143,ColdGANs: Taming Language GANs with Cautious Sampling Strategies,https://papers.nips.cc/paper/2020/file/db261d4f615f0e982983be499e57ccda-Paper.pdf,107.0
154,"Developing theoretical understanding of neural networks is crucial both for understanding their biases,
and predicting when and how they will fail. Understanding biases in models is of critical importance
if we hope to prevent them from perpetuating and exaggerating existing racial, gender, and other
social biases [120–123]. Understanding model failure has a direct impact on human safety, as neural
networks increasingly do things like drive cars and control the electrical grid [124–126].
We believe that wide neural networks are currently the most promising direction for the development
of neural network theory. We further believe that the experiments we present in this paper will provide
empirical underpinnings that allow better theory to be developed. We thus believe that this paper will
in a small way aid the engineering of safer and more just machine learning models.
","Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein",Finite Versus Infinite Neural Networks: an Empirical Study,0.0,"{'Google Brain', 'Google'}",,Deep learning,{'USA'},False,False,"Developing theoretical understanding of neural networks is crucial both for understanding their biases, and predicting when and how they will fail. Understanding biases in models is of critical importance if we hope to prevent them from perpetuating and exaggerating existing racial, gender, and other social biases [ 120, 121, 122, 123]. Understanding model failure has a direct impact on human safety, as neural networks increasingly do things like drive cars and control the electrical grid [124, 125, 126]. We believe that wide neural networks are currently the most promising direction for the development of neural network theory. We further believe that the experiments we present in this paper will provide empirical underpinnings that allow better theory to be developed. We thus believe that this paper will in a small way aid the engineering of safer and more just machine learning models.",Broader Impact,1.0,False,0.0,,"Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein",ad086f59924fffe0773f8d0ca22ea712,https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf,Finite Versus Infinite Neural Networks: an Empirical Study,Finite Versus Infinite Neural Networks: an Empirical Study,Deep Learning -> Analysis and Understanding of Deep Networks,Algorithms -> Kernel Methods; Probabilistic Methods -> Gaussian Processes; Theory -> Statistical Physics of Learning,6.0,"Developing theoretical understanding of neural networks is crucial both for understanding their biases,
and predicting when and how they will fail. Understanding biases in models is of critical importance
if we hope to prevent them from perpetuating and exaggerating existing racial, gender, and other
social biases [120–123]. Understanding model failure has a direct impact on human safety, as neural
networks increasingly do things like drive cars and control the electrical grid [124–126].
We believe that wide neural networks are currently the most promising direction for the development
of neural network theory. We further believe that the experiments we present in this paper will provide
empirical underpinnings that allow better theory to be developed. We thus believe that this paper will
in a small way aid the engineering of safer and more just machine learning models.
",154,Finite Versus Infinite Neural Networks: an Empirical Study,https://papers.nips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf,141.0
156,"
Reinforcement learning has been applied to various domains for learning decision-making agents,
including games, intelligent control, robotics, ﬁnance and data analytics. Reinforcement learning
tends to suffer poor generalization performance when the trained agent is deployed in a new environ-
ment. This work proposes a simple data augmentation based solution that substantially improves RL
agents’ generalization performance. This work would have following positive inﬂuences in this ﬁeld.
The proposed method is simple and easy to deploy, and would improve generalization performance
and robustness of RL agents in various environments. This will be inspiring for following research
works, and also beneﬁt deployment of RL agents in practice and help develop trustworthy agents.
On the ﬂip-over side, the effectiveness of the proposed method is only veriﬁed in the game domain.
It remains unclear how it will perform in other domains like ﬁnance, where the data have different
modalities. Improper deployment of the proposed method may even worsen the performance of the
RL agents trained with the proposed method and hurt the quality of output decisions.

","KAIXIN WANG, Bingyi Kang, Jie Shao, Jiashi Feng",Improving Generalization in Reinforcement Learning with Mixture Regularization,1.0,"{'Fudan University', 'National University of Singapore'}",,Reinforcement learning and planning,"{'Singapore', 'China'}",False,False,"Reinforcement learning has been applied to various domains for learning decision-making agents, including games, intelligent control, robotics, finance and data analytics. Reinforcement learning tends to suffer poor generalization performance when the trained agent is deployed in a new environ- ment. This work proposes a simple data augmentation based solution that substantially improves RL agents’ generalization performance. This work would have following positive influences in this field. The proposed method is simple and easy to deploy, and would improve generalization performance and robustness of RL agents in various environments. This will be inspiring for following research works, and also benefit deployment of RL agents in practice and help develop trustworthy agents. On the flip-over side, the effectiveness of the proposed method is only verified in the game domain. It remains unclear how it will perform in other domains like finance, where the data have different modalities. Improper deployment of the proposed method may even worsen the performance of the RL agents trained with the proposed method and hurt the quality of output decisions.",Broader Impact,0.0,False,0.0,,"KAIXIN WANG, Bingyi Kang, Jie Shao, Jiashi Feng",5a751d6a0b6ef05cfe51b86e5d1458e6,https://proceedings.neurips.cc/paper/2020/file/5a751d6a0b6ef05cfe51b86e5d1458e6-Paper.pdf,Improving Generalization in Reinforcement Learning with Mixture Regularization,Improving Generalization in Reinforcement Learning with Mixture Regularization,Reinforcement Learning and Planning -> Reinforcement Learning,,9.0,"
Reinforcement learning has been applied to various domains for learning decision-making agents,
including games, intelligent control, robotics, ﬁnance and data analytics. Reinforcement learning
tends to suffer poor generalization performance when the trained agent is deployed in a new environ-
ment. This work proposes a simple data augmentation based solution that substantially improves RL
agents’ generalization performance. This work would have following positive inﬂuences in this ﬁeld.
The proposed method is simple and easy to deploy, and would improve generalization performance
and robustness of RL agents in various environments. This will be inspiring for following research
works, and also beneﬁt deployment of RL agents in practice and help develop trustworthy agents.
On the ﬂip-over side, the effectiveness of the proposed method is only veriﬁed in the game domain.
It remains unclear how it will perform in other domains like ﬁnance, where the data have different
modalities. Improper deployment of the proposed method may even worsen the performance of the
RL agents trained with the proposed method and hurt the quality of output decisions.

",156,Improving Generalization in Reinforcement Learning with Mixture Regularization,https://papers.nips.cc/paper/2020/file/5a751d6a0b6ef05cfe51b86e5d1458e6-Paper.pdf,172.0
157,"
Algorithms for exploring an environment are a central piece of learning efﬁcient policies for un-
known sequential decision-making tasks. In this section, we discuss the wider impacts of our re-
search both in the Machine Learning (ML) ﬁeld and beyond.

We ﬁrst consider the beneﬁts and risks of our method on ML applications. Efﬁcient exploration
in unknown environments has the possibility to improve methods for tasks that require accurate
knowledge of its environment. By exploring states that are more novel, agents have a more robust
dataset. For control tasks, our method improves the sample efﬁciency of its learning by ﬁnding more
novel states in terms of dynamics for use in training. Our learnt low-dimensional representation also
helps the interpretability of our decision making agents (as seen in Figure 1). More interpretable
agents have potential beneﬁts for many areas of ML, including allowing human understandability
and intervention in human-in-the-loop approaches.

With such applications in mind, we consider societal impacts of our method, along with potential
future work that could be done to improve these societal impacts. One speciﬁc instance of how
efﬁcient exploration and environment modeling might help is in disaster relief settings. With the
incipience of robotic systems for disaster area exploration, autonomous agents need to efﬁciently
explore their unknown surroundings. Further research into scaling these MBRL approaches could
allow for these robotic agents to ﬁnd points of interest (survivors, etc.) efﬁciently.

One potential risk of our application is safe exploration. Our method ﬁnds and learns from states
that are novel in terms of its dynamics. Without safety mechanisms, our agent could view potentially
harmful scenarios as novel due to the rarity of such a situation. For example, a car crash might be
seen as a highly novel state. To mitigate this safety concern we look to literature on Safety in RL
(Garc´ıa and Fern´andez, 2015). In particular, developing a risk metric based on the interpretability
of our approach may be an area of research worth developing.

","Ruo Yu Tao, Vincent Francois-Lavet, Joelle Pineau",Novelty Search in Representational Space for Sample Efficient Exploration,1.0,"{'McGill', 'McGill University'}",,,{'Canada'},False,False,"Algorithms for exploring an environment are a central piece of learning efficient policies for unknown sequential decision-making tasks. In this section, we discuss the wider impacts of our research both in the Machine Learning (ML) field and beyond. We first consider the benefits and risks of our method on ML applications. Efficient exploration in unknown environments has the possibility to improve methods for tasks that require accurate knowledge of its environment. By exploring states that are more novel, agents have a more robust dataset. For control tasks, our method improves the sample efficiency of its learning by finding more novel states in terms of dynamics for use in training. Our learnt low-dimensional representation also helps the interpretability of our decision making agents (as seen in Figure 1). More interpretable agents have potential benefits for many areas of ML, including allowing human understandability and intervention in human-in-the-loop approaches. With such applications in mind, we consider societal impacts of our method, along with potential future work that could be done to improve these societal impacts. One specific instance of how efficient exploration and environment modeling might help is in disaster relief settings. With the incipience of robotic systems for disaster area exploration, autonomous agents need to efficiently explore their unknown surroundings. Further research into scaling these MBRL approaches could allow for these robotic agents to find points of interest (survivors, etc.) efficiently. One potential risk of our application is safe exploration. Our method finds and learns from states that are novel in terms of its dynamics. Without safety mechanisms, our agent could view potentially harmful scenarios as novel due to the rarity of such a situation. For example, a car crash might be seen as a highly novel state. To mitigate this safety concern we look to literature on Safety in RL (Garcıa and Fernández, 2015). In particular, developing a risk metric based on the interpretability of our approach may be an area of research worth developing.",Broader Impact,0.0,False,0.0,,"Ruo Yu Tao, Vincent Francois-Lavet, Joelle Pineau",5ca41a86596a5ed567d15af0be224952,https://proceedings.neurips.cc/paper/2020/file/5ca41a86596a5ed567d15af0be224952-Paper.pdf,Novelty Search in Representational Space for Sample Efficient Exploration,Novelty Search in Representational Space for Sample Efficient Exploration,Reinforcement Learning and Planning -> Exploration,Reinforcement Learning and Planning -> Model-Based RL,19.0,"
Algorithms for exploring an environment are a central piece of learning efﬁcient policies for un-
known sequential decision-making tasks. In this section, we discuss the wider impacts of our re-
search both in the Machine Learning (ML) ﬁeld and beyond.

We ﬁrst consider the beneﬁts and risks of our method on ML applications. Efﬁcient exploration
in unknown environments has the possibility to improve methods for tasks that require accurate
knowledge of its environment. By exploring states that are more novel, agents have a more robust
dataset. For control tasks, our method improves the sample efﬁciency of its learning by ﬁnding more
novel states in terms of dynamics for use in training. Our learnt low-dimensional representation also
helps the interpretability of our decision making agents (as seen in Figure 1). More interpretable
agents have potential beneﬁts for many areas of ML, including allowing human understandability
and intervention in human-in-the-loop approaches.

With such applications in mind, we consider societal impacts of our method, along with potential
future work that could be done to improve these societal impacts. One speciﬁc instance of how
efﬁcient exploration and environment modeling might help is in disaster relief settings. With the
incipience of robotic systems for disaster area exploration, autonomous agents need to efﬁciently
explore their unknown surroundings. Further research into scaling these MBRL approaches could
allow for these robotic agents to ﬁnd points of interest (survivors, etc.) efﬁciently.

One potential risk of our application is safe exploration. Our method ﬁnds and learns from states
that are novel in terms of its dynamics. Without safety mechanisms, our agent could view potentially
harmful scenarios as novel due to the rarity of such a situation. For example, a car crash might be
seen as a highly novel state. To mitigate this safety concern we look to literature on Safety in RL
(Garc´ıa and Fern´andez, 2015). In particular, developing a risk metric based on the interpretability
of our approach may be an area of research worth developing.

",157,Novelty Search in Representational Space for Sample Efficient Exploration,https://papers.nips.cc/paper/2020/file/5ca41a86596a5ed567d15af0be224952-Paper.pdf,325.0
159,"
Our work promotes robustness and fairness in machine learning. First, we study algorithms that make
machine learning models robust when deployed in the real world. Second, our work addresses the
scenario where the target domain is under-resourced and hence collecting labels is difﬁcult. Third,
our theoretical work guides efforts to mitigate dataset bias. We demonstrate that curating a diverse
pool of unlabeled data from the true population can help combating existing bias in labeled datasets.
We give conditions for when bias will be mitigated and when it will be reinforced or ampliﬁed by
popular algorithms used in practice. We take a ﬁrst step towards understanding and preventing the
adverse effects of self-training.

","Yining Chen, Colin Wei, Ananya Kumar, Tengyu Ma",Self-training Avoids Using Spurious Features Under Domain Shift,1.0,{'Stanford University'},,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"Our work promotes robustness and fairness in machine learning. First, we study algorithms that make machine learning models robust when deployed in the real world. Second, our work addresses the scenario where the target domain is under-resourced and hence collecting labels is difficult. Third, our theoretical work guides efforts to mitigate dataset bias. We demonstrate that curating a diverse pool of unlabeled data from the true population can help combating existing bias in labeled datasets. We give conditions for when bias will be mitigated and when it will be reinforced or amplified by popular algorithms used in practice. We take a first step towards understanding and preventing the adverse effects of self-training.",Broader Impact,0.0,False,0.0,,"Yining Chen, Colin Wei, Ananya Kumar, Tengyu Ma",f1298750ed09618717f9c10ea8d1d3b0,https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf,Self-training Avoids Using Spurious Features Under Domain Shift,Self-training Avoids Using Spurious Features Under Domain Shift,Theory -> Statistical Learning Theory,Theory,7.0,"
Our work promotes robustness and fairness in machine learning. First, we study algorithms that make
machine learning models robust when deployed in the real world. Second, our work addresses the
scenario where the target domain is under-resourced and hence collecting labels is difﬁcult. Third,
our theoretical work guides efforts to mitigate dataset bias. We demonstrate that curating a diverse
pool of unlabeled data from the true population can help combating existing bias in labeled datasets.
We give conditions for when bias will be mitigated and when it will be reinforced or ampliﬁed by
popular algorithms used in practice. We take a ﬁrst step towards understanding and preventing the
adverse effects of self-training.

",159,Self-training Avoids Using Spurious Features Under Domain Shift,https://papers.nips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf,112.0
173,"
As machine learning models are trained on increasingly large datasets, more and more training
processes are done in a distributive fashion, where the training data are distributed across the nodes
in a network. These nodes may correspond to data centers across different regions or mobile devices
of individual users. In these settings, the data distributions across different nodes can be inherently
heterogeneous. Under this setting of heterogeneous distributions across nodes, our work provides
new understandings on the behavior of popular distributed training algorithms that are optimized for
robustness and communication efﬁciency. These insights can be useful for practitioners to formally
reason about the trade-offs across accuracy, robustness, and communication efﬁciency.

","Xiangyi Chen, Tiancong Chen, Haoran Sun, Steven Z. Wu, Mingyi Hong",Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,1.0,"{'University of Minnesota', 'Carnegie Mellon University'}",,Optimization Methods (continuous or discrete),{'USA'},False,False,"As machine learning models are trained on increasingly large datasets, more and more training processes are done in a distributive fashion, where the training data are distributed across the nodes in a network. These nodes may correspond to data centers across different regions or mobile devices of individual users. In these settings, the data distributions across different nodes can be inherently heterogeneous. Under this setting of heterogeneous distributions across nodes, our work provides new understandings on the behavior of popular distributed training algorithms that are optimized for robustness and communication efficiency. These insights can be useful for practitioners to formally reason about the trade-offs across accuracy, robustness, and communication efficiency.",7 Broader Impacts,0.0,False,0.0,,"Xiangyi Chen, Tiancong Chen, Haoran Sun, Steven Z. Wu, Mingyi Hong",f629ed9325990b10543ab5946c1362fb,https://proceedings.neurips.cc/paper/2020/file/f629ed9325990b10543ab5946c1362fb-Paper.pdf,Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,Optimization -> Stochastic Optimization,Deep Learning -> Optimization for Deep Networks; Optimization -> Non-Convex Optimization; Theory -> Statistical Learning Theory,5.0,"
As machine learning models are trained on increasingly large datasets, more and more training
processes are done in a distributive fashion, where the training data are distributed across the nodes
in a network. These nodes may correspond to data centers across different regions or mobile devices
of individual users. In these settings, the data distributions across different nodes can be inherently
heterogeneous. Under this setting of heterogeneous distributions across nodes, our work provides
new understandings on the behavior of popular distributed training algorithms that are optimized for
robustness and communication efﬁciency. These insights can be useful for practitioners to formally
reason about the trade-offs across accuracy, robustness, and communication efﬁciency.

",173,Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms,https://papers.nips.cc/paper/2020/file/f629ed9325990b10543ab5946c1362fb-Paper.pdf,110.0
182,"
In this paper, we propose a self-supervised compression technique for generative adversarial networks
and prove its effectiveness across various typical and complex tasks. We also show the ﬁne-grained
compression strategy works better than coarse-grained compression methods.

Our proposed compression technique can beneﬁt various applications for creative endeavors. Mobile
applications performing style transfer or super-resolution on the client to save bandwidth can beneﬁt
from simpler generators. Artists may use inpainting or other texture-generation techniques to save
asset storage space or interactive video generation to save rendering time, and musicians may want a
backing track to generate novel accompaniment that responds in real-time.

GANs are also used to augment training data for tasks like autonomous driving, medical imaging,
etc. Compressed models with higher deployment efﬁciency will help generate more valuable data
to train more robust and accurate networks for pedestrian detection, emergency protection, medical
analysis, and diagnosis. Further, a more efﬁcient data augmentation solution will leave more resources
available to train a more capable network. Our hope is that these effects eventually improve peoples’
safety and well-being.

We also encourage researchers to understand and mitigate the risks arising from GAN applications.
As a generative network has the power to change the style or content of paintings and photos, we
should notice the risk that it can be used to misrepresent objective truth. However, we expect such
misuse will become ineffectual as GAN and detection techniques improve; these techniques may
similarly beneﬁt from our contributions.

","Chong Yu, Jeff Pool",Self-Supervised Generative Adversarial Compression,0.0,"{'Intel', 'NVIDIA'}",,Optimization Methods (continuous or discrete),{'USA'},False,False,"In this paper, we propose a self-supervised compression technique for generative adversarial networks and prove its effectiveness across various typical and complex tasks. We also show the fine-grained compression strategy works better than coarse-grained compression methods. Our proposed compression technique can benefit various applications for creative endeavors. Mobile applications performing style transfer or super-resolution on the client to save bandwidth can benefit from simpler generators. Artists may use inpainting or other texture-generation techniques to save asset storage space or interactive video generation to save rendering time, and musicians may want a backing track to generate novel accompaniment that responds in real-time. GANs are also used to augment training data for tasks like autonomous driving, medical imaging, etc. Compressed models with higher deployment efficiency will help generate more valuable data to train more robust and accurate networks for pedestrian detection, emergency protection, medical analysis, and diagnosis. Further, a more efficient data augmentation solution will leave more resources available to train a more capable network. Our hope is that these effects eventually improve peoples’ safety and well-being. We also encourage researchers to understand and mitigate the risks arising from GAN applications. As a generative network has the power to change the style or content of paintings and photos, we should notice the risk that it can be used to misrepresent objective truth. However, we expect such misuse will become ineffectual as GAN and detection techniques improve; these techniques may similarly benefit from our contributions.",Broader Impact,1.0,False,0.0,,"Chong Yu, Jeff Pool",5d79099fcdf499f12b79770834c0164a,https://proceedings.neurips.cc/paper/2020/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf,Self-Supervised Generative Adversarial Compression,Self-Supervised Generative Adversarial Compression,Algorithms -> Sparsity and Compressed Sensing,Deep Learning -> Optimization for Deep Networks,12.0,"
In this paper, we propose a self-supervised compression technique for generative adversarial networks
and prove its effectiveness across various typical and complex tasks. We also show the ﬁne-grained
compression strategy works better than coarse-grained compression methods.

Our proposed compression technique can beneﬁt various applications for creative endeavors. Mobile
applications performing style transfer or super-resolution on the client to save bandwidth can beneﬁt
from simpler generators. Artists may use inpainting or other texture-generation techniques to save
asset storage space or interactive video generation to save rendering time, and musicians may want a
backing track to generate novel accompaniment that responds in real-time.

GANs are also used to augment training data for tasks like autonomous driving, medical imaging,
etc. Compressed models with higher deployment efﬁciency will help generate more valuable data
to train more robust and accurate networks for pedestrian detection, emergency protection, medical
analysis, and diagnosis. Further, a more efﬁcient data augmentation solution will leave more resources
available to train a more capable network. Our hope is that these effects eventually improve peoples’
safety and well-being.

We also encourage researchers to understand and mitigate the risks arising from GAN applications.
As a generative network has the power to change the style or content of paintings and photos, we
should notice the risk that it can be used to misrepresent objective truth. However, we expect such
misuse will become ineffectual as GAN and detection techniques improve; these techniques may
similarly beneﬁt from our contributions.

",182,Self-Supervised Generative Adversarial Compression,https://papers.nips.cc/paper/2020/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf,242.0
184,"(as required by NeurIPS 2020 CFP)

Continual learning is a key desiderata for Artiﬁcial General Intelligence (AGI). Hence, this line of
research has the beneﬁts as well as the pitfalls of any other research effort geared in this direction.
In particular, our work can help deliver impact on making smarter AI products and services, which
can learn and update themselves on-the-ﬂy when newer tasks and domains are encountered, without
forgetting previously acquired knowledge. This is a necessity in any large-scale deployments of
machine learning and computer vision, including in social media, e-commerce, surveillance, e-
governance, etc - each of which have newer settings, tasks or domains added continually over time.
Any negative effect of our work, such as legal and ethical concerns, are not unique to this work - to
the best of our knowledge, but are shared with any other new development in machine learning, in
general.
","Joseph K J, Vineeth Nallure Balasubramanian",Meta-Consolidation for Continual Learning,1.0,"{'Indian Institute of Technology Hyderabad', 'Indian Institute of Technology, Hyderabad'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'India'},False,False,"(as required by NeurIPS 2020 CFP) Continual learning is a key desiderata for Artificial General Intelligence (AGI). Hence, this line of research has the benefits as well as the pitfalls of any other research effort geared in this direction. In particular, our work can help deliver impact on making smarter AI products and services, which can learn and update themselves on-the-fly when newer tasks and domains are encountered, without forgetting previously acquired knowledge. This is a necessity in any large-scale deployments of machine learning and computer vision, including in social media, e-commerce, surveillance, e- governance, etc - each of which have newer settings, tasks or domains added continually over time. Any negative effect of our work, such as legal and ethical concerns, are not unique to this work - to the best of our knowledge, but are shared with any other new development in machine learning, in general.",Broader Impact,0.0,False,0.0,,"Joseph K J, Vineeth Nallure Balasubramanian",a5585a4d4b12277fee5cad0880611bc6,https://proceedings.neurips.cc/paper/2020/file/a5585a4d4b12277fee5cad0880611bc6-Paper.pdf,Meta-Consolidation for Continual Learning,Meta-Consolidation for Continual Learning,Algorithms -> Classification,Deep Learning -> Supervised Deep Networks,5.0,"(as required by NeurIPS 2020 CFP)

Continual learning is a key desiderata for Artiﬁcial General Intelligence (AGI). Hence, this line of
research has the beneﬁts as well as the pitfalls of any other research effort geared in this direction.
In particular, our work can help deliver impact on making smarter AI products and services, which
can learn and update themselves on-the-ﬂy when newer tasks and domains are encountered, without
forgetting previously acquired knowledge. This is a necessity in any large-scale deployments of
machine learning and computer vision, including in social media, e-commerce, surveillance, e-
governance, etc - each of which have newer settings, tasks or domains added continually over time.
Any negative effect of our work, such as legal and ethical concerns, are not unique to this work - to
the best of our knowledge, but are shared with any other new development in machine learning, in
general.
",184,Meta-Consolidation for Continual Learning,https://papers.nips.cc/paper/2020/file/a5585a4d4b12277fee5cad0880611bc6-Paper.pdf,148.0
185,"
Deep neural networks have achieved great success in recent years. In this paper, we provide a strong
justiﬁcation for the clipping technique in training deep neural networks and provides a satisfactory
answer on how to efﬁciently optimize a general possibly non-convex (L0, L1)-smooth objective
function. It closely aligns with the community’s pursuit of explainability, controllability, and practi-
cability of machine learning.

Besides its efﬁciency in training deep neural networks, a series of recent work ( Thakkar et al. [2019],
Chen et al. [2020], Lee and Kifer [2020] ) also studies the relation between clipping and privacy
preservation, which appears to be a major concern in machine learning applications. Therefore, we
hope that a thorough understanding of clipping methods will be beneﬁcial to the modern society.
","Bohang Zhang, Jikai Jin, Cong Fang, Liwei Wang",Improved Analysis of Clipping Algorithms for Non-convex Optimization,1.0,{'Peking University'},,Optimization Methods (continuous or discrete),{'China'},False,False,"Deep neural networks have achieved great success in recent years. In this paper, we provide a strong justification for the clipping technique in training deep neural networks and provides a satisfactory answer on how to efficiently optimize a general possibly non-convex ( L 0 , L 1 ) -smooth objective function. It closely aligns with the community’s pursuit of explainability, controllability, and practicability of machine learning. Besides its efficiency in training deep neural networks, a series of recent work ( Thakkar et al. [2019], Chen et al. [2020], Lee and Kifer [2020] ) also studies the relation between clipping and privacy preservation, which appears to be a major concern in machine learning applications. Therefore, we hope that a thorough understanding of clipping methods will be beneficial to the modern society.",Broader Impact,0.0,False,0.0,,"Bohang Zhang, Jikai Jin, Cong Fang, Liwei Wang",b282d1735283e8eea45bce393cefe265,https://proceedings.neurips.cc/paper/2020/file/b282d1735283e8eea45bce393cefe265-Paper.pdf,Improved Analysis of Clipping Algorithms for Non-convex Optimization,Improved Analysis of Clipping Algorithms for Non-convex Optimization,Optimization -> Non-Convex Optimization,Optimization -> Stochastic Optimization,7.0,"
Deep neural networks have achieved great success in recent years. In this paper, we provide a strong
justiﬁcation for the clipping technique in training deep neural networks and provides a satisfactory
answer on how to efﬁciently optimize a general possibly non-convex (L0, L1)-smooth objective
function. It closely aligns with the community’s pursuit of explainability, controllability, and practi-
cability of machine learning.

Besides its efﬁciency in training deep neural networks, a series of recent work ( Thakkar et al. [2019],
Chen et al. [2020], Lee and Kifer [2020] ) also studies the relation between clipping and privacy
preservation, which appears to be a major concern in machine learning applications. Therefore, we
hope that a thorough understanding of clipping methods will be beneﬁcial to the modern society.
",185,Improved Analysis of Clipping Algorithms for Non-convex Optimization,https://papers.nips.cc/paper/2020/file/b282d1735283e8eea45bce393cefe265-Paper.pdf,130.0
190,"
Information hiding is commonly used in an nefarious context, such as criminals secretly coordinating
plans through messages hidden in images on public websites. However, we investigate the potential
of deep hiding for beneﬁcial applications. By comparing the existing DDH and the proposed UDH
on various aspects, we provide an intuition behind the mechanisms of DNN-based deep hiding. With
this understanding, we further push the simple use case of hiding one image in another to a more
general case of hiding M in N images. Meanwhile, we demonstrate the possibility that different
recipients can retrieve different secret images through the same container image, which can be used
to provide different content to different users based on their practical needs. Intellectual property
has become a major concern with the exponentially increasing number of images and videos. The
proposed UDH constitutes a timely solution for addressing this issue with the concept of “universal
watermarking”. Finally, we show that UDH can be used for light ﬁeld messaging. Different from
prior works that only hide simple binary information, our work demonstrates the possibility of hiding
a full image, which can greatly expand its use cases. For example, museums and exhibitions, can
adopt light ﬁeld messaging to provide a more informative and vivid experience for visitors.","Chaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, In Kweon","UDH: Universal Deep Hiding for Steganography, Watermarking, and Light Field Messaging",1.0,{'KAIST'},,Deep learning,{'South Korea'},False,False,"Information hiding is commonly used in an nefarious context, such as criminals secretly coordinating plans through messages hidden in images on public websites. However, we investigate the potential of deep hiding for beneficial applications. By comparing the existing DDH and the proposed UDH on various aspects, we provide an intuition behind the mechanisms of DNN-based deep hiding. With this understanding, we further push the simple use case of hiding one image in another to a more general case of hiding M in N images. Meanwhile, we demonstrate the possibility that different recipients can retrieve different secret images through the same container image, which can be used to provide different content to different users based on their practical needs. Intellectual property has become a major concern with the exponentially increasing number of images and videos. The proposed UDH constitutes a timely solution for addressing this issue with the concept of “universal watermarking”. Finally, we show that UDH can be used for light field messaging. Different from prior works that only hide simple binary information, our work demonstrates the possibility of hiding a full image, which can greatly expand its use cases. For example, museums and exhibitions, can adopt light field messaging to provide a more informative and vivid experience for visitors.",7 Broader impact,0.0,False,0.0,,"Chaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, In Kweon",73d02e4344f71a0b0d51a925246990e7,https://proceedings.neurips.cc/paper/2020/file/73d02e4344f71a0b0d51a925246990e7-Paper.pdf,"UDH: Universal Deep Hiding for Steganography, Watermarking, and Light Field Messaging","UDH: Universal Deep Hiding for Steganography, Watermarking, and Light Field Messaging","Deep Learning -> Visualization, Interpretability, and Explainability",,10.0,"
Information hiding is commonly used in an nefarious context, such as criminals secretly coordinating
plans through messages hidden in images on public websites. However, we investigate the potential
of deep hiding for beneﬁcial applications. By comparing the existing DDH and the proposed UDH
on various aspects, we provide an intuition behind the mechanisms of DNN-based deep hiding. With
this understanding, we further push the simple use case of hiding one image in another to a more
general case of hiding M in N images. Meanwhile, we demonstrate the possibility that different
recipients can retrieve different secret images through the same container image, which can be used
to provide different content to different users based on their practical needs. Intellectual property
has become a major concern with the exponentially increasing number of images and videos. The
proposed UDH constitutes a timely solution for addressing this issue with the concept of “universal
watermarking”. Finally, we show that UDH can be used for light ﬁeld messaging. Different from
prior works that only hide simple binary information, our work demonstrates the possibility of hiding
a full image, which can greatly expand its use cases. For example, museums and exhibitions, can
adopt light ﬁeld messaging to provide a more informative and vivid experience for visitors.",190,"UDH: Universal Deep Hiding for Steganography, Watermarking, and Light Field Messaging",https://papers.nips.cc/paper/2020/file/73d02e4344f71a0b0d51a925246990e7-Paper.pdf,210.0
203,"
Accurate evaluation enables more reliable machine learning methods and more trustworthy commu-
nication of their capabilities. To the extent that machine learning methods may be beneﬁcial – in
that they may be used to facilitate medical diagnosis, assistive technology for individuals with motor
impairments, or understanding of helpful economic interventions – accurate evaluation ensures these
beneﬁts are fully realized. To the extent that machine learning methods may be harmful – in that
they may used to facilitate the spread of false information or privacy erosion – accurate evaluation
should still make these methods more effective at their goals, even if societally undesirable. As in any
machine learning methodology, it is also important for the buyer to beware; while we have tried to
pick a broad array of experimental settings and to support our methods with theory, there may remain
cases of interest when our approximations fail without warning. In fact, we take care to note that
cross-validation and its points of failure are still not fully understood. All of our results are relative
to exact cross-validation – since it is taken as the de facto standard for evaluation in the machine
learning community (not without reason [Musgrave et al., 2020]). But when exact cross-validation
fails, we therefore expect our method to fail as well.","Soumya Ghosh, William T. Stephenson, Tin D. Nguyen, Sameer Deshpande, Tamara Broderick",Approximate Cross-Validation for Structured Models,1.0,"{'Wharton Statistics', 'IBM Research', 'MIT'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'USA'},False,False,"Accurate evaluation enables more reliable machine learning methods and more trustworthy commu- nication of their capabilities. To the extent that machine learning methods may be beneficial – in that they may be used to facilitate medical diagnosis, assistive technology for individuals with motor impairments, or understanding of helpful economic interventions – accurate evaluation ensures these benefits are fully realized. To the extent that machine learning methods may be harmful – in that they may used to facilitate the spread of false information or privacy erosion – accurate evaluation should still make these methods more effective at their goals, even if societally undesirable. As in any machine learning methodology, it is also important for the buyer to beware; while we have tried to pick a broad array of experimental settings and to support our methods with theory, there may remain cases of interest when our approximations fail without warning. In fact, we take care to note that cross-validation and its points of failure are still not fully understood. All of our results are relative to exact cross-validation – since it is taken as the de facto standard for evaluation in the machine learning community (not without reason [Musgrave et al., 2020]). But when exact cross-validation fails, we therefore expect our method to fail as well.  Acknowledgments. This work was supported by the MIT-IBM Watson AI Lab, DARPA, the CSAIL–MSR Trustworthy AI Initiative, an NSF CAREER Award, an ARO YIP Award, ONR, and Amazon. Broderick Group is also supported by the Sloan Foundation, ARPA-E, Department of the Air Force, and MIT Lincoln Laboratory.",Broader Impact,1.0,False,1.0,,"Soumya Ghosh, William T. Stephenson, Tin D. Nguyen, Sameer Deshpande, Tamara Broderick",636efd4f9aeb5781e9ea815cdd633e52,https://proceedings.neurips.cc/paper/2020/file/636efd4f9aeb5781e9ea815cdd633e52-Paper.pdf,Approximate Cross-Validation for Structured Models,Approximate Cross-Validation for Structured Models,Probabilistic Methods -> Graphical Models,Algorithms -> Model Selection and Structure Learning; Algorithms -> Structured Prediction; Applications -> Time Series Analysis; Probabilistic Methods -> Latent Variable Models,10.0,"
Accurate evaluation enables more reliable machine learning methods and more trustworthy commu-
nication of their capabilities. To the extent that machine learning methods may be beneﬁcial – in
that they may be used to facilitate medical diagnosis, assistive technology for individuals with motor
impairments, or understanding of helpful economic interventions – accurate evaluation ensures these
beneﬁts are fully realized. To the extent that machine learning methods may be harmful – in that
they may used to facilitate the spread of false information or privacy erosion – accurate evaluation
should still make these methods more effective at their goals, even if societally undesirable. As in any
machine learning methodology, it is also important for the buyer to beware; while we have tried to
pick a broad array of experimental settings and to support our methods with theory, there may remain
cases of interest when our approximations fail without warning. In fact, we take care to note that
cross-validation and its points of failure are still not fully understood. All of our results are relative
to exact cross-validation – since it is taken as the de facto standard for evaluation in the machine
learning community (not without reason [Musgrave et al., 2020]). But when exact cross-validation
fails, we therefore expect our method to fail as well.",203,Approximate Cross-Validation for Structured Models,https://papers.nips.cc/paper/2020/file/636efd4f9aeb5781e9ea815cdd633e52-Paper.pdf,261.0
205,"
This work provides a technical advancement in the ﬁeld of supervised classiﬁcation, which already
has tremendous impact throughout industry. Whether or not they realize it, most people experience
the results of this type of classiﬁer many times a day.

As we have shown, supervised contrastive learning can improve both the accuracy and robustness
of classiﬁers, which for most applications should strictly be an improvement. For example, an au-
tonomous car that makes a classiﬁcation error due to data distribution shift can result in catastrophic
results. Thus decreasing this class of error undoubtedly promotes safety. Human driver error is a
massive source of fatalities around the world, so improving the safety of autonomous cars furthers
the efforts of replacing human drivers. The ﬂip side of that progress is of course the potential for loss
of employment in ﬁelds like trucking and taxi driving. Similar two-sided coins can be considered
for assessing the impact of any application of classiﬁcation.

An additional potential impact of our work in particular is showing the value of training with large
batch sizes. Generally, large batch size training comes at the cost of substantial energy consumption,
which unfortunately today requires the burning of fossil fuels, which in turn warms our planet. We
are proud to say that the model training that was done in the course of this research was entirely
carbon-neutral, where all power consumed was either green to start with, or offset by purchases of
green energy. There is unfortunately no way to guarantee that once this research is publicly available
that all practitioners of it will choose, or even have the ability to choose, to limit the environmental
impact of their model training.","Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan",Supervised Contrastive Learning,1.0,"{'Google Research', 'Google', 'Google LLC', 'Massachusetts Institute of Technology', 'MIT'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"This work provides a technical advancement in the field of supervised classification, which already has tremendous impact throughout industry. Whether or not they realize it, most people experience the results of this type of classifier many times a day. As we have shown, supervised contrastive learning can improve both the accuracy and robustness of classifiers, which for most applications should strictly be an improvement. For example, an autonomous car that makes a classification error due to data distribution shift can result in catastrophic results. Thus decreasing this class of error undoubtedly promotes safety. Human driver error is a massive source of fatalities around the world, so improving the safety of autonomous cars furthers the efforts of replacing human drivers. The flip side of that progress is of course the potential for loss of employment in fields like trucking and taxi driving. Similar two-sided coins can be considered for assessing the impact of any application of classification. An additional potential impact of our work in particular is showing the value of training with large batch sizes. Generally, large batch size training comes at the cost of substantial energy consumption, which unfortunately today requires the burning of fossil fuels, which in turn warms our planet. We are proud to say that the model training that was done in the course of this research was entirely carbon-neutral, where all power consumed was either green to start with, or offset by purchases of green energy. There is unfortunately no way to guarantee that once this research is publicly available that all practitioners of it will choose, or even have the ability to choose, to limit the environmental impact of their model training.",Broader Impact,1.0,False,1.0,,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan",d89a66c7c80a29b1bdbab0f2a1a94af8,https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf,Supervised Contrastive Learning,Supervised Contrastive Learning,Deep Learning,Deep Learning -> Supervised Deep Networks,12.0,"
This work provides a technical advancement in the ﬁeld of supervised classiﬁcation, which already
has tremendous impact throughout industry. Whether or not they realize it, most people experience
the results of this type of classiﬁer many times a day.

As we have shown, supervised contrastive learning can improve both the accuracy and robustness
of classiﬁers, which for most applications should strictly be an improvement. For example, an au-
tonomous car that makes a classiﬁcation error due to data distribution shift can result in catastrophic
results. Thus decreasing this class of error undoubtedly promotes safety. Human driver error is a
massive source of fatalities around the world, so improving the safety of autonomous cars furthers
the efforts of replacing human drivers. The ﬂip side of that progress is of course the potential for loss
of employment in ﬁelds like trucking and taxi driving. Similar two-sided coins can be considered
for assessing the impact of any application of classiﬁcation.

An additional potential impact of our work in particular is showing the value of training with large
batch sizes. Generally, large batch size training comes at the cost of substantial energy consumption,
which unfortunately today requires the burning of fossil fuels, which in turn warms our planet. We
are proud to say that the model training that was done in the course of this research was entirely
carbon-neutral, where all power consumed was either green to start with, or offset by purchases of
green energy. There is unfortunately no way to guarantee that once this research is publicly available
that all practitioners of it will choose, or even have the ability to choose, to limit the environmental
impact of their model training.",205,Supervised Contrastive Learning,https://papers.nips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf,278.0
209,"
Algorithmic fairness has a potential high social importance. The goal is to make safer the application
of automatic agents as decision makers in our society. We think that learning a fair representation
can be a practical way to pursue the goal of generating unbiased machine learning. A fair machine
learning is needed in our society, especially after several discoveries of unfair biases in the current
standard machine learning models. With less biased and more fair machine learning models, we can
increase the trust of people in automatic agents – and we can also spread awareness of the possible
issue of bias in machine learning models among colleagues in our research community. We have the
possibility to enhance the beneﬁts that using machine learning can provide to society and we need to
avoid translating the negative human biases to the learned models.

We are aware that statistical measures of fairness (such as statistical parity or equal opportunity)
cannot be considered as the unique deﬁnitions for bias. In fact, many others have been presented,
exploring areas like – for example – causality. Indeed, we know that the choice of a deﬁnition of
fairness for the task at hand has to be carefully understood by the user (i.e., a human) and not selected
by an automatic agent. In this sense, it is well known that different deﬁnitions of fairness are even in
contrast one each other. Consequently, enforcing one deﬁnition, we are simultaneously forcing other
deﬁnitions to be violated. The choice of the right deﬁnition is fundamental but it is out of the scope
of our proposal, and requires a careful human-in-the-loop approach.

","Luca Oneto, Michele Donini, Giulia Luise, Carlo Ciliberto, Andreas Maurer, Massimiliano Pontil",Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,1.0,"{'Adalbertstr 55', 'IIT', 'University of Genoa', 'Imperial College London', 'Amazon', 'University College London'}",,"Social aspects of machine learning (e.g., fairness, safety, privacy)","{'USA', 'UK', 'Italy', 'India'}",False,False,"Algorithmic fairness has a potential high social importance. The goal is to make safer the application of automatic agents as decision makers in our society. We think that learning a fair representation can be a practical way to pursue the goal of generating unbiased machine learning. A fair machine learning is needed in our society, especially after several discoveries of unfair biases in the current standard machine learning models. With less biased and more fair machine learning models, we can increase the trust of people in automatic agents – and we can also spread awareness of the possible issue of bias in machine learning models among colleagues in our research community. We have the possibility to enhance the benefits that using machine learning can provide to society and we need to avoid translating the negative human biases to the learned models. We are aware that statistical measures of fairness (such as statistical parity or equal opportunity) cannot be considered as the unique definitions for bias. In fact, many others have been presented, exploring areas like – for example – causality. Indeed, we know that the choice of a definition of fairness for the task at hand has to be carefully understood by the user (i.e., a human) and not selected by an automatic agent. In this sense, it is well known that different definitions of fairness are even in contrast one each other. Consequently, enforcing one definition, we are simultaneously forcing other definitions to be violated. The choice of the right definition is fundamental but it is out of the scope of our proposal, and requires a careful human-in-the-loop approach.",Broader impact,1.0,False,1.0,,"Luca Oneto, Michele Donini, Giulia Luise, Carlo Ciliberto, Andreas Maurer, Massimiliano Pontil",af9c0e0c1dee63e5acad8b7ed1a5be96,https://proceedings.neurips.cc/paper/2020/file/af9c0e0c1dee63e5acad8b7ed1a5be96-Paper.pdf,Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,"Social Aspects of Machine Learning -> Fairness, Accountability, and Transparency",Theory -> Statistical Learning Theory,12.0,"
Algorithmic fairness has a potential high social importance. The goal is to make safer the application
of automatic agents as decision makers in our society. We think that learning a fair representation
can be a practical way to pursue the goal of generating unbiased machine learning. A fair machine
learning is needed in our society, especially after several discoveries of unfair biases in the current
standard machine learning models. With less biased and more fair machine learning models, we can
increase the trust of people in automatic agents – and we can also spread awareness of the possible
issue of bias in machine learning models among colleagues in our research community. We have the
possibility to enhance the beneﬁts that using machine learning can provide to society and we need to
avoid translating the negative human biases to the learned models.

We are aware that statistical measures of fairness (such as statistical parity or equal opportunity)
cannot be considered as the unique deﬁnitions for bias. In fact, many others have been presented,
exploring areas like – for example – causality. Indeed, we know that the choice of a deﬁnition of
fairness for the task at hand has to be carefully understood by the user (i.e., a human) and not selected
by an automatic agent. In this sense, it is well known that different deﬁnitions of fairness are even in
contrast one each other. Consequently, enforcing one deﬁnition, we are simultaneously forcing other
deﬁnitions to be violated. The choice of the right deﬁnition is fundamental but it is out of the scope
of our proposal, and requires a careful human-in-the-loop approach.

",209,Exploiting MMD and Sinkhorn Divergences for Fair and Transferable Representation Learning,https://papers.nips.cc/paper/2020/file/af9c0e0c1dee63e5acad8b7ed1a5be96-Paper.pdf,270.0
211,"
The HD-GaBO formulation presented in this paper makes a step towards more explainable and
interpretable BO approaches.
Indeed, in addition to the beneﬁts in terms of performance, the
inclusion of domain knowledge via Riemannian manifolds into the BO framework permits to treat
the space parameters in a principled way. This can notably be contrasted with approaches based on
random features, that generally remain hard to interpret for humans. As often, the gains in terms of
explainability and interpretability come at the expense of the low computational cost that characterizes
random-based approaches. However, the carbon footprint of the proposed approach remains low
compared to many deep approaches used nowadays in machine learning applications.

","Noémie Jaquier, Leonel Rozo",High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds,1.0,"{'Karlsruhe Institute of Technology', 'Bosch Center for Artificial Intelligence'}",,Optimization Methods (continuous or discrete),{'Germany'},False,False,"The HD-GaBO formulation presented in this paper makes a step towards more explainable and interpretable BO approaches. Indeed, in addition to the benefits in terms of performance, the inclusion of domain knowledge via Riemannian manifolds into the BO framework permits to treat the space parameters in a principled way. This can notably be contrasted with approaches based on random features, that generally remain hard to interpret for humans. As often, the gains in terms of explainability and interpretability come at the expense of the low computational cost that characterizes random-based approaches. However, the carbon footprint of the proposed approach remains low compared to many deep approaches used nowadays in machine learning applications.",Broader Impact,1.0,False,1.0,,"Noémie Jaquier, Leonel Rozo",f05da679342107f92111ad9d65959cd3,https://proceedings.neurips.cc/paper/2020/file/f05da679342107f92111ad9d65959cd3-Paper.pdf,High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds,High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds,Algorithms -> Bandit Algorithms,Algorithms -> Nonlinear Dimensionality Reduction and Manifold Learning; Probabilistic Methods -> Gaussian Processes,5.0,"
The HD-GaBO formulation presented in this paper makes a step towards more explainable and
interpretable BO approaches.
Indeed, in addition to the beneﬁts in terms of performance, the
inclusion of domain knowledge via Riemannian manifolds into the BO framework permits to treat
the space parameters in a principled way. This can notably be contrasted with approaches based on
random features, that generally remain hard to interpret for humans. As often, the gains in terms of
explainability and interpretability come at the expense of the low computational cost that characterizes
random-based approaches. However, the carbon footprint of the proposed approach remains low
compared to many deep approaches used nowadays in machine learning applications.

",211,High-Dimensional Bayesian Optimization via Nested Riemannian Manifolds,https://papers.nips.cc/paper/2020/file/f05da679342107f92111ad9d65959cd3-Paper.pdf,112.0
217,"
Our work presents a novel approach to solve the extreme multilabel (XML) classiﬁcation problem.
The main contributions of our paper are (a) the use of matrix reordering techniques to hierarchically
partition the label space, and (b) the development of a data-dependent group testing scheme, that
improves label grouping signiﬁcantly for MLGT, and can leverage the recently proposed log-time
decoding algorithm. These innovations lead to a signiﬁcantly faster training algorithm compared to
most existing methods that yields comparable results. The XML problem is encountered in a number
of applications from related searches to ad recommendations to natural language understanding in
the technology industry, and from gene and molecule classiﬁcations to learning neural activities in
scientiﬁc data analysis. A fast algorithm like ours will enable prediction in high-throughput and
real-time settings, and address some of the limitations in traditional inline search suggestions or
approaches for related searches. The preliminary results presented in the supplement demonstrate
how the group testing approach achieves learning with less data for MLC. There is increasing interest
among research (and defense) agencies in developing learning algorithms that achieve ‘Learning
with Less Labels’ (LwLL), see darpa.mil/program/learning-with-less-labels. Thus, the
business and research impacts are likely to be signiﬁcant (and clear).","Shashanka Ubaru, Sanjeeb Dash, Arya Mazumdar, Oktay Gunluk",Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping,1.0,"{'IBM Research', 'University of Massachusetts Amherst', 'Cornell University'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'USA'},True,False,"Our work presents a novel approach to solve the extreme multilabel (XML) classification problem. The main contributions of our paper are (a) the use of matrix reordering techniques to hierarchically partition the label space, and (b) the development of a data-dependent group testing scheme, that improves label grouping significantly for MLGT, and can leverage the recently proposed log-time decoding algorithm. These innovations lead to a significantly faster training algorithm compared to most existing methods that yields comparable results. The XML problem is encountered in a number of applications from related searches to ad recommendations to natural language understanding in the technology industry, and from gene and molecule classifications to learning neural activities in scientific data analysis. A fast algorithm like ours will enable prediction in high-throughput and real-time settings, and address some of the limitations in traditional inline search suggestions or approaches for related searches. The preliminary results presented in the supplement demonstrate how the group testing approach achieves learning with less data for MLC. There is increasing interest among research (and defense) agencies in developing learning algorithms that achieve ‘Learning with Less Labels’ (LwLL), see darpa.mil/program/learning-with-less-labels . Thus, the business and research impacts are likely to be significant (and clear).",Broader Impact,1.0,False,1.0,,"Shashanka Ubaru, Sanjeeb Dash, Arya Mazumdar, Oktay Gunluk",fea16e782bc1b1240e4b3c797012e289,https://proceedings.neurips.cc/paper/2020/file/fea16e782bc1b1240e4b3c797012e289-Paper.pdf,Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping,Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping,Algorithms -> Classification,Algorithms -> Data Compression; Algorithms -> Sparsity and Compressed Sensing; Theory -> Information Theory,8.0,"
Our work presents a novel approach to solve the extreme multilabel (XML) classiﬁcation problem.
The main contributions of our paper are (a) the use of matrix reordering techniques to hierarchically
partition the label space, and (b) the development of a data-dependent group testing scheme, that
improves label grouping signiﬁcantly for MLGT, and can leverage the recently proposed log-time
decoding algorithm. These innovations lead to a signiﬁcantly faster training algorithm compared to
most existing methods that yields comparable results. The XML problem is encountered in a number
of applications from related searches to ad recommendations to natural language understanding in
the technology industry, and from gene and molecule classiﬁcations to learning neural activities in
scientiﬁc data analysis. A fast algorithm like ours will enable prediction in high-throughput and
real-time settings, and address some of the limitations in traditional inline search suggestions or
approaches for related searches. The preliminary results presented in the supplement demonstrate
how the group testing approach achieves learning with less data for MLC. There is increasing interest
among research (and defense) agencies in developing learning algorithms that achieve ‘Learning
with Less Labels’ (LwLL), see darpa.mil/program/learning-with-less-labels. Thus, the
business and research impacts are likely to be signiﬁcant (and clear).",217,Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping,https://papers.nips.cc/paper/2020/file/fea16e782bc1b1240e4b3c797012e289-Paper.pdf,201.0
220,"
We would like to begin by praising the community for it’s recent shift of attention to the study
and design of high-quality model explanations, particularly when attempting to provide insight
into inferences that cannot be easily interpreted. This essential direction of research indicates that
scientists and technologists care about making complex methods more accessible and understandable.
Encouraging a world where the advanced computational techniques used to inﬂuence modern society
are generally understandable empowers us all to steer its direction with clearer vision toward a more
desirable state. At the very least, we can be more prepared to avoid negative outcomes.

Fear of the Unknown. As deep neural networks have brought performance improvements that were
seemingly unimaginable, the general public remains largely ignorant of what makes these approaches
so powerful; to them, it’s equivalent to magic. This is popularly conveyed in sensational headlines
and the ﬁctional portrayals of dystopian futures, laying somewhere at the intersection of Black Mirror,
Orwell’s 1984, and Terminator’s Skynet. Nevertheless, these fears underlay legitimate concerns that
motivate privacy preserving regulations including GDPR and CCPA, not to mention the inclusion
of the NeurIPS Broader Impact Statement. The continuing efforts to research effective methods
of ensuring explainability of these models can help partially alleviate this tension, particular if we
succeed in providing an honest (yet comprehensible) representation of the underlying method. This
is the motivation for our work.

Positive Impact. The stated objective and ideal impact of this paper is relatively straightforward;
we intend to empower the community with knowledge of the average end-user perspective. In this
way, we might all have a better understanding of the right approach to increase transparency and
effectively communicate with the public, thereby offering a bridge between the technologist and
non-technical layperson.

Unintended Risks. One of our research conclusions is that exposing subsets of the underlying
training data is an effective means of justifying complex model predictions. This poses inherent
privacy risks. A necessary complement to explanation-by-example are techniques to anonymize and
sanitize personally identiﬁable information from the revealed training data. Thankfully, the active
body of literature surrounding differential privacy offers techniques that can be employed when
needed to ensure these privacy violations do not occur. It is critical that the noble effort to explain
complex models do not unintentionally harm the individuals potentially comprising the training data.

One possible side-effect of this paper is that researchers may be discouraged from advancing explain-
ability if faced with a published relative ranking. On the very contrary, we instead hope it inspires
novel solutions and brings increased attention to the importance of this problem. This paper is by no
means meant to provide closure, but instead serve as a stepping stone detailing the current landscape
for community reﬂection and reevaluation.

Moving Forward. The only way that we can ensure that our community does not bring harm to the
public is through transparency and honesty. Of course, this is impossible without an effective means
of mutual communication — one that can be understood by all. This paper is an attempt to elucidate
the methods and styles of presentation that offer this universal language by which we can create a
more informed society.

","Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, Mani Srivastava",How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods,1.0,"{'UCLA', 'University of California, Los Angeles'}",,Deep learning,{'USA'},False,False,"We would like to begin by praising the community for it’s recent shift of attention to the study and design of high-quality model explanations, particularly when attempting to provide insight into inferences that cannot be easily interpreted. This essential direction of research indicates that scientists and technologists care about making complex methods more accessible and understandable. Encouraging a world where the advanced computational techniques used to influence modern society are generally understandable empowers us all to steer its direction with clearer vision toward a more desirable state. At the very least, we can be more prepared to avoid negative outcomes. Fear of the Unknown. As deep neural networks have brought performance improvements that were seemingly unimaginable, the general public remains largely ignorant of what makes these approaches so powerful; to them, it’s equivalent to magic. This is popularly conveyed in sensational headlines and the fictional portrayals of dystopian futures, laying somewhere at the intersection of Black Mirror , Orwell’s 1984 , and Terminator ’s Skynet. Nevertheless, these fears underlay legitimate concerns that motivate privacy preserving regulations including GDPR and CCPA, not to mention the inclusion of the NeurIPS Broader Impact Statement. The continuing efforts to research effective methods of ensuring explainability of these models can help partially alleviate this tension, particular if we succeed in providing an honest (yet comprehensible) representation of the underlying method. This is the motivation for our work. Positive Impact. The stated objective and ideal impact of this paper is relatively straightforward; we intend to empower the community with knowledge of the average end-user perspective. In this way, we might all have a better understanding of the right approach to increase transparency and effectively communicate with the public, thereby offering a bridge between the technologist and non-technical layperson.  Unintended Risks. One of our research conclusions is that exposing subsets of the underlying training data is an effective means of justifying complex model predictions. This poses inherent privacy risks. A necessary complement to explanation-by-example are techniques to anonymize and sanitize personally identifiable information from the revealed training data. Thankfully, the active body of literature surrounding differential privacy offers techniques that can be employed when needed to ensure these privacy violations do not occur. It is critical that the noble effort to explain complex models do not unintentionally harm the individuals potentially comprising the training data. One possible side-effect of this paper is that researchers may be discouraged from advancing explainability if faced with a published relative ranking. On the very contrary, we instead hope it inspires novel solutions and brings increased attention to the importance of this problem. This paper is by no means meant to provide closure, but instead serve as a stepping stone detailing the current landscape for community reflection and reevaluation. Moving Forward. The only way that we can ensure that our community does not bring harm to the public is through transparency and honesty. Of course, this is impossible without an effective means of mutual communication — one that can be understood by all. This paper is an attempt to elucidate the methods and styles of presentation that offer this universal language by which we can create a more informed society.",Broader Impact,1.0,False,1.0,,"Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, Mani Srivastava",2c29d89cc56cdb191c60db2f0bae796b,https://proceedings.neurips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf,How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods,How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods,Deep Learning,"Deep Learning -> Analysis and Understanding of Deep Networks; Deep Learning -> Visualization, Interpretability, and Explainability",26.0,"
We would like to begin by praising the community for it’s recent shift of attention to the study
and design of high-quality model explanations, particularly when attempting to provide insight
into inferences that cannot be easily interpreted. This essential direction of research indicates that
scientists and technologists care about making complex methods more accessible and understandable.
Encouraging a world where the advanced computational techniques used to inﬂuence modern society
are generally understandable empowers us all to steer its direction with clearer vision toward a more
desirable state. At the very least, we can be more prepared to avoid negative outcomes.

Fear of the Unknown. As deep neural networks have brought performance improvements that were
seemingly unimaginable, the general public remains largely ignorant of what makes these approaches
so powerful; to them, it’s equivalent to magic. This is popularly conveyed in sensational headlines
and the ﬁctional portrayals of dystopian futures, laying somewhere at the intersection of Black Mirror,
Orwell’s 1984, and Terminator’s Skynet. Nevertheless, these fears underlay legitimate concerns that
motivate privacy preserving regulations including GDPR and CCPA, not to mention the inclusion
of the NeurIPS Broader Impact Statement. The continuing efforts to research effective methods
of ensuring explainability of these models can help partially alleviate this tension, particular if we
succeed in providing an honest (yet comprehensible) representation of the underlying method. This
is the motivation for our work.

Positive Impact. The stated objective and ideal impact of this paper is relatively straightforward;
we intend to empower the community with knowledge of the average end-user perspective. In this
way, we might all have a better understanding of the right approach to increase transparency and
effectively communicate with the public, thereby offering a bridge between the technologist and
non-technical layperson.

Unintended Risks. One of our research conclusions is that exposing subsets of the underlying
training data is an effective means of justifying complex model predictions. This poses inherent
privacy risks. A necessary complement to explanation-by-example are techniques to anonymize and
sanitize personally identiﬁable information from the revealed training data. Thankfully, the active
body of literature surrounding differential privacy offers techniques that can be employed when
needed to ensure these privacy violations do not occur. It is critical that the noble effort to explain
complex models do not unintentionally harm the individuals potentially comprising the training data.

One possible side-effect of this paper is that researchers may be discouraged from advancing explain-
ability if faced with a published relative ranking. On the very contrary, we instead hope it inspires
novel solutions and brings increased attention to the importance of this problem. This paper is by no
means meant to provide closure, but instead serve as a stepping stone detailing the current landscape
for community reﬂection and reevaluation.

Moving Forward. The only way that we can ensure that our community does not bring harm to the
public is through transparency and honesty. Of course, this is impossible without an effective means
of mutual communication — one that can be understood by all. This paper is an attempt to elucidate
the methods and styles of presentation that offer this universal language by which we can create a
more informed society.

",220,How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods,https://papers.nips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf,528.0
221,"
This work contributes towards making neural networks more robust to adversarial examples. This is a
crucial roadblock before neural networks can be widely adopted in safety-critical applications like self-
driving cars and smart grids. The ultimate, overarching goal is to take the high performance of neural
networks—already enjoyed by applications in computer vision and natural language processing—and
extend towards applications in societal infrastructure.

Towards this direction, SDP relaxations allow us to make mathematical guarantees on the robustness
of a given neural network model. However, a blind reliance on mathematical guarantees leads to a
false sense of security. While this work contributes towards robustness of neural networks, much
more work is needed to understand the appropriateness of neural networks for societal applications in
the ﬁrst place.
",Richard Zhang,On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples,1.0,{'UIUC'},,Theory (including computational and statistical analyses),{'USA'},False,False,"This work contributes towards making neural networks more robust to adversarial examples. This is a crucial roadblock before neural networks can be widely adopted in safety-critical applications like self- driving cars and smart grids. The ultimate, overarching goal is to take the high performance of neural networks—already enjoyed by applications in computer vision and natural language processing—and extend towards applications in societal infrastructure. Towards this direction, SDP relaxations allow us to make mathematical guarantees on the robustness of a given neural network model. However, a blind reliance on mathematical guarantees leads to a false sense of security. While this work contributes towards robustness of neural networks, much more work is needed to understand the appropriateness of neural networks for societal applications in the first place.",Broader Impact,0.0,False,0.0,,Richard Zhang,27b587bbe83aecf9a98c8fe6ab48cacc,https://proceedings.neurips.cc/paper/2020/file/27b587bbe83aecf9a98c8fe6ab48cacc-Paper.pdf,On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples,On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples,Optimization,Algorithms -> Adversarial Learning; Deep Learning -> Analysis and Understanding of Deep Networks; Deep Learning -> Optimization for Deep Networks; Optimization -> Convex Optimization; Optimization -> Non-Convex Optimization,6.0,"
This work contributes towards making neural networks more robust to adversarial examples. This is a
crucial roadblock before neural networks can be widely adopted in safety-critical applications like self-
driving cars and smart grids. The ultimate, overarching goal is to take the high performance of neural
networks—already enjoyed by applications in computer vision and natural language processing—and
extend towards applications in societal infrastructure.

Towards this direction, SDP relaxations allow us to make mathematical guarantees on the robustness
of a given neural network model. However, a blind reliance on mathematical guarantees leads to a
false sense of security. While this work contributes towards robustness of neural networks, much
more work is needed to understand the appropriateness of neural networks for societal applications in
the ﬁrst place.
",221,On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples,https://papers.nips.cc/paper/2020/file/27b587bbe83aecf9a98c8fe6ab48cacc-Paper.pdf,125.0
223,"
We believe that researchers of multi-agent reinforcement learning (MARL), especially those who
are interested in the theoretical foundations of MARL, would beneﬁt from this work. In particular,
prior to this work, though intuitive and widely-used, the sample efﬁciency, speciﬁcally the minimax
optimality of the sample complexity, of this model-based approach had not been established for
MARL. This work justiﬁed the efﬁciency of this simple method for the ﬁrst time in the MARL
setting. We have also raised several important open questions on the sample complexity of MARL in
zero-sum Markov games in general, which open up some future research directions toward rigorous
theoretical understandings of MARL. In contrast to the rich literature on the theory of model-free
MARL algorithms, the theory of model-based ones is relatively lacking. Our results have advocated
the use of model-based MARL due to its sample efﬁciency, which would beneﬁt MARL practitioners
when choosing between the two types of algorithms in practice. As a theory-oriented work, we do
not believe that our research will cause any ethical issue, or put anyone at any disadvantage.

","Kaiqing Zhang, Sham Kakade, Tamer Basar, Lin Yang",Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity,1.0,"{'UCLA', 'University of Illinois at Urbana-Champaign', 'UIUC', 'University of Washington'}",,Reinforcement learning and planning,{'USA'},False,False,"We believe that researchers of multi-agent reinforcement learning (MARL), especially those who are interested in the theoretical foundations of MARL, would benefit from this work. In particular, prior to this work, though intuitive and widely-used, the sample efficiency, specifically the minimax optimality of the sample complexity, of this model-based approach had not been established for MARL. This work justified the efficiency of this simple method for the first time in the MARL setting. We have also raised several important open questions on the sample complexity of MARL in zero-sum Markov games in general, which open up some future research directions toward rigorous theoretical understandings of MARL. In contrast to the rich literature on the theory of model-free MARL algorithms, the theory of model-based ones is relatively lacking. Our results have advocated the use of model-based MARL due to its sample efficiency, which would benefit MARL practitioners when choosing between the two types of algorithms in practice. As a theory-oriented work, we do not believe that our research will cause any ethical issue, or put anyone at any disadvantage.",Broader Impact,0.0,False,0.0,,"Kaiqing Zhang, Sham Kakade, Tamer Basar, Lin Yang",0cc6ee01c82fc49c28706e0918f57e2d,https://proceedings.neurips.cc/paper/2020/file/0cc6ee01c82fc49c28706e0918f57e2d-Paper.pdf,Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity,Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity,Reinforcement Learning and Planning -> Multi-Agent RL,Reinforcement Learning and Planning -> Model-Based RL; Theory -> Game Theory and Computational Economics; Theory -> Statistical Learning Theory,7.0,"
We believe that researchers of multi-agent reinforcement learning (MARL), especially those who
are interested in the theoretical foundations of MARL, would beneﬁt from this work. In particular,
prior to this work, though intuitive and widely-used, the sample efﬁciency, speciﬁcally the minimax
optimality of the sample complexity, of this model-based approach had not been established for
MARL. This work justiﬁed the efﬁciency of this simple method for the ﬁrst time in the MARL
setting. We have also raised several important open questions on the sample complexity of MARL in
zero-sum Markov games in general, which open up some future research directions toward rigorous
theoretical understandings of MARL. In contrast to the rich literature on the theory of model-free
MARL algorithms, the theory of model-based ones is relatively lacking. Our results have advocated
the use of model-based MARL due to its sample efﬁciency, which would beneﬁt MARL practitioners
when choosing between the two types of algorithms in practice. As a theory-oriented work, we do
not believe that our research will cause any ethical issue, or put anyone at any disadvantage.

",223,Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity,https://papers.nips.cc/paper/2020/file/0cc6ee01c82fc49c28706e0918f57e2d-Paper.pdf,178.0
227,"
Understanding the dynamics for individuals who attempt to change and maintain behaviors to improve
health has important societal value, for example, a comprehensive understanding of how smokers
attempt to quit smoking may guide behavioral scientists to design better intervention strategies
that tailor to the highest risk windows of relapse. Our theory and method provide an approach to
understanding a particular aspect of the smoking behavior (mean function). The resulting algoithm
is robust to Poisson process violations, readily adaptable and simple to implement, highlighting the
potential for its wider adoption. The negative use case could be lack of sensitivity analysis around the
assumptions such as missing data mechanism which may lead to misleading conclusions. Our current
recommendation is to consult scientists about the plausibility of the assumption about missing data.","Alexander Moreno, Zhenke Wu, Jamie Roslyn Yap, Cho Lam, David Wetter, Inbal Nahum-Shani, Walter Dempsey, James M. Rehg",A Robust Functional EM Algorithm for Incomplete Panel Count Data,1.0,"{'University of Michigan', 'Georgia Institute of Technology', 'Georgia Tech', 'University of Utah'}",,,{'USA'},False,False,"Understanding the dynamics for individuals who attempt to change and maintain behaviors to improve health has important societal value, for example, a comprehensive understanding of how smokers attempt to quit smoking may guide behavioral scientists to design better intervention strategies that tailor to the highest risk windows of relapse. Our theory and method provide an approach to understanding a particular aspect of the smoking behavior (mean function). The resulting algoithm is robust to Poisson process violations, readily adaptable and simple to implement, highlighting the potential for its wider adoption. The negative use case could be lack of sensitivity analysis around the assumptions such as missing data mechanism which may lead to misleading conclusions. Our current recommendation is to consult scientists about the plausibility of the assumption about missing data.",Broader Impact,0.0,False,0.0,,"Alexander Moreno, Zhenke Wu, Jamie Roslyn Yap, Cho Lam, David Wetter, Inbal Nahum-Shani, Walter Dempsey, James M. Rehg",e56eea9a45b153de634b23780365f976,https://proceedings.neurips.cc/paper/2020/file/e56eea9a45b153de634b23780365f976-Paper.pdf,A Robust Functional EM Algorithm for Incomplete Panel Count Data,A Robust Functional EM Algorithm for Incomplete Panel Count Data,Theory -> Spaces of Functions and Kernels,Algorithms -> Missing Data; Applications -> Health; Optimization -> Non-Convex Optimization,5.0,"
Understanding the dynamics for individuals who attempt to change and maintain behaviors to improve
health has important societal value, for example, a comprehensive understanding of how smokers
attempt to quit smoking may guide behavioral scientists to design better intervention strategies
that tailor to the highest risk windows of relapse. Our theory and method provide an approach to
understanding a particular aspect of the smoking behavior (mean function). The resulting algoithm
is robust to Poisson process violations, readily adaptable and simple to implement, highlighting the
potential for its wider adoption. The negative use case could be lack of sensitivity analysis around the
assumptions such as missing data mechanism which may lead to misleading conclusions. Our current
recommendation is to consult scientists about the plausibility of the assumption about missing data.",227,A Robust Functional EM Algorithm for Incomplete Panel Count Data,https://papers.nips.cc/paper/2020/file/e56eea9a45b153de634b23780365f976-Paper.pdf,129.0
232,"
This is a theoretical paper, so discussion of broader impacts has to speculate on future applications
for this broad line of research. In particular, this paper considers active regression that is robust and
sample-efﬁcient. In other words, we advance our understanding of learning patterns when samples
are expensive and noise is large. We speculate on the various impacts of machine learning in these
high-sample-cost and high-noise settings.

One of the most common guiding positive beneﬁts is the beneﬁt to medical imaging, where instru-
ments have biased noisy measurements and are expensive to run. So, having ML techniques with
low sample complexity and high robustness can give medical practitioners high conﬁdence in their
medical conclusions while keeping costs low.

However, this broad framework can just as easily apply to large-scale illegal surveillance through
“Internet of Things” (IoT) devices. Many IoT devices are known to have cheap microphones, weak
security, and internet access. Well crafted internet crawling code could plausibly access a massive
number of such IoT devices, and hence be able to access a massive number of private microphones.

Without progress in our line of research, it may be prohibitively expensive to process or store all
the speech heard from all of these microphones. Alternatively, the internet download patterns might
be too noticeable for such an operation to secretly run at a large scale. However, progress toward
statistically efﬁcient algorithms in the high-sample-cost and high-noise regime might allow code that
sends very few messages online while still transmitting all the interesting audio to a third party. This
would allow such illegal surveillance on a massive scale.

Both of these settings, the medical and the surveillance, look equivalent from our current level of
mathematical/statistical abstraction. Notably, our current research is still too abstract to directly
beneﬁt either application. It will require more research to connect results like ours to either the
medical or the surveillance applications. Note that the speciﬁc examples of medical imaging and IoT
surveillance are just two examples of highly positive or highly negative ML applications. So, under
the assumption that the research community will focus on positive applications like medical imaging
while avoiding negative applications like large scale IoT surveillance, we believe that the beneﬁts of
our theoretical research outweigh the negative potential ramiﬁcations. Admittedly, this may be an
optimistic assumption.

","Raphael Meyer, Christopher Musco",The Statistical Cost of Robust Kernel Hyperparameter Turning,,,,,,,,,,,,,,,,,,,,,,"
This is a theoretical paper, so discussion of broader impacts has to speculate on future applications
for this broad line of research. In particular, this paper considers active regression that is robust and
sample-efﬁcient. In other words, we advance our understanding of learning patterns when samples
are expensive and noise is large. We speculate on the various impacts of machine learning in these
high-sample-cost and high-noise settings.

One of the most common guiding positive beneﬁts is the beneﬁt to medical imaging, where instru-
ments have biased noisy measurements and are expensive to run. So, having ML techniques with
low sample complexity and high robustness can give medical practitioners high conﬁdence in their
medical conclusions while keeping costs low.

However, this broad framework can just as easily apply to large-scale illegal surveillance through
“Internet of Things” (IoT) devices. Many IoT devices are known to have cheap microphones, weak
security, and internet access. Well crafted internet crawling code could plausibly access a massive
number of such IoT devices, and hence be able to access a massive number of private microphones.

Without progress in our line of research, it may be prohibitively expensive to process or store all
the speech heard from all of these microphones. Alternatively, the internet download patterns might
be too noticeable for such an operation to secretly run at a large scale. However, progress toward
statistically efﬁcient algorithms in the high-sample-cost and high-noise regime might allow code that
sends very few messages online while still transmitting all the interesting audio to a third party. This
would allow such illegal surveillance on a massive scale.

Both of these settings, the medical and the surveillance, look equivalent from our current level of
mathematical/statistical abstraction. Notably, our current research is still too abstract to directly
beneﬁt either application. It will require more research to connect results like ours to either the
medical or the surveillance applications. Note that the speciﬁc examples of medical imaging and IoT
surveillance are just two examples of highly positive or highly negative ML applications. So, under
the assumption that the research community will focus on positive applications like medical imaging
while avoiding negative applications like large scale IoT surveillance, we believe that the beneﬁts of
our theoretical research outweigh the negative potential ramiﬁcations. Admittedly, this may be an
optimistic assumption.

",232,,https://papers.nips.cc/paper/2020/file/f4661398cb1a3abd3ffe58600bf11322-Paper.pdf,
237,"
In this paper, we offer a new interpretation of the self-distillation training framework, a commonly
used technique for improved accuracy used among practitioners in the deep learning community,
which allows us to gain some deeper understanding of the reasons for its success. With the ubiquity
of deep learning in our society today and countless potential future applications of it, we believe our
work can potentially bring positive impacts in several ways.

Firstly, despite the empirical utility of distillation and numerous successful applications in many
tasks and applications ranging from computer vision to natural language processing problems, we
still lack a thorough understanding of why it works. In our opinion, blindly applying methods and
algorithms without a good grasp on the underlying mechanisms can be dangerous. Our perspective
offers a theoretically grounded explanation for its success that allows us to apply the techniques to
real-world applications broadly with greater conﬁdence.

In addition, the proposed interpretation of distillation as a regularization to neural networks can
potentially allow us to obtain models that are more generalizable and reliable. This is an extremely
important aspect of applying deep learning to sensitive domains like healthcare and autonomous
driving, in which wrong predictions made by machines can lead to catastrophic consequences.
Moreover, our new experimental demonstration that models trained with the distillation process can
potentially lead to better-calibrated models that can facilitate safer and more interpretable applications
of neural networks. Indeed, for real-world classiﬁcation tasks like disease diagnosis, in addition to
accurate predictions, we need reliable estimates of the level of conﬁdence of the predictions made,
which is something that neural networks are lacking currently as pointed out by recent research. More
calibrated models, in our opinion, enhances the explainability and transparency of neural network
models.

Lastly, we believe the introduced framework can stimulate further research on the regularization of
deep learning models for better generalization and thus safer applications. It was recently demon-
strated that deep neural networks do not seem to suffer from overﬁtting. Our ﬁnding suggests that
overﬁtting can still occur, though in a different way than conventional wisdom, and deep learning can
still beneﬁt from regularization. As such, we encourage research into more efﬁcient and principled
forms of regularization to improve upon the distillation strategy.

We acknowledge the risks associated with our work. To be more speciﬁc, our ﬁnding advocates for
the use of priors for the regularization of neural networks. Despite the potentially better generalization
performance of trained models, depending on the choice of priors used for training, unwanted bias
can be inevitably introduced into the deep learning system, potentially causing issues of fairness and
privacy.

","Zhilu Zhang, Mert Sabuncu",Self-Distillation as Instance-Specific Label Smoothing,1.0,"{'Cornell', 'Cornell University'}",,Deep learning,{'USA'},False,False,"In this paper, we offer a new interpretation of the self-distillation training framework, a commonly used technique for improved accuracy used among practitioners in the deep learning community, which allows us to gain some deeper understanding of the reasons for its success. With the ubiquity of deep learning in our society today and countless potential future applications of it, we believe our work can potentially bring positive impacts in several ways. Firstly, despite the empirical utility of distillation and numerous successful applications in many tasks and applications ranging from computer vision to natural language processing problems, we still lack a thorough understanding of why it works. In our opinion, blindly applying methods and algorithms without a good grasp on the underlying mechanisms can be dangerous. Our perspective offers a theoretically grounded explanation for its success that allows us to apply the techniques to real-world applications broadly with greater confidence. In addition, the proposed interpretation of distillation as a regularization to neural networks can potentially allow us to obtain models that are more generalizable and reliable. This is an extremely important aspect of applying deep learning to sensitive domains like healthcare and autonomous driving, in which wrong predictions made by machines can lead to catastrophic consequences. Moreover, our new experimental demonstration that models trained with the distillation process can potentially lead to better-calibrated models that can facilitate safer and more interpretable applications of neural networks. Indeed, for real-world classification tasks like disease diagnosis, in addition to accurate predictions, we need reliable estimates of the level of confidence of the predictions made, which is something that neural networks are lacking currently as pointed out by recent research. More calibrated models, in our opinion, enhances the explainability and transparency of neural network models. Lastly, we believe the introduced framework can stimulate further research on the regularization of deep learning models for better generalization and thus safer applications. It was recently demon- strated that deep neural networks do not seem to suffer from overfitting. Our finding suggests that overfitting can still occur, though in a different way than conventional wisdom, and deep learning can still benefit from regularization. As such, we encourage research into more efficient and principled forms of regularization to improve upon the distillation strategy. We acknowledge the risks associated with our work. To be more specific, our finding advocates for the use of priors for the regularization of neural networks. Despite the potentially better generalization performance of trained models, depending on the choice of priors used for training, unwanted bias can be inevitably introduced into the deep learning system, potentially causing issues of fairness and privacy.",Statement of the Potential Broader Impact,0.0,False,0.0,,"Zhilu Zhang, Mert Sabuncu",1731592aca5fb4d789c4119c65c10b4b,https://proceedings.neurips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Paper.pdf,Self-Distillation as Instance-Specific Label Smoothing,Self-Distillation as Instance-Specific Label Smoothing,Deep Learning,Deep Learning -> Analysis and Understanding of Deep Networks; Deep Learning -> Optimization for Deep Networks; Deep Learning -> Supervised Deep Networks,17.0,"
In this paper, we offer a new interpretation of the self-distillation training framework, a commonly
used technique for improved accuracy used among practitioners in the deep learning community,
which allows us to gain some deeper understanding of the reasons for its success. With the ubiquity
of deep learning in our society today and countless potential future applications of it, we believe our
work can potentially bring positive impacts in several ways.

Firstly, despite the empirical utility of distillation and numerous successful applications in many
tasks and applications ranging from computer vision to natural language processing problems, we
still lack a thorough understanding of why it works. In our opinion, blindly applying methods and
algorithms without a good grasp on the underlying mechanisms can be dangerous. Our perspective
offers a theoretically grounded explanation for its success that allows us to apply the techniques to
real-world applications broadly with greater conﬁdence.

In addition, the proposed interpretation of distillation as a regularization to neural networks can
potentially allow us to obtain models that are more generalizable and reliable. This is an extremely
important aspect of applying deep learning to sensitive domains like healthcare and autonomous
driving, in which wrong predictions made by machines can lead to catastrophic consequences.
Moreover, our new experimental demonstration that models trained with the distillation process can
potentially lead to better-calibrated models that can facilitate safer and more interpretable applications
of neural networks. Indeed, for real-world classiﬁcation tasks like disease diagnosis, in addition to
accurate predictions, we need reliable estimates of the level of conﬁdence of the predictions made,
which is something that neural networks are lacking currently as pointed out by recent research. More
calibrated models, in our opinion, enhances the explainability and transparency of neural network
models.

Lastly, we believe the introduced framework can stimulate further research on the regularization of
deep learning models for better generalization and thus safer applications. It was recently demon-
strated that deep neural networks do not seem to suffer from overﬁtting. Our ﬁnding suggests that
overﬁtting can still occur, though in a different way than conventional wisdom, and deep learning can
still beneﬁt from regularization. As such, we encourage research into more efﬁcient and principled
forms of regularization to improve upon the distillation strategy.

We acknowledge the risks associated with our work. To be more speciﬁc, our ﬁnding advocates for
the use of priors for the regularization of neural networks. Despite the potentially better generalization
performance of trained models, depending on the choice of priors used for training, unwanted bias
can be inevitably introduced into the deep learning system, potentially causing issues of fairness and
privacy.

",237,Self-Distillation as Instance-Specific Label Smoothing,https://papers.nips.cc/paper/2020/file/1731592aca5fb4d789c4119c65c10b4b-Paper.pdf,435.0
238,"
This work proposes FEDAC, a principled acceleration of FEDAVG, which provably improves conver-
gence speed and communication efﬁciency. Our theory and experiments suggest that FEDAC saves
computational resources and reduces communication overhead, especially in the setting of abundant
workers and infrequent communication. Our analysis could promote a better understanding of feder-
ated / distributed optimization and acceleration theory. We expect FEDAC could be generalized to
broader settings, e.g., non-convex objective and/or heterogenous workers.

The opportunity for privacy-preserving learning is another advantage of Federated Learning beyond
parallelization, since the user data are kept local during learning. While we do not analyze the privacy
guarantee in this work, we conjecture that FEDAC could potentially enjoy better privacy-preserving
property since less communication is required to achieve the same accuracy. However, this intuition
should be applied with caution for high-risk data until theoretical privacy guarantee is established.

","Honglin Yuan, Tengyu Ma",Federated Accelerated Stochastic Gradient Descent,1.0,{'Stanford University'},,Optimization Methods (continuous or discrete),{'USA'},False,False,"This work proposes F ED A C , a principled acceleration of F ED A VG , which provably improves conver- gence speed and communication efficiency. Our theory and experiments suggest that F ED A C saves computational resources and reduces communication overhead, especially in the setting of abundant workers and infrequent communication. Our analysis could promote a better understanding of feder- ated / distributed optimization and acceleration theory. We expect F ED A C could be generalized to broader settings, e.g. , non-convex objective and/or heterogenous workers. The opportunity for privacy-preserving learning is another advantage of Federated Learning beyond parallelization, since the user data are kept local during learning. While we do not analyze the privacy guarantee in this work, we conjecture that F ED A C could potentially enjoy better privacy-preserving property since less communication is required to achieve the same accuracy. However, this intuition should be applied with caution for high-risk data until theoretical privacy guarantee is established.",Broader Impact,0.0,False,0.0,,"Honglin Yuan, Tengyu Ma",39d0a8908fbe6c18039ea8227f827023,https://proceedings.neurips.cc/paper/2020/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf,Federated Accelerated Stochastic Gradient Descent,Federated Accelerated Stochastic Gradient Descent,Optimization -> Stochastic Optimization,Algorithms -> Communication- or Memory-Bounded Learning; Optimization -> Convex Optimization,7.0,"
This work proposes FEDAC, a principled acceleration of FEDAVG, which provably improves conver-
gence speed and communication efﬁciency. Our theory and experiments suggest that FEDAC saves
computational resources and reduces communication overhead, especially in the setting of abundant
workers and infrequent communication. Our analysis could promote a better understanding of feder-
ated / distributed optimization and acceleration theory. We expect FEDAC could be generalized to
broader settings, e.g., non-convex objective and/or heterogenous workers.

The opportunity for privacy-preserving learning is another advantage of Federated Learning beyond
parallelization, since the user data are kept local during learning. While we do not analyze the privacy
guarantee in this work, we conjecture that FEDAC could potentially enjoy better privacy-preserving
property since less communication is required to achieve the same accuracy. However, this intuition
should be applied with caution for high-risk data until theoretical privacy guarantee is established.

",238,Federated Accelerated Stochastic Gradient Descent,https://papers.nips.cc/paper/2020/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf,161.0
243,"
Our research falls under the category of advancing machine learning techniques for computer vision
and scene understanding. We focus on improving image representations for dense prediction tasks,
which subsumes a large array of fundamental vision tasks, such as image segmentation and object
detection. While there are potentially many implications for using these applications, here we discuss
two aspects. First, we highlight some social implications for image understanding with no or very
little labeled data. Second, we provide some insights on foundational research questions regarding the
evaluation of general purpose representation learning methods.

Improving capabilities of image understanding using unlabeled data, especially for pixel-level tasks,
opens up a wide range of applications that are beneﬁcial to the society, and which cannot be tackled
otherwise. Medical imagery applications suffers from lack of labeled data due to the need of very
specialized labelers. Another application, tackling harmful online content—including but not limited
to terrorist propaganda, hateful speech, fake news and misinformation—is a huge challenge for
governments and businesses. What makes these problems especially difﬁcult is that it is very difﬁcult
to obtain clean labeled data for training machine learning models—think of ﬁlming a terrorist attack on
live video as in the unfortunate Christchurch attack. Self-supervised learning can potentially move the
needle in advancing models for detecting extremely rare yet highly impactful incidents. On the other
hand, such technologies can be potentially misused for violating privacy and freedom of expression.
We acknowledge these risks as being a feature of any amoral technology, and we invite governments,
policy makers and all citizens—including the research community—to work hard on striking a balance
between those beneﬁts and risks.

Another interesting aspect of our research is highlighting the importance of aligning representation
learning methods with the nature of downstream applications. With our method, we show that learning
pixel-level representations from unlabeled data we can outperform image-level methods on a variety of
dense prediction tasks. Our ﬁndings highlight that the research community should go beyond limited
test-beds for evaluating generic representation learning techniques. We invite further research on
developing comprehensive evaluation protocols for such methods. In fact, we see many research
opportunities in the computer vision domain, such as developing a sweep of standardized benchmarks
across a variety of geometric and semantic image understanding tasks, and designing methods that can
bridge the gap between ofﬂine and online performance.

","Pedro O. O. Pinheiro, Amjad Almahairi, Ryan Benmalek, Florian Golemo, Aaron C. Courville",Unsupervised Learning of Dense Visual Representations,1.0,"{'MILA / ElementAI', 'Element AI', 'Cornell University'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)","{'Canada', 'USA'}",False,False,"Our research falls under the category of advancing machine learning techniques for computer vision and scene understanding. We focus on improving image representations for dense prediction tasks, which subsumes a large array of fundamental vision tasks, such as image segmentation and object detection. While there are potentially many implications for using these applications, here we discuss two aspects. First, we highlight some social implications for image understanding with no or very little labeled data. Second, we provide some insights on foundational research questions regarding the evaluation of general purpose representation learning methods. Improving capabilities of image understanding using unlabeled data, especially for pixel-level tasks, opens up a wide range of applications that are beneficial to the society, and which cannot be tackled otherwise. Medical imagery applications suffers from lack of labeled data due to the need of very specialized labelers. Another application, tackling harmful online content—including but not limited to terrorist propaganda, hateful speech, fake news and misinformation—is a huge challenge for governments and businesses. What makes these problems especially difficult is that it is very difficult to obtain clean labeled data for training machine learning models—think of filming a terrorist attack on live video as in the unfortunate Christchurch attack. Self-supervised learning can potentially move the needle in advancing models for detecting extremely rare yet highly impactful incidents. On the other hand, such technologies can be potentially misused for violating privacy and freedom of expression. We acknowledge these risks as being a feature of any amoral technology, and we invite governments, policy makers and all citizens—including the research community—to work hard on striking a balance between those benefits and risks. Another interesting aspect of our research is highlighting the importance of aligning representation learning methods with the nature of downstream applications. With our method, we show that learning pixel-level representations from unlabeled data we can outperform image-level methods on a variety of dense prediction tasks. Our findings highlight that the research community should go beyond limited test-beds for evaluating generic representation learning techniques. We invite further research on developing comprehensive evaluation protocols for such methods. In fact, we see many research opportunities in the computer vision domain, such as developing a sweep of standardized benchmarks across a variety of geometric and semantic image understanding tasks, and designing methods that can bridge the gap between offline and online performance.",6 Broader Impact,1.0,False,1.0,,"Pedro O. O. Pinheiro, Amjad Almahairi, Ryan Benmalek, Florian Golemo, Aaron C. Courville",3000311ca56a1cb93397bc676c0b7fff,https://proceedings.neurips.cc/paper/2020/file/3000311ca56a1cb93397bc676c0b7fff-Paper.pdf,Unsupervised Learning of Dense Visual Representations,Unsupervised Learning of Dense Visual Representations,Algorithms -> Representation Learning,Algorithms -> Unsupervised Learning; Applications -> Computer Vision,17.0,"
Our research falls under the category of advancing machine learning techniques for computer vision
and scene understanding. We focus on improving image representations for dense prediction tasks,
which subsumes a large array of fundamental vision tasks, such as image segmentation and object
detection. While there are potentially many implications for using these applications, here we discuss
two aspects. First, we highlight some social implications for image understanding with no or very
little labeled data. Second, we provide some insights on foundational research questions regarding the
evaluation of general purpose representation learning methods.

Improving capabilities of image understanding using unlabeled data, especially for pixel-level tasks,
opens up a wide range of applications that are beneﬁcial to the society, and which cannot be tackled
otherwise. Medical imagery applications suffers from lack of labeled data due to the need of very
specialized labelers. Another application, tackling harmful online content—including but not limited
to terrorist propaganda, hateful speech, fake news and misinformation—is a huge challenge for
governments and businesses. What makes these problems especially difﬁcult is that it is very difﬁcult
to obtain clean labeled data for training machine learning models—think of ﬁlming a terrorist attack on
live video as in the unfortunate Christchurch attack. Self-supervised learning can potentially move the
needle in advancing models for detecting extremely rare yet highly impactful incidents. On the other
hand, such technologies can be potentially misused for violating privacy and freedom of expression.
We acknowledge these risks as being a feature of any amoral technology, and we invite governments,
policy makers and all citizens—including the research community—to work hard on striking a balance
between those beneﬁts and risks.

Another interesting aspect of our research is highlighting the importance of aligning representation
learning methods with the nature of downstream applications. With our method, we show that learning
pixel-level representations from unlabeled data we can outperform image-level methods on a variety of
dense prediction tasks. Our ﬁndings highlight that the research community should go beyond limited
test-beds for evaluating generic representation learning techniques. We invite further research on
developing comprehensive evaluation protocols for such methods. In fact, we see many research
opportunities in the computer vision domain, such as developing a sweep of standardized benchmarks
across a variety of geometric and semantic image understanding tasks, and designing methods that can
bridge the gap between ofﬂine and online performance.

",243,Unsupervised Learning of Dense Visual Representations,https://papers.nips.cc/paper/2020/file/3000311ca56a1cb93397bc676c0b7fff-Paper.pdf,389.0
244,"
We believe that it is difﬁcult to clearly foresee societal consequence of the present, purely theoretical,
work. The results presented inscribe themselves in the larger theme of providing guidelines for
better and parsimonious use of data when possible, for example when learning a sparse rule. On the
long run, such guidelines must be taken into account for building engineering systems that are more
efﬁcient in terms of computational and energetic cost.

","Clément Luneau, jean barbier, Nicolas Macris",Information theoretic limits of learning a sparse rule,1.0,"{'École Polytechnique de Lausanne', 'EPFL'}",,Theory (including computational and statistical analyses),{'Switzerland'},False,False,"We believe that it is difficult to clearly foresee societal consequence of the present, purely theoretical, work. The results presented inscribe themselves in the larger theme of providing guidelines for better and parsimonious use of data when possible, for example when learning a sparse rule. On the long run, such guidelines must be taken into account for building engineering systems that are more efficient in terms of computational and energetic cost.",Broader Impact,0.0,False,0.0,,"Clément Luneau, jean barbier, Nicolas Macris",713fd63d76c8a57b16fc433fb4ae718a,https://proceedings.neurips.cc/paper/2020/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf,Information theoretic limits of learning a sparse rule,Information theoretic limits of learning a sparse rule,Theory -> Statistical Physics of Learning,Algorithms -> Sparsity and Compressed Sensing; Probabilistic Methods -> Bayesian Theory; Theory -> Information Theory,3.0,"
We believe that it is difﬁcult to clearly foresee societal consequence of the present, purely theoretical,
work. The results presented inscribe themselves in the larger theme of providing guidelines for
better and parsimonious use of data when possible, for example when learning a sparse rule. On the
long run, such guidelines must be taken into account for building engineering systems that are more
efﬁcient in terms of computational and energetic cost.

",244,Information theoretic limits of learning a sparse rule,https://papers.nips.cc/paper/2020/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf,71.0
246,"
In this work we design algorithms with provable robustness guarantees in the challenging setting
where the level of noise is allowed to vary across the domain. This models several scenarios of
interest, most notably situations where data provided by certain demographic groups is subject to
more noise than others. In a natural experiment on the UCI Adult dataset, we show that coping with
this type of noise can help mitigate some natural types of unfairness that arise with off-the-shelf
algorithms. Moreover our algorithms have the additional beneﬁt that they lead to more readily
interpretable hypotheses. In many settings of interest, we are able to give proper learning algorithms
(where previously only improper learning algorithms were known). This could potentially help
practitioners better understand and diagnose complex machine learning systems they are designing,
and troubleshoot ways that the algorithm might be amplifying biases in the data.

","Sitan Chen, Frederic Koehler, Ankur Moitra, Morris  Yau","Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability",1.0,"{'UC Berkeley', 'MIT'}",,Theory (including computational and statistical analyses),{'USA'},False,False,"In this work we design algorithms with provable robustness guarantees in the challenging setting where the level of noise is allowed to vary across the domain. This models several scenarios of interest, most notably situations where data provided by certain demographic groups is subject to more noise than others. In a natural experiment on the UCI Adult dataset, we show that coping with this type of noise can help mitigate some natural types of unfairness that arise with off-the-shelf algorithms. Moreover our algorithms have the additional benefit that they lead to more readily interpretable hypotheses. In many settings of interest, we are able to give proper learning algorithms (where previously only improper learning algorithms were known). This could potentially help practitioners better understand and diagnose complex machine learning systems they are designing, and troubleshoot ways that the algorithm might be amplifying biases in the data.",Broader Impacts,0.0,False,0.0,,"Sitan Chen, Frederic Koehler, Ankur Moitra, Morris  Yau",5f8b73c0d4b1bf60dd7173b660b87c29,https://proceedings.neurips.cc/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf,"Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability","Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability",Theory -> Computational Learning Theory,Algorithms -> Adversarial Learning; Algorithms -> Classification; Theory -> Statistical Learning Theory,6.0,"
In this work we design algorithms with provable robustness guarantees in the challenging setting
where the level of noise is allowed to vary across the domain. This models several scenarios of
interest, most notably situations where data provided by certain demographic groups is subject to
more noise than others. In a natural experiment on the UCI Adult dataset, we show that coping with
this type of noise can help mitigate some natural types of unfairness that arise with off-the-shelf
algorithms. Moreover our algorithms have the additional beneﬁt that they lead to more readily
interpretable hypotheses. In many settings of interest, we are able to give proper learning algorithms
(where previously only improper learning algorithms were known). This could potentially help
practitioners better understand and diagnose complex machine learning systems they are designing,
and troubleshoot ways that the algorithm might be amplifying biases in the data.

",246,"Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability",https://papers.nips.cc/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf,145.0
260,"
Deep neural networks take up tremendous amounts of energy, leaving a large carbon footprint.
Quantization can improve energy efﬁciency of neural networks on both commodity GPUs and
specialized accelerators. Robust quantization takes another step and create one model that can be
deployed across many different inference chips avoiding the need to re-train it before deployment
(i.e., reducing CO2 emissions associated with re-training).

","Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser",Robust Quantization: One Model to Rule Them All,1.0,"{'Technion', 'Intel', 'Technion - Israel Institute of Technology', 'AIPG'}",,quantization of NN,"{'USA', 'Israel'}",False,False,"Deep neural networks take up tremendous amounts of energy, leaving a large carbon footprint. Quantization can improve energy efficiency of neural networks on both commodity GPUs and specialized accelerators. Robust quantization takes another step and create one model that can be deployed across many different inference chips avoiding the need to re-train it before deployment (i.e., reducing CO2 emissions associated with re-training).",Broader Impact,1.0,False,1.0,,"Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser",3948ead63a9f2944218de038d8934305,https://proceedings.neurips.cc/paper/2020/file/3948ead63a9f2944218de038d8934305-Paper.pdf,Robust Quantization: One Model to Rule Them All,Robust Quantization: One Model to Rule Them All,Deep Learning -> Efficient Inference Methods,Algorithms -> Classification; Algorithms -> Data Compression,3.0,"
Deep neural networks take up tremendous amounts of energy, leaving a large carbon footprint.
Quantization can improve energy efﬁciency of neural networks on both commodity GPUs and
specialized accelerators. Robust quantization takes another step and create one model that can be
deployed across many different inference chips avoiding the need to re-train it before deployment
(i.e., reducing CO2 emissions associated with re-training).

",260,Robust Quantization: One Model to Rule Them All,https://papers.nips.cc/paper/2020/file/3948ead63a9f2944218de038d8934305-Paper.pdf,62.0
266,"
Our work incorporates task-speciﬁc domain knowledge, in the form of output constraints, into BNNs.
We wish to highlight two key positive impacts. (1) OC-BNNs allow us to manipulate an interpretable
form of knowledge. They can be useful even to domain experts without technical machine learning
expertise, who can easily specify such constraints for model behavior. A tool like this can be used
alongside experts in the real world, such as physicians or judges. (2) Bayesian models like BNNs and
OC-BNNs are typically deployed in “high-stakes” domains, which include those with societal impact.
We intentionally showcase applications of high societal relevance, such as recidivism prediction
and credit scoring, where the ability to specify and satisfy constraints can lead to fairer and more
ethical model behavior.

That being said, there are considerations and limitations. (1) If the model capacity is low (e.g.
the BNN is small), constraints and model capacity may interact in unexpected ways that are not
transparent to the domain expert. (2) Our sampling approach allows us to be very general in specifying
constraints, but it also creates a trade-off between computational efﬁciency and accuracy of constraint
enforcement. (3) Finally, the expert could mis-specify or even maliciously specify constraints. The
ﬁrst two considerations can be mitigated by careful optimization and robustness checks; the latter by
making the constraints public and reviewable by others.

","Wanqian Yang, Lars Lorch, Moritz Graule, Himabindu Lakkaraju, Finale Doshi-Velez",Incorporating Interpretable Output Constraints in Bayesian Neural Networks,1.0,"{'Harvard University', 'Harvard'}",,Probabilistic methods and inference,{'USA'},False,False,"Our work incorporates task-specific domain knowledge, in the form of output constraints, into BNNs. We wish to highlight two key positive impacts. ( 1) OC-BNNs allow us to manipulate an interpretable form of knowledge. They can be useful even to domain experts without technical machine learning expertise, who can easily specify such constraints for model behavior. A tool like this can be used alongside experts in the real world, such as physicians or judges. (2) Bayesian models like BNNs and OC-BNNs are typically deployed in “high-stakes” domains, which include those with societal impact. We intentionally showcase applications of high societal relevance , such as recidivism prediction and credit scoring, where the ability to specify and satisfy constraints can lead to fairer and more ethical model behavior. That being said, there are considerations and limitations. (1) If the model capacity is low (e.g. the BNN is small), constraints and model capacity may interact in unexpected ways that are not transparent to the domain expert. (2) Our sampling approach allows us to be very general in specifying constraints, but it also creates a trade-off between computational efficiency and accuracy of constraint enforcement. (3) Finally, the expert could mis-specify or even maliciously specify constraints. The first two considerations can be mitigated by careful optimization and robustness checks; the latter by making the constraints public and reviewable by others.",Broader Impact,0.0,False,0.0,,"Wanqian Yang, Lars Lorch, Moritz Graule, Himabindu Lakkaraju, Finale Doshi-Velez",95c7dfc5538e1ce71301cf92a9a96bd0,https://proceedings.neurips.cc/paper/2020/file/95c7dfc5538e1ce71301cf92a9a96bd0-Paper.pdf,Incorporating Interpretable Output Constraints in Bayesian Neural Networks,Incorporating Interpretable Output Constraints in Bayesian Neural Networks,Probabilistic Methods,"Social Aspects of Machine Learning -> Fairness, Accountability, and Transparency",12.0,"
Our work incorporates task-speciﬁc domain knowledge, in the form of output constraints, into BNNs.
We wish to highlight two key positive impacts. (1) OC-BNNs allow us to manipulate an interpretable
form of knowledge. They can be useful even to domain experts without technical machine learning
expertise, who can easily specify such constraints for model behavior. A tool like this can be used
alongside experts in the real world, such as physicians or judges. (2) Bayesian models like BNNs and
OC-BNNs are typically deployed in “high-stakes” domains, which include those with societal impact.
We intentionally showcase applications of high societal relevance, such as recidivism prediction
and credit scoring, where the ability to specify and satisfy constraints can lead to fairer and more
ethical model behavior.

That being said, there are considerations and limitations. (1) If the model capacity is low (e.g.
the BNN is small), constraints and model capacity may interact in unexpected ways that are not
transparent to the domain expert. (2) Our sampling approach allows us to be very general in specifying
constraints, but it also creates a trade-off between computational efﬁciency and accuracy of constraint
enforcement. (3) Finally, the expert could mis-specify or even maliciously specify constraints. The
ﬁrst two considerations can be mitigated by careful optimization and robustness checks; the latter by
making the constraints public and reviewable by others.

",266,Incorporating Interpretable Output Constraints in Bayesian Neural Networks,https://papers.nips.cc/paper/2020/file/95c7dfc5538e1ce71301cf92a9a96bd0-Paper.pdf,225.0
270,"Image segmentation has been one of the main challenges in modern medical image analysis, and
describes the process of assigning each pixel or voxel in images with biologically meaningful discrete
labels, such as anatomical structures and tissue types (e.g. pathology and healthy tissues). The task
is required in many clinical and research applications, including surgical planning [41, 42], and the
study of disease progression, aging or healthy development [43–45]. However, there are many cases
in practice where the correct delineation of structures is challenging; this is also reflected in the
well-known presence of high inter- and intra-reader variability in segmentation labels obtained from
trained experts [9, 23, 5].
Although expert manual annotations of lesions is feasible in practice, this task is time consuming.
It usually takes 1.5 to 2 hours to label a MS patient with average 3 visit scans. Meanwhile, the
long-established gold standard for segmentation of medical images has been manually voxel-by-voxel
labeled by an expert anatomist. Unfortunately, this process is fraught with both interand intra-rater
variability (e.g., on the order of approximately 10% by volume [46, 47]). Thus, developing an automatic
segmentation technique to fix the variability among inter- and intra-readers could be meaningful not only
in terms of the accuracy in delineating MS lesions but also in the related reductions in time and economic
costs derived from manual lesion labeling. The lack of consistency in labelling is also common to see in
other medical imaging applications, e.g., in lung abnormalities segmentation from CT images. A lesion
might be clearly visible by one annotator, but the information about whether it is cancer tissue or not
might not be clear to others. While our work in the current form has only been demonstrated on medical
images, we would like to stress that the medical imaging domain offers a considerably broad range of
opportunities for impact; e.g., diagnosis/prognosis in radiology, surgical planning and study of disease
progression and treatment, etc. In addition, the annotator information could be potentially utilised for the
purpose of education. Another potential opportunity is to integrate such information into the data/label
acquisition scheme in order to train reliable segmentation algorithms in a data-efficient manner","Le Zhang, Ryutaro Tanno, Moucheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, Daniel Alexander",Disentangling Human Error from Ground Truth in Segmentation of Medical Images,,,,,,,,,,,,,,,,,,,,,,"Image segmentation has been one of the main challenges in modern medical image analysis, and
describes the process of assigning each pixel or voxel in images with biologically meaningful discrete
labels, such as anatomical structures and tissue types (e.g. pathology and healthy tissues). The task
is required in many clinical and research applications, including surgical planning [41, 42], and the
study of disease progression, aging or healthy development [43–45]. However, there are many cases
in practice where the correct delineation of structures is challenging; this is also reflected in the
well-known presence of high inter- and intra-reader variability in segmentation labels obtained from
trained experts [9, 23, 5].
Although expert manual annotations of lesions is feasible in practice, this task is time consuming.
It usually takes 1.5 to 2 hours to label a MS patient with average 3 visit scans. Meanwhile, the
long-established gold standard for segmentation of medical images has been manually voxel-by-voxel
labeled by an expert anatomist. Unfortunately, this process is fraught with both interand intra-rater
variability (e.g., on the order of approximately 10% by volume [46, 47]). Thus, developing an automatic
segmentation technique to fix the variability among inter- and intra-readers could be meaningful not only
in terms of the accuracy in delineating MS lesions but also in the related reductions in time and economic
costs derived from manual lesion labeling. The lack of consistency in labelling is also common to see in
other medical imaging applications, e.g., in lung abnormalities segmentation from CT images. A lesion
might be clearly visible by one annotator, but the information about whether it is cancer tissue or not
might not be clear to others. While our work in the current form has only been demonstrated on medical
images, we would like to stress that the medical imaging domain offers a considerably broad range of
opportunities for impact; e.g., diagnosis/prognosis in radiology, surgical planning and study of disease
progression and treatment, etc. In addition, the annotator information could be potentially utilised for the
purpose of education. Another potential opportunity is to integrate such information into the data/label
acquisition scheme in order to train reliable segmentation algorithms in a data-efficient manner",270,,https://papers.nips.cc/paper/2020/file/b5d17ed2b502da15aa727af0d51508d6-Paper.pdf,
277,"
Our work investigates the efﬁciency and effectiveness of AI agents to explore the environments
and ultimately achieve optimality. Our results provide a tool for AI engineers and researchers to
identify where the inefﬁciency of a policy may be coming from, including potentially unintended side
effects. Further, the characterization provided in this work can suggest how systemic improvements
are possible given non-parametric causal understanding of the underlying systems. The very topic of
our paper about efﬁciency and effectiveness has been studied for several decades in diverse ﬁelds:
bandits, reinforcement learning, design of experiments, etc. Hence, it is not difﬁcult to imagine that
our work will share the common problems with other automated decision making tools and methods
such as (i) the system optimized based on an ill-deﬁned reward may harm ‘unknown unknowns’
(e.g., increasing the revenue of alcoholic beverage companies based on targeted advertising over
recovering alcoholics if their health is not properly modeled) or (ii) the optimization can be impossible
due to the participants of adversarial players (e.g., rewarding the number of software bugs ﬁxed
makes software engineers to create more bugs to ﬁx, see Goodhart’s law). Mitigating the ﬁrst kind
of risks will require deploying proper countermeasure through, e.g., regulations by governments.
The second kind of risks (errors or failures) can be detected through examining possible changes
of underlying mechanisms (i.e., anomaly detection). However, the current work does not consider
multiple adversarial participants (e.g., game-theoretic settings), which is a subject of future research.

","Sanghack Lee, Elias Bareinboim",Characterizing Optimal Mixed Policies: Where to Intervene and What to Observe,1.0,"{'Columbia University', 'Purdue University'}",,Causality,{'USA'},False,False,"Our work investigates the efficiency and effectiveness of AI agents to explore the environments and ultimately achieve optimality. Our results provide a tool for AI engineers and researchers to identify where the inefficiency of a policy may be coming from, including potentially unintended side effects. Further, the characterization provided in this work can suggest how systemic improvements are possible given non-parametric causal understanding of the underlying systems. The very topic of our paper about efficiency and effectiveness has been studied for several decades in diverse fields: bandits, reinforcement learning, design of experiments, etc. Hence, it is not difficult to imagine that our work will share the common problems with other automated decision making tools and methods such as (i) the system optimized based on an ill-defined reward may harm ‘unknown unknowns’ (e.g., increasing the revenue of alcoholic beverage companies based on targeted advertising over recovering alcoholics if their health is not properly modeled) or (ii) the optimization can be impossible due to the participants of adversarial players (e.g., rewarding the number of software bugs fixed makes software engineers to create more bugs to fix, see Goodhart’s law). Mitigating the first kind of risks will require deploying proper countermeasure through, e.g., regulations by governments. The second kind of risks (errors or failures) can be detected through examining possible changes of underlying mechanisms (i.e., anomaly detection). However, the current work does not consider multiple adversarial participants (e.g., game-theoretic settings), which is a subject of future research.",Broader Impact,0.0,False,0.0,,"Sanghack Lee, Elias Bareinboim",61a10e6abb1149ad9d08f303267f9bc4,https://proceedings.neurips.cc/paper/2020/file/61a10e6abb1149ad9d08f303267f9bc4-Paper.pdf,Characterizing Optimal Mixed Policies: Where to Intervene and What to Observe,Characterizing Optimal Mixed Policies: Where to Intervene and What to Observe,Probabilistic Methods -> Causal Inference,,8.0,"
Our work investigates the efﬁciency and effectiveness of AI agents to explore the environments
and ultimately achieve optimality. Our results provide a tool for AI engineers and researchers to
identify where the inefﬁciency of a policy may be coming from, including potentially unintended side
effects. Further, the characterization provided in this work can suggest how systemic improvements
are possible given non-parametric causal understanding of the underlying systems. The very topic of
our paper about efﬁciency and effectiveness has been studied for several decades in diverse ﬁelds:
bandits, reinforcement learning, design of experiments, etc. Hence, it is not difﬁcult to imagine that
our work will share the common problems with other automated decision making tools and methods
such as (i) the system optimized based on an ill-deﬁned reward may harm ‘unknown unknowns’
(e.g., increasing the revenue of alcoholic beverage companies based on targeted advertising over
recovering alcoholics if their health is not properly modeled) or (ii) the optimization can be impossible
due to the participants of adversarial players (e.g., rewarding the number of software bugs ﬁxed
makes software engineers to create more bugs to ﬁx, see Goodhart’s law). Mitigating the ﬁrst kind
of risks will require deploying proper countermeasure through, e.g., regulations by governments.
The second kind of risks (errors or failures) can be detected through examining possible changes
of underlying mechanisms (i.e., anomaly detection). However, the current work does not consider
multiple adversarial participants (e.g., game-theoretic settings), which is a subject of future research.

",277,Characterizing Optimal Mixed Policies: Where to Intervene and What to Observe,https://papers.nips.cc/paper/2020/file/61a10e6abb1149ad9d08f303267f9bc4-Paper.pdf,244.0
280,"
Our work contributes an advancement in Inverse Reinforcement Learning (IRL) methods that can be
used for imitation learning. Importantly, they enable non-expert users to program robots and other
technical devices merely by demonstrating a desired behavior. On the one hand, this is a crucial
requirement for realising visions such as Industry 4.0, where people increasingly work alongside
ﬂexible and lightweight robots and both have to constantly adapt to changing task requirements. This
is expected to boost productivity, lower production costs, and contribute to bringing back local jobs
that were lost due to globalization strategies. Since the IRL approach we present can incorporate and
enforce constraints on behavior even if the demonstrations violate them, it has interesting applications
in safety critical applications, such as creating safe vehicle behaviors for automated driving. On the
other hand, the improvements we present can potentially accelerate existing trends for automation,
requiring less and less human workers if they can be replaced by ﬂexible and easily programmable
robots. Estimates for the percentage of jobs at risk for automation range between 14% (OECD report,
[17]) and 47% [9] of available jobs (also depending on the country in question). Thus, our work
could potentially add to the societal challenge to ﬁnd solutions that mitigate these consequences (such
as e.g. re-training, continuing education, universal basic income, etc.) and make sure that affected
individuals remain active, contributing, and self-determined members of society. While it has been
argued that IRL methods are essential for value-alignment of artiﬁcial intelligence agents [22, 2],
the standard framework might not cover all aspects necessary [3]. Our Deep Constrained Inverse
Q-learning approach, however, improves on this situation by providing a means to enforce additional
behavioral rules and constraints to guarantee behavior imitation consistent within moral and ethical
frames of society.
","Gabriel Kalweit, Maria Huegle, Moritz Werling, Joschka Boedecker",Deep Inverse Q-learning with Constraints,1.0,"{'BMWGroup, Unterschleissheim', 'University of Freiburg'}",,Reinforcement learning and planning,{'Germany'},False,False,"Our work contributes an advancement in Inverse Reinforcement Learning (IRL) methods that can be used for imitation learning. Importantly, they enable non-expert users to program robots and other technical devices merely by demonstrating a desired behavior. On the one hand, this is a crucial requirement for realising visions such as Industry 4.0, where people increasingly work alongside flexible and lightweight robots and both have to constantly adapt to changing task requirements. This is expected to boost productivity, lower production costs, and contribute to bringing back local jobs that were lost due to globalization strategies. Since the IRL approach we present can incorporate and enforce constraints on behavior even if the demonstrations violate them, it has interesting applications in safety critical applications, such as creating safe vehicle behaviors for automated driving. On the other hand, the improvements we present can potentially accelerate existing trends for automation, requiring less and less human workers if they can be replaced by flexible and easily programmable robots. Estimates for the percentage of jobs at risk for automation range between 14% (OECD report, [17]) and 47% [9] of available jobs (also depending on the country in question). Thus, our work could potentially add to the societal challenge to find solutions that mitigate these consequences (such as e.g. re-training, continuing education, universal basic income, etc.) and make sure that affected individuals remain active, contributing, and self-determined members of society. While it has been argued that IRL methods are essential for value-alignment of artificial intelligence agents [22, 2], the standard framework might not cover all aspects necessary [3]. Our Deep Constrained Inverse Q-learning approach, however, improves on this situation by providing a means to enforce additional behavioral rules and constraints to guarantee behavior imitation consistent within moral and ethical frames of society.",Broader Impact,1.0,False,1.0,,"Gabriel Kalweit, Maria Huegle, Moritz Werling, Joschka Boedecker",a4c42bfd5f5130ddf96e34a036c75e0a,https://proceedings.neurips.cc/paper/2020/file/a4c42bfd5f5130ddf96e34a036c75e0a-Paper.pdf,Deep Inverse Q-learning with Constraints,Deep Inverse Q-learning with Constraints,Reinforcement Learning and Planning,Reinforcement Learning and Planning -> Reinforcement Learning,11.0,"
Our work contributes an advancement in Inverse Reinforcement Learning (IRL) methods that can be
used for imitation learning. Importantly, they enable non-expert users to program robots and other
technical devices merely by demonstrating a desired behavior. On the one hand, this is a crucial
requirement for realising visions such as Industry 4.0, where people increasingly work alongside
ﬂexible and lightweight robots and both have to constantly adapt to changing task requirements. This
is expected to boost productivity, lower production costs, and contribute to bringing back local jobs
that were lost due to globalization strategies. Since the IRL approach we present can incorporate and
enforce constraints on behavior even if the demonstrations violate them, it has interesting applications
in safety critical applications, such as creating safe vehicle behaviors for automated driving. On the
other hand, the improvements we present can potentially accelerate existing trends for automation,
requiring less and less human workers if they can be replaced by ﬂexible and easily programmable
robots. Estimates for the percentage of jobs at risk for automation range between 14% (OECD report,
[17]) and 47% [9] of available jobs (also depending on the country in question). Thus, our work
could potentially add to the societal challenge to ﬁnd solutions that mitigate these consequences (such
as e.g. re-training, continuing education, universal basic income, etc.) and make sure that affected
individuals remain active, contributing, and self-determined members of society. While it has been
argued that IRL methods are essential for value-alignment of artiﬁcial intelligence agents [22, 2],
the standard framework might not cover all aspects necessary [3]. Our Deep Constrained Inverse
Q-learning approach, however, improves on this situation by providing a means to enforce additional
behavioral rules and constraints to guarantee behavior imitation consistent within moral and ethical
frames of society.
",280,Deep Inverse Q-learning with Constraints,https://papers.nips.cc/paper/2020/file/a4c42bfd5f5130ddf96e34a036c75e0a-Paper.pdf,294.0
283,"

a) Who may beneﬁt from this research? In this paper we proposed a new pipeline for deep metric
learning. Like many other relevant studies in this area, our work aims at establishing similarity or
dissimilarity relationships among data inputs. Our work can be applied to many practical scenarios,
such as big data analysis, face/object recognition, person re-identiﬁcation, voice veriﬁcation, etc.
Corporations or other non-proﬁt organizations/persons with such purposes may beneﬁt from our work.
b) Who may be put at disadvantage from this research? Since our work can be used in social media companies or any other occasions where user data can be accessed, people who are worried about
their privacy being analyzed or targeted may be put at disadvantage. c) What are the consequences
of failure of the system? Before formal deployment, the DML model should be properly trained
and tested with available data samples, i.e., the risk should be controllable. If any failure happens,
the most immediate consequence can be recognition/analysis errors for the systems in which our
proposed model is leveraged, which may further result in unnecessary economic costs or losses of
other resources. d) Whether the task/method leverages biases in the data? Our work is posed with a
general purpose of learning a more discriminative feature embedding without speciﬁc requirements
on training data. Thus, our work does not leverage biases in the data, but rather, may possess the
ability to suppress/capture such biases (if any) using our adaptive proxy strategy.

","Yuehua Zhu, Muli Yang, Cheng Deng, Wei Liu",Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies,1.0,"{'Xidian University', 'Tencent AI Lab'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)",{'China'},False,False,"a) Who may benefit from this research? In this paper we proposed a new pipeline for deep metric learning. Like many other relevant studies in this area, our work aims at establishing similarity or dissimilarity relationships among data inputs. Our work can be applied to many practical scenarios, such as big data analysis, face/object recognition, person re-identification, voice verification, etc . Corporations or other non-profit organizations/persons with such purposes may benefit from our work. b) Who may be put at disadvantage from this research? Since our work can be used in social media  companies or any other occasions where user data can be accessed, people who are worried about their privacy being analyzed or targeted may be put at disadvantage. c) What are the consequences of failure of the system? Before formal deployment, the DML model should be properly trained and tested with available data samples, i . e ., the risk should be controllable. If any failure happens, the most immediate consequence can be recognition/analysis errors for the systems in which our proposed model is leveraged, which may further result in unnecessary economic costs or losses of other resources. d) Whether the task/method leverages biases in the data? Our work is posed with a general purpose of learning a more discriminative feature embedding without specific requirements on training data. Thus, our work does not leverage biases in the data, but rather, may possess the ability to suppress/capture such biases (if any) using our adaptive proxy strategy.",Broader Impact,1.0,False,1.0,,"Yuehua Zhu, Muli Yang, Cheng Deng, Wei Liu",ce016f59ecc2366a43e1c96a4774d167,https://proceedings.neurips.cc/paper/2020/file/ce016f59ecc2366a43e1c96a4774d167-Paper.pdf,Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies,Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies,Applications -> Computer Vision,Algorithms -> Classification; Algorithms -> Clustering; Algorithms -> Metric Learning; Deep Learning,14.0,"

a) Who may beneﬁt from this research? In this paper we proposed a new pipeline for deep metric
learning. Like many other relevant studies in this area, our work aims at establishing similarity or
dissimilarity relationships among data inputs. Our work can be applied to many practical scenarios,
such as big data analysis, face/object recognition, person re-identiﬁcation, voice veriﬁcation, etc.
Corporations or other non-proﬁt organizations/persons with such purposes may beneﬁt from our work.
b) Who may be put at disadvantage from this research? Since our work can be used in social media companies or any other occasions where user data can be accessed, people who are worried about
their privacy being analyzed or targeted may be put at disadvantage. c) What are the consequences
of failure of the system? Before formal deployment, the DML model should be properly trained
and tested with available data samples, i.e., the risk should be controllable. If any failure happens,
the most immediate consequence can be recognition/analysis errors for the systems in which our
proposed model is leveraged, which may further result in unnecessary economic costs or losses of
other resources. d) Whether the task/method leverages biases in the data? Our work is posed with a
general purpose of learning a more discriminative feature embedding without speciﬁc requirements
on training data. Thus, our work does not leverage biases in the data, but rather, may possess the
ability to suppress/capture such biases (if any) using our adaptive proxy strategy.

",283,Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies,https://papers.nips.cc/paper/2020/file/ce016f59ecc2366a43e1c96a4774d167-Paper.pdf,247.0
1,"We believe that the ﬁeld of decentralized learning plays a key role in translating the recent successes
in deep learning from large organizations with large centralized datasets to smaller industry players
and individuals. In particular, decentralized and therefore collaborative training on decentralized
data is an important building block towards helping to better align each individual’s data ownership
and privacy with the resulting utility from jointly trained machine learning models. The ability
to train collaboratively on decentralized data may lead to transformative insights in many ﬁelds,
especially in applications where data is user-provided and privacy sensitive (Nedic, 2020). In addition
to privacy, efﬁciency gains in distributed training reduce the environmental impact of training large
machine learning models. The introduction of a practical and reliable communication compression
technique is a small step towards achieving these goals on collaborative privacy-preserving and
efﬁcient decentralized learning.","Thijs Vogels, Sai Praneeth Karimireddy, Martin Jaggi",Practical Low-Rank Communication Compression in Decentralized Deep Learning,1.0,{'EPFL'},,Optimization Methods (continuous or discrete),{'Switzerland'},False,False,"We believe that the field of decentralized learning plays a key role in translating the recent successes in deep learning from large organizations with large centralized datasets to smaller industry players and individuals. In particular, decentralized and therefore collaborative training on decentralized data is an important building block towards helping to better align each individual’s data ownership and privacy with the resulting utility from jointly trained machine learning models. The ability to train collaboratively on decentralized data may lead to transformative insights in many fields, especially in applications where data is user-provided and privacy sensitive (Nedic, 2020). In addition to privacy, efficiency gains in distributed training reduce the environmental impact of training large machine learning models. The introduction of a practical and reliable communication compression technique is a small step towards achieving these goals on collaborative privacy-preserving and efficient decentralized learning.",8 Broader Impact,0.0,False,0.0,,"Thijs Vogels, Sai Praneeth Karimireddy, Martin Jaggi",a376802c0811f1b9088828288eb0d3f0,https://proceedings.neurips.cc/paper/2020/file/a376802c0811f1b9088828288eb0d3f0-Paper.pdf,Practical Low-Rank Communication Compression in Decentralized Deep Learning,Practical Low-Rank Communication Compression in Decentralized Deep Learning,Deep Learning -> Optimization for Deep Networks,Optimization; Optimization -> Non-Convex Optimization,5.0,"We believe that the ﬁeld of decentralized learning plays a key role in translating the recent successes
in deep learning from large organizations with large centralized datasets to smaller industry players
and individuals. In particular, decentralized and therefore collaborative training on decentralized
data is an important building block towards helping to better align each individual’s data ownership
and privacy with the resulting utility from jointly trained machine learning models. The ability
to train collaboratively on decentralized data may lead to transformative insights in many ﬁelds,
especially in applications where data is user-provided and privacy sensitive (Nedic, 2020). In addition
to privacy, efﬁciency gains in distributed training reduce the environmental impact of training large
machine learning models. The introduction of a practical and reliable communication compression
technique is a small step towards achieving these goals on collaborative privacy-preserving and
efﬁcient decentralized learning.",1,Practical Low-Rank Communication Compression in Decentralized Deep Learning,https://papers.nips.cc/paper/2020/file/a376802c0811f1b9088828288eb0d3f0-Paper.pdf,141.0
10,"
Applications and Beneﬁts

Our diversity-driven learning approach for improved robustness can be beneﬁcial for bringing RL to
real-world applications, such as robotics. It is critical that various types of robots, including service
robotics, home robots, and robots used for disaster relief or search-and-rescue are able to handle
varying environment conditions. Otherwise, they may fail to complete the tasks they are supposed to
accomplish, which could have signiﬁcant consequences in safety-critical situations.

It is conceivable that, during deployment of robotics systems, the system may encounter changes
in its environment that it has not previously dealt with. For example, a robot may be tasked with
picking up a set of objects. At test time, the environment may slightly differ from the training setting,
e.g. some objects may be missing or additional objects may be present. These previously unseen
conﬁgurations may confuse the agent’s policy and lead to unpredictable and sub-optimal behavior. If
RL algorithms are to be used to prescribe actions from input observations in a robotics application,
the algorithms must be robust to these perturbations. Our approach of learning multiple diverse
solutions to the task is a step towards achieving the desired robustness.

Risks and Ethical Issues

RL algorithms, in general, face a number of risks. First, they tend to suffer from reward speciﬁcation
- in particular, the reward may not necessarily be completely aligned with the desired behavior.
Therefore, it can be difﬁcult for a practitioner to predict the behavior of an algorithm when it
is deployed. Since our algorithm learns multiple ways to optimize a task reward, the robustness
and predictability of its behavior is also limited by the alignment of the reward function with the
qualitative task objective. Additionally, even if the reward is well-speciﬁed, RL algorithms face a
number of other risks, including (but not limited to) safety and stability. Our diversity-driven learning
paradigm suffers from the same issues, as different latent-conditioned policies may not produce
reliable behavior when executed in real world settings if the underlying RL algorithm is unstable.

","Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn",One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL,1.0,"{'UC Berkeley', 'Stanford University', 'Stanford'}",,Reinforcement learning and planning,{'USA'},False,False,"Applications and Benefits Our diversity-driven learning approach for improved robustness can be beneficial for bringing RL to real-world applications, such as robotics. It is critical that various types of robots, including service robotics, home robots, and robots used for disaster relief or search-and-rescue are able to handle varying environment conditions. Otherwise, they may fail to complete the tasks they are supposed to accomplish, which could have significant consequences in safety-critical situations. It is conceivable that, during deployment of robotics systems, the system may encounter changes in its environment that it has not previously dealt with. For example, a robot may be tasked with picking up a set of objects. At test time, the environment may slightly differ from the training setting, e.g. some objects may be missing or additional objects may be present. These previously unseen configurations may confuse the agent’s policy and lead to unpredictable and sub-optimal behavior. If RL algorithms are to be used to prescribe actions from input observations in a robotics application, the algorithms must be robust to these perturbations. Our approach of learning multiple diverse solutions to the task is a step towards achieving the desired robustness. Risks and Ethical Issues RL algorithms, in general, face a number of risks. First, they tend to suffer from reward specification - in particular, the reward may not necessarily be completely aligned with the desired behavior. Therefore, it can be difficult for a practitioner to predict the behavior of an algorithm when it is deployed. Since our algorithm learns multiple ways to optimize a task reward, the robustness and predictability of its behavior is also limited by the alignment of the reward function with the qualitative task objective. Additionally, even if the reward is well-specified, RL algorithms face a number of other risks, including (but not limited to) safety and stability. Our diversity-driven learning paradigm suffers from the same issues, as different latent-conditioned policies may not produce reliable behavior when executed in real world settings if the underlying RL algorithm is unstable.",Broader Impacts,0.0,False,0.0,,"Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn",5d151d1059a6281335a10732fc49620e,https://proceedings.neurips.cc/paper/2020/file/5d151d1059a6281335a10732fc49620e-Paper.pdf,One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL,One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL,Reinforcement Learning and Planning,Reinforcement Learning and Planning -> Reinforcement Learning,15.0,"
Applications and Beneﬁts

Our diversity-driven learning approach for improved robustness can be beneﬁcial for bringing RL to
real-world applications, such as robotics. It is critical that various types of robots, including service
robotics, home robots, and robots used for disaster relief or search-and-rescue are able to handle
varying environment conditions. Otherwise, they may fail to complete the tasks they are supposed to
accomplish, which could have signiﬁcant consequences in safety-critical situations.

It is conceivable that, during deployment of robotics systems, the system may encounter changes
in its environment that it has not previously dealt with. For example, a robot may be tasked with
picking up a set of objects. At test time, the environment may slightly differ from the training setting,
e.g. some objects may be missing or additional objects may be present. These previously unseen
conﬁgurations may confuse the agent’s policy and lead to unpredictable and sub-optimal behavior. If
RL algorithms are to be used to prescribe actions from input observations in a robotics application,
the algorithms must be robust to these perturbations. Our approach of learning multiple diverse
solutions to the task is a step towards achieving the desired robustness.

Risks and Ethical Issues

RL algorithms, in general, face a number of risks. First, they tend to suffer from reward speciﬁcation
- in particular, the reward may not necessarily be completely aligned with the desired behavior.
Therefore, it can be difﬁcult for a practitioner to predict the behavior of an algorithm when it
is deployed. Since our algorithm learns multiple ways to optimize a task reward, the robustness
and predictability of its behavior is also limited by the alignment of the reward function with the
qualitative task objective. Additionally, even if the reward is well-speciﬁed, RL algorithms face a
number of other risks, including (but not limited to) safety and stability. Our diversity-driven learning
paradigm suffers from the same issues, as different latent-conditioned policies may not produce
reliable behavior when executed in real world settings if the underlying RL algorithm is unstable.

",10,One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL,https://papers.nips.cc/paper/2020/file/5d151d1059a6281335a10732fc49620e-Paper.pdf,334.0
30,"In this work, we study the problem of adversarial robustness of metric learning. Adversarial robustness, especially robustness verification, is very important when deploying machine learning models
into real-world systems. A potential risk is the research on adversarial attack, while understanding
adversarial attack is a necessary step towards developing provably robust models. In general, this
work does not involve specific applications and ethical issues.","Lu Wang, Xuanqing Liu, Jinfeng Yi, Yuan Jiang, Cho-Jui Hsieh",Provably Robust Metric Learning,1.0,"{'UCLA', 'University of California, Los Angeles', 'National Key lab for Novel Software Technology', 'Nanjing University', 'JD Research'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)","{'USA', 'China'}",False,False,"In this work, we study the problem of adversarial robustness of metric learning. Adversarial robust- ness, especially robustness verification, is very important when deploying machine learning models into real-world systems. A potential risk is the research on adversarial attack, while understanding adversarial attack is a necessary step towards developing provably robust models. In general, this work does not involve specific applications and ethical issues.",Broader impact,1.0,False,1.0,,"Lu Wang, Xuanqing Liu, Jinfeng Yi, Yuan Jiang, Cho-Jui Hsieh",e038453073d221a4f32d0bab94ca7cee,https://proceedings.neurips.cc/paper/2020/file/e038453073d221a4f32d0bab94ca7cee-Paper.pdf,Provably Robust Metric Learning,Provably Robust Metric Learning,Algorithms -> Adversarial Learning,Social Aspects of Machine Learning -> AI Safety,4.0,"In this work, we study the problem of adversarial robustness of metric learning. Adversarial robustness, especially robustness verification, is very important when deploying machine learning models
into real-world systems. A potential risk is the research on adversarial attack, while understanding
adversarial attack is a necessary step towards developing provably robust models. In general, this
work does not involve specific applications and ethical issues.",30,Provably Robust Metric Learning,https://papers.nips.cc/paper/2020/file/e038453073d221a4f32d0bab94ca7cee-Paper.pdf,64.0
39,"Computational Fluid Dynamics is key to addressing the critical engineering problem of designing
shapes that maximize aerodynamic, hydrodynamic, and heat transfer performance, and much else
beside. The techniques we propose therefore have the potential to have a major impact in the ﬁeld of
Computer Assisted Design by unleashing the full power of deep learning in an area where it is not
yet fully established.","Edoardo Remelli, Artem Lukoyanov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua",MeshSDF: Differentiable Iso-Surface Extraction,1.0,"{'Facebook', 'Neural Concept SA', 'Intel Labs', 'EPFL, Switzerland', 'EPFL'}",,Deep learning,"{'USA', 'Switzerland'}",False,False,"Computational Fluid Dynamics is key to addressing the critical engineering problem of designing shapes that maximize aerodynamic, hydrodynamic, and heat transfer performance, and much else beside. The techniques we propose therefore have the potential to have a major impact in the field of Computer Assisted Design by unleashing the full power of deep learning in an area where it is not yet fully established.",7 Broader Impact,1.0,False,1.0,,"Edoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua",fe40fb944ee700392ed51bfe84dd4e3d,https://proceedings.neurips.cc/paper/2020/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf,MeshSDF: Differentiable Iso-Surface Extraction,MeshSDF: Differentiable Iso-Surface Extraction,Algorithms -> Representation Learning,Applications -> Computer Vision; Deep Learning -> Generative Models,2.0,"Computational Fluid Dynamics is key to addressing the critical engineering problem of designing
shapes that maximize aerodynamic, hydrodynamic, and heat transfer performance, and much else
beside. The techniques we propose therefore have the potential to have a major impact in the ﬁeld of
Computer Assisted Design by unleashing the full power of deep learning in an area where it is not
yet fully established.",39,MeshSDF: Differentiable Iso-Surface Extraction,https://papers.nips.cc/paper/2020/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf,64.0
43,"We believe ESRL is a tool that can help bring RL closer to real-world applications. In particular
this will be useful in the clinical setting to ﬁnd optimal dynamic treatment regimes for complex
diseases, or at least assist in treatment decision making. This is because ESRL’s framework lends
itself to be questioned by users (physicians) and sheds light into potential biases introduced by the
data sampling mechanism used to generate the observed data set. Additionally, using hypothesis
testing and accommodating different levels of risk aversion makes the method sensible to ofﬂine
settings and different real-world applications. It is important when using ESRL and any RL method,
to question the validity of the policy’s decisions, the quality of the data, and the method that was used
to derive these.

","Aaron Sonabend, Junwei Lu, Leo Anthony Celi, Tianxi Cai, Peter Szolovits",Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,1.0,"{'Harvard School of Public Health', 'Harvard University', 'Massachusetts Institute of Technology', 'MIT'}",,Reinforcement learning and planning,{'USA'},False,False,"We believe ESRL is a tool that can help bring RL closer to real-world applications. In particular this will be useful in the clinical setting to find optimal dynamic treatment regimes for complex diseases, or at least assist in treatment decision making. This is because ESRL’s framework lends itself to be questioned by users (physicians) and sheds light into potential biases introduced by the data sampling mechanism used to generate the observed data set. Additionally, using hypothesis testing and accommodating different levels of risk aversion makes the method sensible to offline settings and different real-world applications. It is important when using ESRL and any RL method, to question the validity of the policy’s decisions, the quality of the data, and the method that was used to derive these.",Broader Impact,0.0,False,0.0,,"Aaron Sonabend, Junwei Lu, Leo Anthony Celi, Tianxi Cai, Peter Szolovits",daf642455364613e2120c636b5a1f9c7,https://proceedings.neurips.cc/paper/2020/file/daf642455364613e2120c636b5a1f9c7-Paper.pdf,Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,Reinforcement Learning and Planning -> Model-Based RL,Applications -> Computational Biology and Bioinformatics; Applications -> Health,5.0,"We believe ESRL is a tool that can help bring RL closer to real-world applications. In particular
this will be useful in the clinical setting to ﬁnd optimal dynamic treatment regimes for complex
diseases, or at least assist in treatment decision making. This is because ESRL’s framework lends
itself to be questioned by users (physicians) and sheds light into potential biases introduced by the
data sampling mechanism used to generate the observed data set. Additionally, using hypothesis
testing and accommodating different levels of risk aversion makes the method sensible to ofﬂine
settings and different real-world applications. It is important when using ESRL and any RL method,
to question the validity of the policy’s decisions, the quality of the data, and the method that was used
to derive these.

",43,Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,https://papers.nips.cc/paper/2020/file/daf642455364613e2120c636b5a1f9c7-Paper.pdf,128.0
48,"This paper investigates a fundamental problem in machine learning and statistics. The theory and
algorithms presented in this paper are expected to beneﬁt many broad ﬁelds in science and engineering,
such as learning theory, robust statistics, optimization, and applications in biology, climatology, and
seismology, to name a few. Our research belongs to the general paradigm of interactive learning, in
which the learning agent need to design adaptive sampling schemes to maximize data efﬁciency. We
are well aware that one needs to be careful in designing such sampling schemes, to avoid unintended
harms such as discrimination.

","Chicheng Zhang, Jie Shen, Pranjal Awasthi",Efficient active learning of sparse halfspaces with arbitrary bounded noise,1.0,"{'Stevens Institute of Technology', 'Rutgers University/Google', 'University of Arizona'}",,Theory (including computational and statistical analyses),{'USA'},False,False,"This paper investigates a fundamental problem in machine learning and statistics. The theory and algorithms presented in this paper are expected to benefit many broad fields in science and engineering, such as learning theory, robust statistics, optimization, and applications in biology, climatology, and seismology, to name a few. Our research belongs to the general paradigm of interactive learning, in which the learning agent need to design adaptive sampling schemes to maximize data efficiency. We are well aware that one needs to be careful in designing such sampling schemes, to avoid unintended harms such as discrimination.",Broader Impact,1.0,False,1.0,,"Chicheng Zhang, Jie Shen, Pranjal Awasthi",5034a5d62f91942d2a7aeaf527dfe111,https://proceedings.neurips.cc/paper/2020/file/5034a5d62f91942d2a7aeaf527dfe111-Paper.pdf,Efficient active learning of sparse halfspaces with arbitrary bounded noise,Efficient active learning of sparse halfspaces with arbitrary bounded noise,Theory -> Computational Learning Theory,Algorithms -> Active Learning; Theory -> Statistical Learning Theory,4.0,"This paper investigates a fundamental problem in machine learning and statistics. The theory and
algorithms presented in this paper are expected to beneﬁt many broad ﬁelds in science and engineering,
such as learning theory, robust statistics, optimization, and applications in biology, climatology, and
seismology, to name a few. Our research belongs to the general paradigm of interactive learning, in
which the learning agent need to design adaptive sampling schemes to maximize data efﬁciency. We
are well aware that one needs to be careful in designing such sampling schemes, to avoid unintended
harms such as discrimination.

",48,Efficient active learning of sparse halfspaces with arbitrary bounded noise,https://papers.nips.cc/paper/2020/file/5034a5d62f91942d2a7aeaf527dfe111-Paper.pdf,95.0
55,"As our method is an image synthesis tool, it shares with other image synthesis tools the same potential
beneﬁts (e.g., [2]) and dangers that have been discussed extensively elsewhere, e.g., see [18] for one
such discussion.

Our method does not perform any training on images; it takes an existing GAN as input. As discussed
in Section 3.2, our method inherits the biases of the input GAN, e.g., limited ability to place makeup
on male-presenting faces. Conversely, this method provides a tool for discovering biases that would
otherwise be hard to identify.

","Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris",GANSpace: Discovering Interpretable GAN Controls,1.0,"{'Adobe', 'Aalto University'}",,Vision,"{'Finland', 'USA'}",False,False,"As our method is an image synthesis tool, it shares with other image synthesis tools the same potential benefits (e.g., [2]) and dangers that have been discussed extensively elsewhere, e.g., see [18] for one such discussion. Our method does not perform any training on images; it takes an existing GAN as input. As discussed in Section 3.2, our method inherits the biases of the input GAN, e.g., limited ability to place makeup on male-presenting faces. Conversely, this method provides a tool for discovering biases that would otherwise be hard to identify.",Broader Impact,1.0,False,1.0,,"Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris",6fe43269967adbb64ec6149852b5cc3e,https://proceedings.neurips.cc/paper/2020/file/6fe43269967adbb64ec6149852b5cc3e-Paper.pdf,GANSpace: Discovering Interpretable GAN Controls,GANSpace: Discovering Interpretable GAN Controls,Deep Learning -> Generative Models,"Deep Learning -> Visualization, Interpretability, and Explainability",4.0,"As our method is an image synthesis tool, it shares with other image synthesis tools the same potential
beneﬁts (e.g., [2]) and dangers that have been discussed extensively elsewhere, e.g., see [18] for one
such discussion.

Our method does not perform any training on images; it takes an existing GAN as input. As discussed
in Section 3.2, our method inherits the biases of the input GAN, e.g., limited ability to place makeup
on male-presenting faces. Conversely, this method provides a tool for discovering biases that would
otherwise be hard to identify.

",55,GANSpace: Discovering Interpretable GAN Controls,https://papers.nips.cc/paper/2020/file/6fe43269967adbb64ec6149852b5cc3e-Paper.pdf,91.0
57,"In this paper, we develop an automatic framework to enable perturbation analysis on any neural
network structures. Our framework can be used in a wide variety of tasks ranging from robust-
ness veriﬁcation to certiﬁed defense, and potentially many more applications requiring a provable
perturbation analysis. It can also play an important building block for several safety-critical ML
applications, such as transportation, engineering, and healthcare, etc. We expect that our framework
will signiﬁcantly improve the robustness and reliability of real-world ML systems with theoretical
guarantees.

An important product of this paper is an open-source LiRPA library with over 10,000 lines of code,
which provides automatic and differentiable perturbation analysis. This library can tremendously
facilitate the use of LiRPA for the research community as well as industrial applications, such as
veriﬁable plant control [50]. Our library of LiRPA on general computational graphs can also inspire
further improved implementations on automatic outer bounds calculations with provable guarantees.

Although our focus on this paper has been on exploring known perturbations and providing guarantees
in such clairvoyant scenarios, in real-world an adversary (or nature) may not adhere to our assumptions.
Thus, we may additionally want to understand implication of these unknown scenarios on the system
performance. This is a relatively unexplored area in robust machine learning, and we encourage
researchers to understand and mitigate the risks arising from unknown perturbations in these contexts.","Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh",Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,1.0,"{'Tsinghua University', 'UCLA', 'Lawrence Livermore National Lab', 'JD.com', 'Northeastern University'}",,Deep learning,"{'USA', 'China'}",False,False,"In this paper, we develop an automatic framework to enable perturbation analysis on any neural network structures. Our framework can be used in a wide variety of tasks ranging from robustness verification to certified defense, and potentially many more applications requiring a provable perturbation analysis. It can also play an important building block for several safety-critical ML applications, such as transportation, engineering, and healthcare, etc. We expect that our framework will significantly improve the robustness and reliability of real-world ML systems with theoretical guarantees. An important product of this paper is an open-source LiRPA library with over 10,000 lines of code, which provides automatic and differentiable perturbation analysis. This library can tremendously facilitate the use of LiRPA for the research community as well as industrial applications, such as verifiable plant control [50]. Our library of LiRPA on general computational graphs can also inspire further improved implementations on automatic outer bounds calculations with provable guarantees. Although our focus on this paper has been on exploring known perturbations and providing guarantees in such clairvoyant scenarios, in real-world an adversary (or nature) may not adhere to our assumptions. Thus, we may additionally want to understand implication of these unknown scenarios on the system performance. This is a relatively unexplored area in robust machine learning, and we encourage researchers to understand and mitigate the risks arising from unknown perturbations in these contexts.",Broader Impact,1.0,False,1.0,,"Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh",0cbc5671ae26f67871cb914d81ef8fc1,https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,Social Aspects of Machine Learning -> AI Safety,"Algorithms -> Adversarial Learning; Data, Challenges, Implementations, and Software -> Software Toolkits",10.0,"In this paper, we develop an automatic framework to enable perturbation analysis on any neural
network structures. Our framework can be used in a wide variety of tasks ranging from robust-
ness veriﬁcation to certiﬁed defense, and potentially many more applications requiring a provable
perturbation analysis. It can also play an important building block for several safety-critical ML
applications, such as transportation, engineering, and healthcare, etc. We expect that our framework
will signiﬁcantly improve the robustness and reliability of real-world ML systems with theoretical
guarantees.

An important product of this paper is an open-source LiRPA library with over 10,000 lines of code,
which provides automatic and differentiable perturbation analysis. This library can tremendously
facilitate the use of LiRPA for the research community as well as industrial applications, such as
veriﬁable plant control [50]. Our library of LiRPA on general computational graphs can also inspire
further improved implementations on automatic outer bounds calculations with provable guarantees.

Although our focus on this paper has been on exploring known perturbations and providing guarantees
in such clairvoyant scenarios, in real-world an adversary (or nature) may not adhere to our assumptions.
Thus, we may additionally want to understand implication of these unknown scenarios on the system
performance. This is a relatively unexplored area in robust machine learning, and we encourage
researchers to understand and mitigate the risks arising from unknown perturbations in these contexts.",57,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,https://papers.nips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf,228.0
64,"
Model fusion is a fundamental building block in machine learning, as a way of direct knowledge
transfer between trained neural networks. Beyond theoretical interest it can serve a wide range of
concrete applications. For instance, collaborative learning schemes such as federated learning are
of increasing importance for enabling privacy-preserving training of ML models, as well as a better
alignment of each individual’s data ownership with the resulting utility from jointly trained machine
learning models, especially in applications where data is user-provided and privacy sensitive [29].
Here fusion of several models is a key building block to allow several agents to participate in joint
training and knowledge exchange. We propose that a reliable fusion technique can serve as a step
towards more broadly enabling privacy-preserving and efﬁcient collaborative learning.

","Sidak Pal Singh, Martin Jaggi",Model Fusion via Optimal Transport,1.0,{'EPFL'},,Deep learning,{'Switzerland'},False,False,"Model fusion is a fundamental building block in machine learning, as a way of direct knowledge transfer between trained neural networks. Beyond theoretical interest it can serve a wide range of concrete applications. For instance, collaborative learning schemes such as federated learning are of increasing importance for enabling privacy-preserving training of ML models, as well as a better alignment of each individual’s data ownership with the resulting utility from jointly trained machine learning models, especially in applications where data is user-provided and privacy sensitive [29]. Here fusion of several models is a key building block to allow several agents to participate in joint training and knowledge exchange. We propose that a reliable fusion technique can serve as a step towards more broadly enabling privacy-preserving and efficient collaborative learning.",Broader Impact,0.0,False,0.0,,"Sidak Pal Singh, Martin Jaggi",fb2697869f56484404c8ceee2985b01d,https://proceedings.neurips.cc/paper/2020/file/fb2697869f56484404c8ceee2985b01d-Paper.pdf,Model Fusion via Optimal Transport,Model Fusion via Optimal Transport,Deep Learning,Algorithms -> Boosting and Ensemble Methods; Algorithms -> Multitask and Transfer Learning,5.0,"
Model fusion is a fundamental building block in machine learning, as a way of direct knowledge
transfer between trained neural networks. Beyond theoretical interest it can serve a wide range of
concrete applications. For instance, collaborative learning schemes such as federated learning are
of increasing importance for enabling privacy-preserving training of ML models, as well as a better
alignment of each individual’s data ownership with the resulting utility from jointly trained machine
learning models, especially in applications where data is user-provided and privacy sensitive [29].
Here fusion of several models is a key building block to allow several agents to participate in joint
training and knowledge exchange. We propose that a reliable fusion technique can serve as a step
towards more broadly enabling privacy-preserving and efﬁcient collaborative learning.

",64,Model Fusion via Optimal Transport,https://papers.nips.cc/paper/2020/file/fb2697869f56484404c8ceee2985b01d-Paper.pdf,128.0
66,"The main type of broader impact caused by our work is the reduction of time, money and energy due
to the ability to continuously monitor data and hence make critical decisions early. In Appendix A,
we provide four prototypical examples of situations where our methods may prove useful. In Exam-
ple A, every single phone call requires time to collect the opinions, thus using up money as well, and
if we can accurately quantify uncertainty then we can stop collecting data sooner. In Example B,
randomization tests such as those involving permutations are a common way to quantify statistical
signiﬁcance, but they are computationally intensive and thus take up a lot of time. Knowing when
to stop, based on the test being clearly statistically signiﬁcant (or clearly far from it), can save on
energy costs. In Example D, when an educational intervention is unrolled one school at a time, there
are two possibilities again: if it is clearly beneﬁcial, we would like to recognize it quickly so that
every student can avail of the beneﬁts, while if it is for some reason harmful (e.g. causing stress
without measurable beneﬁt), then it would be equally important to end the program quickly. Once
more, accurately quantifying uncertainty as the process unfolds underpins the ability to make these
decisions early to disseminate beneﬁts rapidly or mitigate harms quickly.
As a side remark, though we have not demonstrated it in this paper, our techniques are also applicable
to auditing elections (checking whether the results are as announced by a manual random recount).
‘Risk-limiting audits’ [22] constitute a full-ﬂedged application area that we intend to pursue in future work; there are many variants depending on how voters express their preferences (choose one, or
rank all, or score all) and the aggregation mechanism used to decide on one or multiple winners.
Audits are not currently required by law in many state/county (or federal) elections due to high
perceived effort among other reasons, so being able to stop these audits early, yet accurately and
conﬁdently, is critical to their broad adoption. In this sense, a longer-term broader impact to trust in
elections is anticipated.

","Ian Waudby-Smith, Aaditya Ramdas",Confidence sequences for sampling without replacement,1.0,"{'CMU', 'Carnegie Mellon University'}",,Theory (including computational and statistical analyses),{'USA'},False,False,"The main type of broader impact caused by our work is the reduction of time, money and energy due to the ability to continuously monitor data and hence make critical decisions early. In Appendix A, we provide four prototypical examples of situations where our methods may prove useful. In Exam- ple A, every single phone call requires time to collect the opinions, thus using up money as well, and if we can accurately quantify uncertainty then we can stop collecting data sooner. In Example B, randomization tests such as those involving permutations are a common way to quantify statistical significance, but they are computationally intensive and thus take up a lot of time. Knowing when to stop, based on the test being clearly statistically significant (or clearly far from it), can save on energy costs. In Example D, when an educational intervention is unrolled one school at a time, there are two possibilities again: if it is clearly beneficial, we would like to recognize it quickly so that every student can avail of the benefits, while if it is for some reason harmful (e.g. causing stress without measurable benefit), then it would be equally important to end the program quickly. Once more, accurately quantifying uncertainty as the process unfolds underpins the ability to make these decisions early to disseminate benefits rapidly or mitigate harms quickly. As a side remark, though we have not demonstrated it in this paper, our techniques are also applicable to auditing elections (checking whether the results are as announced by a manual random recount). ‘Risk-limiting audits’ [ 22] constitute a full-fledged application area that we intend to pursue in future work; there are many variants depending on how voters express their preferences (choose one, or rank all, or score all) and the aggregation mechanism used to decide on one or multiple winners. Audits are not currently required by law in many state/county (or federal) elections due to high perceived effort among other reasons, so being able to stop these audits early, yet accurately and confidently, is critical to their broad adoption. In this sense, a longer-term broader impact to trust in elections is anticipated.",Broader impact,0.0,False,0.0,,"Ian Waudby-Smith, Aaditya Ramdas",e96c7de8f6390b1e6c71556e4e0a4959,https://proceedings.neurips.cc/paper/2020/file/e96c7de8f6390b1e6c71556e4e0a4959-Paper.pdf,Confidence sequences for sampling without replacement,Confidence sequences for sampling without replacement,Theory -> Frequentist Statistics,Algorithms -> Uncertainty Estimation,11.0,"The main type of broader impact caused by our work is the reduction of time, money and energy due
to the ability to continuously monitor data and hence make critical decisions early. In Appendix A,
we provide four prototypical examples of situations where our methods may prove useful. In Exam-
ple A, every single phone call requires time to collect the opinions, thus using up money as well, and
if we can accurately quantify uncertainty then we can stop collecting data sooner. In Example B,
randomization tests such as those involving permutations are a common way to quantify statistical
signiﬁcance, but they are computationally intensive and thus take up a lot of time. Knowing when
to stop, based on the test being clearly statistically signiﬁcant (or clearly far from it), can save on
energy costs. In Example D, when an educational intervention is unrolled one school at a time, there
are two possibilities again: if it is clearly beneﬁcial, we would like to recognize it quickly so that
every student can avail of the beneﬁts, while if it is for some reason harmful (e.g. causing stress
without measurable beneﬁt), then it would be equally important to end the program quickly. Once
more, accurately quantifying uncertainty as the process unfolds underpins the ability to make these
decisions early to disseminate beneﬁts rapidly or mitigate harms quickly.
As a side remark, though we have not demonstrated it in this paper, our techniques are also applicable
to auditing elections (checking whether the results are as announced by a manual random recount).
‘Risk-limiting audits’ [22] constitute a full-ﬂedged application area that we intend to pursue in future work; there are many variants depending on how voters express their preferences (choose one, or
rank all, or score all) and the aggregation mechanism used to decide on one or multiple winners.
Audits are not currently required by law in many state/county (or federal) elections due to high
perceived effort among other reasons, so being able to stop these audits early, yet accurately and
conﬁdently, is critical to their broad adoption. In this sense, a longer-term broader impact to trust in
elections is anticipated.

",66,Confidence sequences for sampling without replacement,https://papers.nips.cc/paper/2020/file/e96c7de8f6390b1e6c71556e4e0a4959-Paper.pdf,358.0
70,"
There is a burgeoning recent literature of statistical estimation and adaptive data analysis of the
higher-order structural properties of graphs in both the streaming and non streaming context that
reﬂect the importance and interest of this topic for the graph algorithms and relational learning
research community. On the other hand, shrinkage estimators are an established technique from
more general statistics. This paper is the ﬁrst to apply shrinkage based methods in the context of
graph approximation. The expected broader impact is as a proof of concept that shows the way for
other researchers in this area to improve estimation quality. Moreover, this work ﬁts under statistical
inference for temporal relational/network data, which would enable statistical analysis and learning
for network data that appear in streaming settings, in particular when exact solutions are not feasible
(similar to the important literature on randomization algorithms for data matrices [1]).
Furthermore, there are many applications where the data has a pronounced temporal, relational, and
spatial structure (e.g., relational data). Examples of Non-IID streams include (i) non-independence
due to temporal clustering in communication graphs on internet, online social networks, physical
contact networks, and social media such as ﬂash crowds and coordinated botnet activity; (ii) non-
identical distributions in activity on these networks due to diurnal and other seasonal variations,
synchronization of user network activity e.g., searches stimulated by hourly news reports. The
proposed framework is suitable for these applications, because it makes no statistical assumptions
concerning the arrival stream and the order of the arriving edges.

","Nesreen Ahmed, Nick Duffield",Adaptive Shrinkage Estimation for Streaming Graphs,0.0,{'Intel Labs'},,Theory (including computational and statistical analyses),{'USA'},False,False,"There is a burgeoning recent literature of statistical estimation and adaptive data analysis of the higher-order structural properties of graphs in both the streaming and non streaming context that reflect the importance and interest of this topic for the graph algorithms and relational learning research community. On the other hand, shrinkage estimators are an established technique from more general statistics. This paper is the first to apply shrinkage based methods in the context of graph approximation. The expected broader impact is as a proof of concept that shows the way for other researchers in this area to improve estimation quality. Moreover, this work fits under statistical inference for temporal relational/network data, which would enable statistical analysis and learning for network data that appear in streaming settings, in particular when exact solutions are not feasible (similar to the important literature on randomization algorithms for data matrices [1]). Furthermore, there are many applications where the data has a pronounced temporal, relational, and spatial structure (e.g., relational data). Examples of Non-IID streams include (i) non-independence due to temporal clustering in communication graphs on internet, online social networks, physical contact networks, and social media such as flash crowds and coordinated botnet activity; (ii) non- identical distributions in activity on these networks due to diurnal and other seasonal variations, synchronization of user network activity e.g., searches stimulated by hourly news reports. The proposed framework is suitable for these applications, because it makes no statistical assumptions concerning the arrival stream and the order of the arriving edges.",Broader Impact,1.0,False,0.0,,"Nesreen Ahmed, Nick Duffield",780261c4b9a55cd803080619d0cc3e11,https://proceedings.neurips.cc/paper/2020/file/780261c4b9a55cd803080619d0cc3e11-Paper.pdf,Adaptive Shrinkage Estimation for Streaming Graphs,Adaptive Shrinkage Estimation for Streaming Graphs,Algorithms -> Adaptive Data Analysis,Applications -> Network Analysis,8.0,"
There is a burgeoning recent literature of statistical estimation and adaptive data analysis of the
higher-order structural properties of graphs in both the streaming and non streaming context that
reﬂect the importance and interest of this topic for the graph algorithms and relational learning
research community. On the other hand, shrinkage estimators are an established technique from
more general statistics. This paper is the ﬁrst to apply shrinkage based methods in the context of
graph approximation. The expected broader impact is as a proof of concept that shows the way for
other researchers in this area to improve estimation quality. Moreover, this work ﬁts under statistical
inference for temporal relational/network data, which would enable statistical analysis and learning
for network data that appear in streaming settings, in particular when exact solutions are not feasible
(similar to the important literature on randomization algorithms for data matrices [1]).
Furthermore, there are many applications where the data has a pronounced temporal, relational, and
spatial structure (e.g., relational data). Examples of Non-IID streams include (i) non-independence
due to temporal clustering in communication graphs on internet, online social networks, physical
contact networks, and social media such as ﬂash crowds and coordinated botnet activity; (ii) non-
identical distributions in activity on these networks due to diurnal and other seasonal variations,
synchronization of user network activity e.g., searches stimulated by hourly news reports. The
proposed framework is suitable for these applications, because it makes no statistical assumptions
concerning the arrival stream and the order of the arriving edges.

",70,Adaptive Shrinkage Estimation for Streaming Graphs,https://papers.nips.cc/paper/2020/file/780261c4b9a55cd803080619d0cc3e11-Paper.pdf,251.0
77,"From the sculptor’s chisel to the painter’s brush, tools for creative expression are an important part
of human culture. The advent of digital photography and professional editing tools, such as Adobe
Photoshop, has allowed artists to push creative boundaries. However, the existing tools are typically
too complicated to be useful by the general public. Our work is one of the new generation of visual
content creation methods that aim to democratize the creative process. The goal is to provide intuitive
controls (see Section 4.6) for making a wider range of realistic visual effects available to non-experts.

While the goal of this work is to support artistic and creative applications, the potential misuse of such
technology for purposes of deception – posing generated images as real photographs – is quite concern-
ing. To partially mitigate this concern, we can use the advances in the ﬁeld of image forensics [16], as a
way of verifying the authenticity of a given image. In particular, Wang et al. [72] recently showed that a
classiﬁer trained to classify between real photographs and synthetic images generated by ProGAN [42],
was able to detect fakes produced by other generators, among them, StyleGAN [43] and Style-
GAN2 [44]. We take a pretrained model of [72] and report the detection rates on several datasets in Ap-
pendix ??. Our swap-generated images can be detected with an average rate greater than 90%, and this in-
dicates that our method shares enough architectural components with previous methods to be detectable.
However, these detection methods do not work at 100%, and performance can degrade as the images are
degraded in the wild (e.g., compressed, rescanned) or via adversarial attacks. Therefore, the problem of
verifying image provenance remains a signiﬁcant challenge to society that requires multiple layers of
solutions, from technical (such as learning-based detection systems or authenticity certiﬁcation chains),
to social, such as efforts to increase public awareness of the problem, to regulatory and legislative.","Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, Richard Zhang",Swapping Autoencoder for Deep Image Manipulation,1.0,"{'Adobe', 'UC Berkeley', 'Adobe Research', 'Adobe Research, US', 'Adobe, CMU'}",,Vision,{'USA'},True,False,"From the sculptor’s chisel to the painter’s brush, tools for creative expression are an important part of human culture. The advent of digital photography and professional editing tools, such as Adobe Photoshop, has allowed artists to push creative boundaries. However, the existing tools are typically too complicated to be useful by the general public. Our work is one of the new generation of visual content creation methods that aim to democratize the creative process. The goal is to provide intuitive controls (see Section 4.6) for making a wider range of realistic visual effects available to non-experts. While the goal of this work is to support artistic and creative applications, the potential misuse of such technology for purposes of deception – posing generated images as real photographs – is quite concern- ing. To partially mitigate this concern, we can use the advances in the field of image forensics [16], as a way of verifying the authenticity of a given image. In particular, Wang et al. [72] recently showed that a classifier trained to classify between real photographs and synthetic images generated by ProGAN [42], was able to detect fakes produced by other generators, among them, StyleGAN [43] and Style- GAN2 [44]. We take a pretrained model of [72] and report the detection rates on several datasets in Appendix ?? . Our swap-generated images can be detected with an average rate greater than 90%, and this in- dicates that our method shares enough architectural components with previous methods to be detectable. However, these detection methods do not work at 100 % , and performance can degrade as the images are degraded in the wild (e.g., compressed, rescanned) or via adversarial attacks. Therefore, the problem of verifying image provenance remains a significant challenge to society that requires multiple layers of solutions, from technical (such as learning-based detection systems or authenticity certification chains), to social, such as efforts to increase public awareness of the problem, to regulatory and legislative.",Broader Impact,1.0,False,1.0,,"Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, Richard Zhang",50905d7b2216bfeccb5b41016357176b,https://proceedings.neurips.cc/paper/2020/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,Swapping Autoencoder for Deep Image Manipulation,Swapping Autoencoder for Deep Image Manipulation,Applications -> Computational Photography,Deep Learning -> Deep Autoencoders; Deep Learning -> Generative Models,14.0,"From the sculptor’s chisel to the painter’s brush, tools for creative expression are an important part
of human culture. The advent of digital photography and professional editing tools, such as Adobe
Photoshop, has allowed artists to push creative boundaries. However, the existing tools are typically
too complicated to be useful by the general public. Our work is one of the new generation of visual
content creation methods that aim to democratize the creative process. The goal is to provide intuitive
controls (see Section 4.6) for making a wider range of realistic visual effects available to non-experts.

While the goal of this work is to support artistic and creative applications, the potential misuse of such
technology for purposes of deception – posing generated images as real photographs – is quite concern-
ing. To partially mitigate this concern, we can use the advances in the ﬁeld of image forensics [16], as a
way of verifying the authenticity of a given image. In particular, Wang et al. [72] recently showed that a
classiﬁer trained to classify between real photographs and synthetic images generated by ProGAN [42],
was able to detect fakes produced by other generators, among them, StyleGAN [43] and Style-
GAN2 [44]. We take a pretrained model of [72] and report the detection rates on several datasets in Ap-
pendix ??. Our swap-generated images can be detected with an average rate greater than 90%, and this in-
dicates that our method shares enough architectural components with previous methods to be detectable.
However, these detection methods do not work at 100%, and performance can degrade as the images are
degraded in the wild (e.g., compressed, rescanned) or via adversarial attacks. Therefore, the problem of
verifying image provenance remains a signiﬁcant challenge to society that requires multiple layers of
solutions, from technical (such as learning-based detection systems or authenticity certiﬁcation chains),
to social, such as efforts to increase public awareness of the problem, to regulatory and legislative.",77,Swapping Autoencoder for Deep Image Manipulation,https://papers.nips.cc/paper/2020/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,325.0
88,"BlockGAN is an image generative model that learns an object-oriented 3D scene representation directly
from unlabelled 2D images. Our approach is a new machine learning technique that makes it possible
to generate unseen images from a noise vector, with unprecedented control over the identity and pose
of multiple independent objects as well as the background. In the long term, our approach could enable
powerful tools for digital artists that facilitate artistic control over realistic procedurally generated
digital content. However, any tool can in principle be abused, for example by adding new, manipulating
or removing existing objects or people from images.

At training time, our network performs a task somewhat akin to scene understanding, as our approach
learns to disentangle between multiple objects and individual object properties (speciﬁcally their pose
and identity). At test time, our approach enables sampling new images with control over pose and
identity for each object in the scene, but does not directly take any image input. However, it is possible
to embed images into the latent space of generative models [1]. A highly realistic generative image
model and a good image ﬁt would then make it possible to approximate the input image and, more
importantly, to edit the individual objects in a pictured scene. Similar to existing image editing software,
this enables the creation of image manipulations that could be used for ill-intended misinformation
(fake news), but also for a wide range of creative and other positive applications. We expect the beneﬁts
of positive applications to clearly outweigh the potential downsides of malicious applications.","Thu H. Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, Niloy Mitra",BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,1.0,"{'Adobe Research', 'University College London', 'University of Bath'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)","{'UK', 'USA'}",True,False,"BlockGAN is an image generative model that learns an object-oriented 3D scene representation directly from unlabelled 2D images. Our approach is a new machine learning technique that makes it possible to generate unseen images from a noise vector, with unprecedented control over the identity and pose of multiple independent objects as well as the background. In the long term, our approach could enable powerful tools for digital artists that facilitate artistic control over realistic procedurally generated digital content. However, any tool can in principle be abused, for example by adding new, manipulating or removing existing objects or people from images. At training time, our network performs a task somewhat akin to scene understanding, as our approach learns to disentangle between multiple objects and individual object properties (specifically their pose and identity). At test time, our approach enables sampling new images with control over pose and identity for each object in the scene, but does not directly take any image input. However, it is possible to embed images into the latent space of generative models [1]. A highly realistic generative image model and a good image fit would then make it possible to approximate the input image and, more importantly, to edit the individual objects in a pictured scene. Similar to existing image editing software, this enables the creation of image manipulations that could be used for ill-intended misinformation (fake news), but also for a wide range of creative and other positive applications. We expect the benefits of positive applications to clearly outweigh the potential downsides of malicious applications.",Broader Impact,1.0,True,1.0,,"Thu H. Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, Niloy Mitra",4b29fa4efe4fb7bc667c7b301b74d52d,https://proceedings.neurips.cc/paper/2020/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf,BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,Deep Learning -> Generative Models,Applications -> Computer Vision,10.0,"BlockGAN is an image generative model that learns an object-oriented 3D scene representation directly
from unlabelled 2D images. Our approach is a new machine learning technique that makes it possible
to generate unseen images from a noise vector, with unprecedented control over the identity and pose
of multiple independent objects as well as the background. In the long term, our approach could enable
powerful tools for digital artists that facilitate artistic control over realistic procedurally generated
digital content. However, any tool can in principle be abused, for example by adding new, manipulating
or removing existing objects or people from images.

At training time, our network performs a task somewhat akin to scene understanding, as our approach
learns to disentangle between multiple objects and individual object properties (speciﬁcally their pose
and identity). At test time, our approach enables sampling new images with control over pose and
identity for each object in the scene, but does not directly take any image input. However, it is possible
to embed images into the latent space of generative models [1]. A highly realistic generative image
model and a good image ﬁt would then make it possible to approximate the input image and, more
importantly, to edit the individual objects in a pictured scene. Similar to existing image editing software,
this enables the creation of image manipulations that could be used for ill-intended misinformation
(fake news), but also for a wide range of creative and other positive applications. We expect the beneﬁts
of positive applications to clearly outweigh the potential downsides of malicious applications.",88,BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,https://papers.nips.cc/paper/2020/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf,258.0
132,"The purpose of our work is to improve the general supervised learning performance of SNNs. Even
though we can use SNNs for any cognitive task, the complexity of problems that SNNs are currently
targeting is very limited. This is because of the fundamental problems of the existing learning
methods that are addressed in this work. Nevertheless, SNNs have significant implications as a
biologically plausible artificial neural network, which helps bridge the gap between our understanding of biological neurons and the remarkable success of deep learning. In particular, the successful use of
SNNs can provide clues to the high energy efficiency of the biological brains. We believe our work
lays the groundwork for such a research direction.","Jinseok Kim, Kyungsu Kim, Jae-Joon Kim",Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks,1.0,{'POSTECH'},,Deep learning,{'France'},False,False,"The purpose of our work is to improve the general supervised learning performance of SNNs. Even though we can use SNNs for any cognitive task, the complexity of problems that SNNs are currently targeting is very limited. This is because of the fundamental problems of the existing learning methods that are addressed in this work. Nevertheless, SNNs have significant implications as a biologically plausible artificial neural network, which helps bridge the gap between our understanding  of biological neurons and the remarkable success of deep learning. In particular, the successful use of SNNs can provide clues to the high energy efficiency of the biological brains. We believe our work lays the groundwork for such a research direction.",Broader Impact,0.0,False,0.0,,"Jinseok Kim, Kyungsu Kim, Jae-Joon Kim",e2e5096d574976e8f115a8f1e0ffb52b,https://proceedings.neurips.cc/paper/2020/file/e2e5096d574976e8f115a8f1e0ffb52b-Paper.pdf,Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks,Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks,Deep Learning -> Biologically Plausible Deep Networks,,6.0,"The purpose of our work is to improve the general supervised learning performance of SNNs. Even
though we can use SNNs for any cognitive task, the complexity of problems that SNNs are currently
targeting is very limited. This is because of the fundamental problems of the existing learning
methods that are addressed in this work. Nevertheless, SNNs have significant implications as a
biologically plausible artificial neural network, which helps bridge the gap between our understanding of biological neurons and the remarkable success of deep learning. In particular, the successful use of
SNNs can provide clues to the high energy efficiency of the biological brains. We believe our work
lays the groundwork for such a research direction.",132,Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks,https://papers.nips.cc/paper/2020/file/e2e5096d574976e8f115a8f1e0ffb52b-Paper.pdf,116.0
135,"
Enzyme reduces the amount of work necessary to apply ML to new domains. This has a generally
positive impact as it reduces the workload necessary by researchers to use ML. It could be negative,
however, for those whose job manually rewrites existing code for ML frameworks. Similarly, this
added accessibility advances various scientiﬁc problem domains with all the positives and negatives
that come with it. Enzyme also provides generally positive impact by helping bridge the gap between
the ML and the scientiﬁc computing communities through allowing them to share tools and more
easily interoperate. As an example, Enzyme may allow for improved policy design for climate change
via projected-gradient-descent on a climate simulator.

","William Moses, Valentin Churavy","Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients",1.0,"{'MIT', 'Massachussets Institute of Technology'}",,"Datasets, challenges, software",{'USA'},False,False,"Enzyme reduces the amount of work necessary to apply ML to new domains. This has a generally positive impact as it reduces the workload necessary by researchers to use ML. It could be negative, however, for those whose job manually rewrites existing code for ML frameworks. Similarly, this added accessibility advances various scientific problem domains with all the positives and negatives that come with it. Enzyme also provides generally positive impact by helping bridge the gap between the ML and the scientific computing communities through allowing them to share tools and more easily interoperate. As an example, Enzyme may allow for improved policy design for climate change via projected-gradient-descent on a climate simulator.",Broader Impact,0.0,False,0.0,,"William Moses, Valentin Churavy",9332c513ef44b682e9347822c2e457ac,https://proceedings.neurips.cc/paper/2020/file/9332c513ef44b682e9347822c2e457ac-Paper.pdf,"Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients","Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients","Data, Challenges, Implementations, and Software -> Software Toolkits","Data, Challenges, Implementations, and Software -> Benchmarks; Deep Learning -> Efficient Training Methods",6.0,"
Enzyme reduces the amount of work necessary to apply ML to new domains. This has a generally
positive impact as it reduces the workload necessary by researchers to use ML. It could be negative,
however, for those whose job manually rewrites existing code for ML frameworks. Similarly, this
added accessibility advances various scientiﬁc problem domains with all the positives and negatives
that come with it. Enzyme also provides generally positive impact by helping bridge the gap between
the ML and the scientiﬁc computing communities through allowing them to share tools and more
easily interoperate. As an example, Enzyme may allow for improved policy design for climate change
via projected-gradient-descent on a climate simulator.

",135,"Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients",https://papers.nips.cc/paper/2020/file/9332c513ef44b682e9347822c2e457ac-Paper.pdf,113.0
148,"
The widely usage of deep neural networks which require large amount of computation resource is
putting pressure on the energy source and the natural environment. The proposed model shrinking
method for obtaining tiny neural networks is beneﬁcial to energy conservation and environment
protection.","Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing XU, Tong Zhang","Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets",1.0,"{'Tencent AI Lab', 'Huawei Technologies', 'Beijing University of Posts and Telecommunications'}",False,Deep learning,{'China'},False,False,The widely usage of deep neural networks which require large amount of computation resource is putting pressure on the energy source and the natural environment. The proposed model shrinking method for obtaining tiny neural networks is beneficial to energy conservation and environment protection.,Broader Impact,1.0,False,1.0,False,"Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing XU, Tong Zhang",e069ea4c9c233d36ff9c7f329bc08ff1,https://proceedings.neurips.cc/paper/2020/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf,"Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets","Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets",Deep Learning,Applications -> Object Recognition; Deep Learning -> CNN Architectures; Deep Learning -> Efficient Inference Methods,2.0,"
The widely usage of deep neural networks which require large amount of computation resource is
putting pressure on the energy source and the natural environment. The proposed model shrinking
method for obtaining tiny neural networks is beneﬁcial to energy conservation and environment
protection.",148,"Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets",https://papers.nips.cc/paper/2020/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf,43.0
169,"
Reinforcement learning has achieved great success in areas such as robotics and game playing, and
thus has aroused broad interests and more potential real-world applications. Double Q-learning is a
commonly used technique in deep reinforcement learning to improve the implementation stability and
speed of deep Q-learning. In this paper, we provided the fundamental analysis on the convergence
rate for double Q-learning, which theoretically justiﬁed the empirical success of double Q-learning in
practice. Such a theory also provides practitioners desirable performance guarantee to further develop
such a technique into various transferable technologies.

","Huaqing Xiong, Lin Zhao, Yingbin Liang, Wei  Zhang",Finite-Time Analysis for Double Q-learning,1.0,"{'The Ohio State University', 'Southern University of Science and Technology', 'Ohio State University', 'National University of Singapore'}",,Reinforcement learning theory,"{'Singapore', 'USA', 'China'}",False,False,"Reinforcement learning has achieved great success in areas such as robotics and game playing, and thus has aroused broad interests and more potential real-world applications. Double Q-learning is a commonly used technique in deep reinforcement learning to improve the implementation stability and speed of deep Q-learning. In this paper, we provided the fundamental analysis on the convergence rate for double Q-learning, which theoretically justified the empirical success of double Q-learning in practice. Such a theory also provides practitioners desirable performance guarantee to further develop such a technique into various transferable technologies.",Broader Impact,0.0,False,0.0,,"Huaqing Xiong, Lin Zhao, Yingbin Liang, Wei  Zhang",c20bb2d9a50d5ac1f713f8b34d9aac5a,https://proceedings.neurips.cc/paper/2020/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf,Finite-Time Analysis for Double Q-learning,Finite-Time Analysis for Double Q-learning,Reinforcement Learning and Planning -> Markov Decision Processes,Reinforcement Learning and Planning -> Decision and Control,4.0,"
Reinforcement learning has achieved great success in areas such as robotics and game playing, and
thus has aroused broad interests and more potential real-world applications. Double Q-learning is a
commonly used technique in deep reinforcement learning to improve the implementation stability and
speed of deep Q-learning. In this paper, we provided the fundamental analysis on the convergence
rate for double Q-learning, which theoretically justiﬁed the empirical success of double Q-learning in
practice. Such a theory also provides practitioners desirable performance guarantee to further develop
such a technique into various transferable technologies.

",169,Finite-Time Analysis for Double Q-learning,https://papers.nips.cc/paper/2020/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf,91.0
179,"
We investigated a low-shot visual reasoning problem, which requires a higher-level relational reason-
ing skills that goes beyond perception of each individual elements. To this end, we introduce a new
visual analogical learning framework that allowed the model to learn relational structure among the set
elements from a pair of problems, which achieves signiﬁcantly improved generalization performance
with a limited amount of training samples, and generalizes well to problems with unseen attributes.
Although we only considered a speciﬁc visual reasoning problem in this work, the proposed method
is sufﬁciently general and we believe that the same analogical learning scheme could be applied to
a wide variety of applications from non-vision domains, such as natural language understanding.
Analogical reasoning is also a unique property of human intelligence and thus our work may be in
the footsteps of the progress toward implementing artiﬁcial general intelligence (AGI).

","Youngsung Kim, Jinwoo Shin, Eunho Yang, Sung Ju Hwang",Few-shot Visual Reasoning with Meta-Analogical Contrastive Learning,,,,,,,,,,,,,,,,,,,,,,"
We investigated a low-shot visual reasoning problem, which requires a higher-level relational reason-
ing skills that goes beyond perception of each individual elements. To this end, we introduce a new
visual analogical learning framework that allowed the model to learn relational structure among the set
elements from a pair of problems, which achieves signiﬁcantly improved generalization performance
with a limited amount of training samples, and generalizes well to problems with unseen attributes.
Although we only considered a speciﬁc visual reasoning problem in this work, the proposed method
is sufﬁciently general and we believe that the same analogical learning scheme could be applied to
a wide variety of applications from non-vision domains, such as natural language understanding.
Analogical reasoning is also a unique property of human intelligence and thus our work may be in
the footsteps of the progress toward implementing artiﬁcial general intelligence (AGI).

",179,,https://papers.nips.cc/paper/2020/file/c39e1a03859f9ee215bc49131d0caf33-Paper.pdf,
188,"
Our work provides realizable tools for private data analysis. Given recent concerns centered around
large-scale data collection and surveillance, the production of a mature and robust set of tools which
preserve privacy can help assuage public fears involving misuse of personal data. Additionally, we
hope that developing tools which approach the non-private accuracy will inspire companies to adopt
privacy by default.

As differential privacy requires technical domain knowledge, incorrect use or misinterpretation of
differential privacy is unfortunately easy and can lead to negative side effects including providing
a misleading or false sense of security. Such issues can be avoided by sufﬁcient training and/or
consultation with experts in data privacy, although this may present more of a challenge for smaller,
resource-constrained organizations.

","Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman",CoinPress: Practical Private Mean and Covariance Estimation,1.0,"{'University of Waterloo', 'Microsoft', 'Northeastern University'}",,"Social aspects of machine learning (e.g., fairness, safety, privacy)","{'Canada', 'USA'}",True,False,"Our work provides realizable tools for private data analysis. Given recent concerns centered around large-scale data collection and surveillance, the production of a mature and robust set of tools which preserve privacy can help assuage public fears involving misuse of personal data. Additionally, we hope that developing tools which approach the non-private accuracy will inspire companies to adopt privacy by default. As differential privacy requires technical domain knowledge, incorrect use or misinterpretation of differential privacy is unfortunately easy and can lead to negative side effects including providing a misleading or false sense of security. Such issues can be avoided by sufficient training and/or consultation with experts in data privacy, although this may present more of a challenge for smaller, resource-constrained organizations.",Broader Impact,1.0,True,1.0,,"Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman",a684eceee76fc522773286a895bc8436,https://proceedings.neurips.cc/paper/2020/file/a684eceee76fc522773286a895bc8436-Paper.pdf,CoinPress: Practical Private Mean and Covariance Estimation,CoinPress: Practical Private Mean and Covariance Estimation,"Social Aspects of Machine Learning -> Privacy, Anonymity, and Security",,5.0,"
Our work provides realizable tools for private data analysis. Given recent concerns centered around
large-scale data collection and surveillance, the production of a mature and robust set of tools which
preserve privacy can help assuage public fears involving misuse of personal data. Additionally, we
hope that developing tools which approach the non-private accuracy will inspire companies to adopt
privacy by default.

As differential privacy requires technical domain knowledge, incorrect use or misinterpretation of
differential privacy is unfortunately easy and can lead to negative side effects including providing
a misleading or false sense of security. Such issues can be avoided by sufﬁcient training and/or
consultation with experts in data privacy, although this may present more of a challenge for smaller,
resource-constrained organizations.

",188,CoinPress: Practical Private Mean and Covariance Estimation,https://papers.nips.cc/paper/2020/file/a684eceee76fc522773286a895bc8436-Paper.pdf,121.0
191,"
In this paper we describe a technique to fundamentally improve training for CNNs. This paper has
impact wherever CNNs are used, since they can also be trained using the same regime, which would
results in improved task-performance. Applications of CNNs, such as object recognition, can be
used for good or malicious purposes. Any user or practitioner has the ultimate impact and authority
on how to deploy such a network in practice. The user can use our proposed strategy to improve
their underlying machine learning algorithm, and deploy it in whichever way they choose.

","Samarth Sinha, Animesh Garg, Hugo Larochelle",Curriculum By Smoothing,,,,,,,,,,,,,,,,,,,,,,"
In this paper we describe a technique to fundamentally improve training for CNNs. This paper has
impact wherever CNNs are used, since they can also be trained using the same regime, which would
results in improved task-performance. Applications of CNNs, such as object recognition, can be
used for good or malicious purposes. Any user or practitioner has the ultimate impact and authority
on how to deploy such a network in practice. The user can use our proposed strategy to improve
their underlying machine learning algorithm, and deploy it in whichever way they choose.

",191,,https://papers.nips.cc/paper/2020/file/f6a673f09493afcd8b129a0bcf1cd5bc-Paper.pdf,
230,"
This is a purely theoretical paper. We develop technical tools that make Mat´ern Gaussian processes
easier to work with in the Riemannian setting. This enables practitioners who are not experts in
stochastic partial differential equations to model data that lives on spaces such as spheres and tori.

We envision the impact of this work to be concentrated in the physical sciences, where spaces of
this type occur naturally. Since the state spaces of most robotic arms are Riemannian manifolds,
we expect these ideas to improve performance of model-based reinforcement learning by making it
easier to incorporate geometric prior information into models.

Since climate science is concerned with studying the globe, we also expect that our ideas can be
used to model environmental phenomena, such as sea surface temperatures. By employing Gaussian
processes for data assimilation and building them into larger frameworks, this could facilitate more
accurate climate models compared to current methods.

These impacts carry forward to potential generalizations of our work. We encourage practitioners to
consider impacts on their respective disciplines that arise from incorporating geometry into models.
","Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Deisenroth",Matérn Gaussian Processes on Riemannian Manifolds,,,,,,,,,,,,,,,,,,,,,,"
This is a purely theoretical paper. We develop technical tools that make Mat´ern Gaussian processes
easier to work with in the Riemannian setting. This enables practitioners who are not experts in
stochastic partial differential equations to model data that lives on spaces such as spheres and tori.

We envision the impact of this work to be concentrated in the physical sciences, where spaces of
this type occur naturally. Since the state spaces of most robotic arms are Riemannian manifolds,
we expect these ideas to improve performance of model-based reinforcement learning by making it
easier to incorporate geometric prior information into models.

Since climate science is concerned with studying the globe, we also expect that our ideas can be
used to model environmental phenomena, such as sea surface temperatures. By employing Gaussian
processes for data assimilation and building them into larger frameworks, this could facilitate more
accurate climate models compared to current methods.

These impacts carry forward to potential generalizations of our work. We encourage practitioners to
consider impacts on their respective disciplines that arise from incorporating geometry into models.
",230,,https://papers.nips.cc/paper/2020/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf,
234,"
In this paper, we develop the Shared Space Transfer Learning (SSTL) as a novel transfer learning
(TL) approach that can functionally align homogeneous multi-site fMRI datasets and so improve the
prediction performance in every site. Although the proposed method is used to analyzed multi-site
fMRI datasets, it can also be seen as a general-purpose machine learning method for any multi-view
domain adaption applications. The proposed method is evaluated by using publicly-available fMRI
datasets —- provided by Open NEURO. SSTL is an open-source technique and can also be used via
our GUI-based toolbox called easy fMRI. We do not anticipate any negative consequences for this
study. We believe that SSTL’s multi-view technique for transfer learning will have strong practical
applications — including, but not limited, neuroscience, computational psychiatry, human-brain
interface, etc. In the future, we plan to utilize the proposed framework to analyze high-level cognitive
processes such as movie stimuli.

","Muhammad Yousefnezhad, Alessandro Selvitella, Daoqiang Zhang, Andrew Greenshaw, Russell Greiner",Shared Space Transfer Learning for analyzing multi-site fMRI data,1.0,"{'Purdue University Fort Wayne', 'Nanjing University of Aeronautics and Astronautics', 'University of Alberta'}",,Neuroscience and cognitive science,"{'Canada', 'China'}",False,False,"In this paper, we develop the Shared Space Transfer Learning (SSTL) as a novel transfer learning (TL) approach that can functionally align homogeneous multi-site fMRI datasets and so improve the prediction performance in every site. Although the proposed method is used to analyzed multi-site fMRI datasets, it can also be seen as a general-purpose machine learning method for any multi-view domain adaption applications. The proposed method is evaluated by using publicly-available fMRI datasets —- provided by Open NEURO. SSTL is an open-source technique and can also be used via our GUI-based toolbox called easy fMRI. We do not anticipate any negative consequences for this study. We believe that SSTL’s multi-view technique for transfer learning will have strong practical applications — including, but not limited, neuroscience, computational psychiatry, human-brain interface, etc. In the future, we plan to utilize the proposed framework to analyze high-level cognitive processes such as movie stimuli.",Broader Impact,0.0,False,0.0,,"Muhammad Yousefnezhad, Alessandro Selvitella, Daoqiang Zhang, Andrew Greenshaw, Russell Greiner",b837305e43f7e535a1506fc263eee3ed,https://proceedings.neurips.cc/paper/2020/file/b837305e43f7e535a1506fc263eee3ed-Paper.pdf,Shared Space Transfer Learning for analyzing multi-site fMRI data,Shared Space Transfer Learning for analyzing multi-site fMRI data,Neuroscience and Cognitive Science -> Neuroscience,"Algorithms -> Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorithms -> Representation Learning; Neuroscience and Cognitive Science -> Brain Mapping; Neuroscience and Cognitive Science -> Cognitive Science",7.0,"
In this paper, we develop the Shared Space Transfer Learning (SSTL) as a novel transfer learning
(TL) approach that can functionally align homogeneous multi-site fMRI datasets and so improve the
prediction performance in every site. Although the proposed method is used to analyzed multi-site
fMRI datasets, it can also be seen as a general-purpose machine learning method for any multi-view
domain adaption applications. The proposed method is evaluated by using publicly-available fMRI
datasets —- provided by Open NEURO. SSTL is an open-source technique and can also be used via
our GUI-based toolbox called easy fMRI. We do not anticipate any negative consequences for this
study. We believe that SSTL’s multi-view technique for transfer learning will have strong practical
applications — including, but not limited, neuroscience, computational psychiatry, human-brain
interface, etc. In the future, we plan to utilize the proposed framework to analyze high-level cognitive
processes such as movie stimuli.

",234,Shared Space Transfer Learning for analyzing multi-site fMRI data,https://papers.nips.cc/paper/2020/file/b837305e43f7e535a1506fc263eee3ed-Paper.pdf,149.0
254,"
Machine learning and related technologies have already achieved remarkable performance in many
areas. Current methods still require intensive empirical efforts for network design and hyperparameter
ﬁne-tuning. Our research can search the optimal high order group attention module automatically,
and the searched module is computationally efﬁcient and generalizes well to various tasks. It will
help to build a strong deep neural network model automatically without having to rely on the manual
design of the attention architecture. Since machine learning can promote the development of industry,
healthcare, and education, AutoML can accelerate this process by offering various speciﬁc optimal
models that ﬁt different hardware platforms and latency constraints.

However, AutoML usually searches the model without domain knowledge, and may result in some
uncertain and unreliable models that will make confusing decisions. The excessive trust in these
decision will lead to many ethics issues. For example, when the diagnostic system optimized by
AutoML leads to the death of the patients or other property damage, who should be responsible
for this? What’s more, the abuse of AutoML may cause horrible disasters, especially in military
applications. Machine learning can optimize the design of weapons to make them adapted to the
speciﬁc operational conditions. AutoML will speed up this process and makes it possible to search
the optimal system under any different constraints and to customize the mass production of weapons.
The weapon design systems optimized by AutoML will cause a great threat to world peace, and we
advocate the AutoML will not apply to the ﬁeld of military or warfare.

Further, AutoML will tip the scales in favor of the developed countries that are developing these
technologies to improve living conditions across the board in a variety of ways. However, the labor
force in developing countries is largely unskilled, and the use of AutoML in many cases means higher
unemployment, lower-income, and more social unrest. The purpose of artiﬁcial intelligence in this
condition should be to enhance their workforce skills, not to replace them. As a researcher, we need
to work principally to make sure technology matches our values.

","Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao",Auto Learning Attention,1.0,"{'University of Sydney', 'The University of Sydney', 'Northwestern Polytechnical University'}",,AutoML,"{'Australia', 'China'}",True,False,"Machine learning and related technologies have already achieved remarkable performance in many areas. Current methods still require intensive empirical efforts for network design and hyperparameter fine-tuning. Our research can search the optimal high order group attention module automatically, and the searched module is computationally efficient and generalizes well to various tasks. It will help to build a strong deep neural network model automatically without having to rely on the manual design of the attention architecture. Since machine learning can promote the development of industry, healthcare, and education, AutoML can accelerate this process by offering various specific optimal models that fit different hardware platforms and latency constraints. However, AutoML usually searches the model without domain knowledge, and may result in some uncertain and unreliable models that will make confusing decisions. The excessive trust in these decision will lead to many ethics issues. For example, when the diagnostic system optimized by AutoML leads to the death of the patients or other property damage, who should be responsible for this? What’s more, the abuse of AutoML may cause horrible disasters, especially in military applications. Machine learning can optimize the design of weapons to make them adapted to the specific operational conditions. AutoML will speed up this process and makes it possible to search the optimal system under any different constraints and to customize the mass production of weapons. The weapon design systems optimized by AutoML will cause a great threat to world peace, and we advocate the AutoML will not apply to the field of military or warfare. Further, AutoML will tip the scales in favor of the developed countries that are developing these technologies to improve living conditions across the board in a variety of ways. However, the labor force in developing countries is largely unskilled, and the use of AutoML in many cases means higher unemployment, lower-income, and more social unrest. The purpose of artificial intelligence in this condition should be to enhance their workforce skills, not to replace them. As a researcher, we need to work principally to make sure technology matches our values.",Broader Impact,0.0,True,0.0,,"Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao",103303dd56a731e377d01f6a37badae3,https://proceedings.neurips.cc/paper/2020/file/103303dd56a731e377d01f6a37badae3-Paper.pdf,Auto Learning Attention,Auto Learning Attention,Applications -> Computer Vision,"Applications -> Body Pose, Face, and Gesture Analysis; Applications -> Object Detection; Applications -> Object Recognition",16.0,"
Machine learning and related technologies have already achieved remarkable performance in many
areas. Current methods still require intensive empirical efforts for network design and hyperparameter
ﬁne-tuning. Our research can search the optimal high order group attention module automatically,
and the searched module is computationally efﬁcient and generalizes well to various tasks. It will
help to build a strong deep neural network model automatically without having to rely on the manual
design of the attention architecture. Since machine learning can promote the development of industry,
healthcare, and education, AutoML can accelerate this process by offering various speciﬁc optimal
models that ﬁt different hardware platforms and latency constraints.

However, AutoML usually searches the model without domain knowledge, and may result in some
uncertain and unreliable models that will make confusing decisions. The excessive trust in these
decision will lead to many ethics issues. For example, when the diagnostic system optimized by
AutoML leads to the death of the patients or other property damage, who should be responsible
for this? What’s more, the abuse of AutoML may cause horrible disasters, especially in military
applications. Machine learning can optimize the design of weapons to make them adapted to the
speciﬁc operational conditions. AutoML will speed up this process and makes it possible to search
the optimal system under any different constraints and to customize the mass production of weapons.
The weapon design systems optimized by AutoML will cause a great threat to world peace, and we
advocate the AutoML will not apply to the ﬁeld of military or warfare.

Further, AutoML will tip the scales in favor of the developed countries that are developing these
technologies to improve living conditions across the board in a variety of ways. However, the labor
force in developing countries is largely unskilled, and the use of AutoML in many cases means higher
unemployment, lower-income, and more social unrest. The purpose of artiﬁcial intelligence in this
condition should be to enhance their workforce skills, not to replace them. As a researcher, we need
to work principally to make sure technology matches our values.

",254,Auto Learning Attention,https://papers.nips.cc/paper/2020/file/103303dd56a731e377d01f6a37badae3-Paper.pdf,344.0
263,"
This work is theoretical in nature, and is concerned with the very general framework of stochastic
optimization. As such, there are no foreseen ethical or societal consequences for the research presented
herein. We hope that by providing theoretical groundwork and algorithmic techniques for e�cient
large-scale optimization in settings informed by modern developments in optimization, works like this
one will contribute to alleviating the steep resource and energy costs of large-scale machine learning.

","Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, Cyril Zhang",Stochastic Optimization with Laggard Data Pipelines,1.0,"{'Google', 'Princeton University'}",,Optimization Methods (continuous or discrete),{'USA'},False,False,"This work is theoretical in nature, and is concerned with the very general framework of stochastic optimization. As such, there are no foreseen ethical or societal consequences for the research presented herein. We hope that by providing theoretical groundwork and algorithmic techniques for e cient large-scale optimization in settings informed by modern developments in optimization, works like this one will contribute to alleviating the steep resource and energy costs of large-scale machine learning.",Broader Impact,1.0,False,1.0,,"Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, Cyril Zhang",74dbd1111727a31a2b825d615d80b2e7,https://proceedings.neurips.cc/paper/2020/file/74dbd1111727a31a2b825d615d80b2e7-Paper.pdf,Stochastic Optimization with Laggard Data Pipelines,Stochastic Optimization with Laggard Data Pipelines,Optimization,Optimization -> Convex Optimization; Optimization -> Stochastic Optimization,3.0,"
This work is theoretical in nature, and is concerned with the very general framework of stochastic
optimization. As such, there are no foreseen ethical or societal consequences for the research presented
herein. We hope that by providing theoretical groundwork and algorithmic techniques for e�cient
large-scale optimization in settings informed by modern developments in optimization, works like this
one will contribute to alleviating the steep resource and energy costs of large-scale machine learning.

",263,Stochastic Optimization with Laggard Data Pipelines,https://papers.nips.cc/paper/2020/file/74dbd1111727a31a2b825d615d80b2e7-Paper.pdf,73.0
264,"
This paper investigates the (statistical) consistency and efﬁciency of two existing methods for
estimating target domain label distributions. While this could potentially guide practitioners to
improve detection, estimation, and classiﬁcation in applications where the label shift assumption
holds, we do not believe that it will fundamentally impact how machine learning is used in a way that
could conceivably be socially salient. While we take the potential impact of machine learning on
society seriously, we believe that this work, which addresses a foundational theoretical problem, does
not present a signiﬁcant societal concern.

","Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, Zachary Lipton",A Unified View of Label Shift Estimation,1.0,"{'CMU', 'Carnegie Mellon University'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"This paper investigates the (statistical) consistency and efficiency of two existing methods for estimating target domain label distributions. While this could potentially guide practitioners to improve detection, estimation, and classification in applications where the label shift assumption holds, we do not believe that it will fundamentally impact how machine learning is used in a way that could conceivably be socially salient. While we take the potential impact of machine learning on society seriously, we believe that this work, which addresses a foundational theoretical problem, does not present a significant societal concern.",Broader Impact,0.0,False,0.0,,"Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, Zachary Lipton",219e052492f4008818b8adb6366c7ed6,https://proceedings.neurips.cc/paper/2020/file/219e052492f4008818b8adb6366c7ed6-Paper.pdf,A Unified View of Label Shift Estimation,A Unified View of Label Shift Estimation,Algorithms -> Multitask and Transfer Learning,Theory -> Frequentist Statistics; Theory -> Statistical Learning Theory,3.0,"
This paper investigates the (statistical) consistency and efﬁciency of two existing methods for
estimating target domain label distributions. While this could potentially guide practitioners to
improve detection, estimation, and classiﬁcation in applications where the label shift assumption
holds, we do not believe that it will fundamentally impact how machine learning is used in a way that
could conceivably be socially salient. While we take the potential impact of machine learning on
society seriously, we believe that this work, which addresses a foundational theoretical problem, does
not present a signiﬁcant societal concern.

",264,A Unified View of Label Shift Estimation,https://papers.nips.cc/paper/2020/file/219e052492f4008818b8adb6366c7ed6-Paper.pdf,91.0
288,"
Quantum computation has recently been attracting growing attentions owing to its potential for
achieving computational speedups compared to any conventional classical computation that runs
on existing computers, opening the new ﬁeld of accelerating machine learning tasks via quantum
computation: quantum machine learning. To attain a large quantum speedup, however, existing
algorithms for quantum machine learning require extreme assumptions on sparsity and low rank of
matrices used in the algorithms, which limit applicability of the quantum computation to machine
learning tasks. In contrast, the novelty of this research is to achieve an exponential speedup in quantum
machine learning without the sparsity and low-rank assumptions, broadening the applicability of
quantum machine learning.

Advantageously, our quantum algorithm eliminates the computational bottleneck faced by a class of
existing classical algorithms for scaling up kernel-based learning algorithms by means of random
features. In particular, using this quantum algorithm, we can achieve the learning with the nearly
optimal number of features, whereas this optimization has been hard to realize due to the bottleneck
in the existing classical algorithms. A drawback of our quantum algorithm may arise from the fact
that we use powerful quantum subroutines for achieving the large speedup, and these subroutines are
hard to implement on existing or near-term quantum devices that cannot achieve universal quantum
computation due to noise. At the same time, these subroutines make our quantum algorithm hard to
simulate by classical computation, from which stems the computational advantage of our quantum
algorithm over the existing classical algorithms. Thus, our results open a route to a widely applicable
framework of kernel-based quantum machine learning with an exponential speedup, leading to a
promising candidate of “killer applications” of universal quantum computers.

","Hayata Yamasaki, Sathyawageeswar  Subramanian, Sho Sonoda, Masato Koashi",Learning with Optimized Random Features: Exponential Speedup by Quantum Machine Learning without Sparsity and Low-Rank Assumptions,1.0,"{'University of Cambridge', 'RIKEN AIP', 'The University of Tokyo'}",,"Quantum machine learning, theory, supervised learning, computational complexity","{'Japan', 'UK'}",False,False,"Quantum computation has recently been attracting growing attentions owing to its potential for achieving computational speedups compared to any conventional classical computation that runs on existing computers, opening the new field of accelerating machine learning tasks via quantum computation: quantum machine learning . To attain a large quantum speedup, however, existing algorithms for quantum machine learning require extreme assumptions on sparsity and low rank of matrices used in the algorithms, which limit applicability of the quantum computation to machine learning tasks. In contrast, the novelty of this research is to achieve an exponential speedup in quantum machine learning without the sparsity and low-rank assumptions, broadening the applicability of quantum machine learning. Advantageously, our quantum algorithm eliminates the computational bottleneck faced by a class of existing classical algorithms for scaling up kernel-based learning algorithms by means of random features. In particular, using this quantum algorithm, we can achieve the learning with the nearly optimal number of features, whereas this optimization has been hard to realize due to the bottleneck in the existing classical algorithms. A drawback of our quantum algorithm may arise from the fact that we use powerful quantum subroutines for achieving the large speedup, and these subroutines are hard to implement on existing or near-term quantum devices that cannot achieve universal quantum computation due to noise. At the same time, these subroutines make our quantum algorithm hard to simulate by classical computation, from which stems the computational advantage of our quantum algorithm over the existing classical algorithms. Thus, our results open a route to a widely applicable framework of kernel-based quantum machine learning with an exponential speedup, leading to a promising candidate of “killer applications” of universal quantum computers.",Broader Impact,0.0,False,0.0,,"Hayata Yamasaki, Sathyawageeswar  Subramanian, Sho Sonoda, Masato Koashi",9ddb9dd5d8aee9a76bf217a2a3c54833,https://proceedings.neurips.cc/paper/2020/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf,Learning with Optimized Random Features: Exponential Speedup by Quantum Machine Learning without Sparsity and Low-Rank Assumptions,Learning with Optimized Random Features: Exponential Speedup by Quantum Machine Learning without Sparsity and Low-Rank Assumptions,Applications -> Quantum Learning,Algorithms -> Kernel Methods; Theory -> Computational Learning Theory,8.0,"
Quantum computation has recently been attracting growing attentions owing to its potential for
achieving computational speedups compared to any conventional classical computation that runs
on existing computers, opening the new ﬁeld of accelerating machine learning tasks via quantum
computation: quantum machine learning. To attain a large quantum speedup, however, existing
algorithms for quantum machine learning require extreme assumptions on sparsity and low rank of
matrices used in the algorithms, which limit applicability of the quantum computation to machine
learning tasks. In contrast, the novelty of this research is to achieve an exponential speedup in quantum
machine learning without the sparsity and low-rank assumptions, broadening the applicability of
quantum machine learning.

Advantageously, our quantum algorithm eliminates the computational bottleneck faced by a class of
existing classical algorithms for scaling up kernel-based learning algorithms by means of random
features. In particular, using this quantum algorithm, we can achieve the learning with the nearly
optimal number of features, whereas this optimization has been hard to realize due to the bottleneck
in the existing classical algorithms. A drawback of our quantum algorithm may arise from the fact
that we use powerful quantum subroutines for achieving the large speedup, and these subroutines are
hard to implement on existing or near-term quantum devices that cannot achieve universal quantum
computation due to noise. At the same time, these subroutines make our quantum algorithm hard to
simulate by classical computation, from which stems the computational advantage of our quantum
algorithm over the existing classical algorithms. Thus, our results open a route to a widely applicable
framework of kernel-based quantum machine learning with an exponential speedup, leading to a
promising candidate of “killer applications” of universal quantum computers.

",288,Learning with Optimized Random Features: Exponential Speedup by Quantum Machine Learning without Sparsity and Low-Rank Assumptions,https://papers.nips.cc/paper/2020/file/9ddb9dd5d8aee9a76bf217a2a3c54833-Paper.pdf,280.0
289,"
A notion of distance is such a basic and fundamental concept that it is most often used as a primitive
from which other tools and methods derive utility. In the speciﬁc case of the dataset distance we
propose here, it would most likely be used as tool within a machine learning pipeline. Thus, by its very
nature, the prospect of potential impact of this work is broad enough to essentially encompass most
settings where machine learning is used. In this statement, we focus on aspects that are immediate,
tractable, and precise enough to be discussed constructively in this format.

Perhaps the most immediate impact of this work could be through its application in transfer learning.
Improvements in this paradigm can have a myriad outcomes, ranging from societal to environmental,
both within and beyond the machine learning community. Among potential beneﬁcial outcomes, one
that stands out is the environmental impact of making transfer learning more efﬁcient by providing
guidance as to what resources to use for pretraining (§6.2) or choosing optimal data augmentations
(§6.3). This would be particularly relevant for NLP, where the carbon footprint of models has grown
exponentially in recent years, driven largely by pretraining of very large models on massive datasets
[49]. Another beneﬁcial outcome of this speciﬁc use of the distance proposed in this work rests on
the intuition that more efﬁcient transfer learning would could erode or mitigate economic barriers that
currently limit large-scale data pretraining and adaptation to resource-rich entities and institutions.
However, work studying the impact of improved data and method efﬁciency has pointed out that this
intuition is perhaps too optimistic, as there are various unexpected yet feasible negative collateral
consequences of increased efﬁciency, e. g., in terms of privacy, data markets and misuse [51].

We next highlight a few potential failure modes of this work. The modeling approximations used here
to make this notion of distance efﬁciently computable, in particular the use of Gaussian distribution for
modeling same-class collections, might prove too unrealistic in some datasets, leading to unreliable
distance estimation. This, of course, could have negative impact on downstream applications that
would rely on this distance as a sub-component, especially so given how deeply embedded within an
ML pipeline it would be. In order to mitigate such impact, we suggest the practitioner verify how
realistic these modeling assumptions are for the application at hand. On the other hand, despite the
limited number of hyperparameters the computation of this distance relies on, inadequate choices
for these (e. g., the entropy regularization parameter "") might nevertheless lead to unreliable or
imprecise results. Again, care should be taken in test the validity of the parameters, ideally running
sanity-checks on identical or near-identical datasets to corroborate that the results are sensible.

","David Alvarez Melis, Nicolo Fusi",Geometric Dataset Distances via Optimal Transport,1.0,"{'MIT', 'Microsoft Research'}",,AutoML,{'USA'},True,False,"A notion of distance is such a basic and fundamental concept that it is most often used as a primitive from which other tools and methods derive utility. In the specific case of the dataset distance we propose here, it would most likely be used as tool within a machine learning pipeline. Thus, by its very nature, the prospect of potential impact of this work is broad enough to essentially encompass most settings where machine learning is used. In this statement, we focus on aspects that are immediate, tractable, and precise enough to be discussed constructively in this format. Perhaps the most immediate impact of this work could be through its application in transfer learning. Improvements in this paradigm can have a myriad outcomes, ranging from societal to environmental, both within and beyond the machine learning community. Among potential beneficial outcomes, one that stands out is the environmental impact of making transfer learning more efficient by providing guidance as to what resources to use for pretraining (§6.2) or choosing optimal data augmentations (§6.3). This would be particularly relevant for NLP, where the carbon footprint of models has grown exponentially in recent years, driven largely by pretraining of very large models on massive datasets [49]. Another beneficial outcome of this specific use of the distance proposed in this work rests on the intuition that more efficient transfer learning would could erode or mitigate economic barriers that currently limit large-scale data pretraining and adaptation to resource-rich entities and institutions. However, work studying the impact of improved data and method efficiency has pointed out that this intuition is perhaps too optimistic, as there are various unexpected yet feasible negative collateral consequences of increased efficiency, e. g., in terms of privacy, data markets and misuse [51]. We next highlight a few potential failure modes of this work. The modeling approximations used here to make this notion of distance efficiently computable, in particular the use of Gaussian distribution for modeling same-class collections, might prove too unrealistic in some datasets, leading to unreliable distance estimation. This, of course, could have negative impact on downstream applications that would rely on this distance as a sub-component, especially so given how deeply embedded within an ML pipeline it would be. In order to mitigate such impact, we suggest the practitioner verify how realistic these modeling assumptions are for the application at hand. On the other hand, despite the limited number of hyperparameters the computation of this distance relies on, inadequate choices for these ( e. g., the entropy regularization parameter "" ) might nevertheless lead to unreliable or imprecise results. Again, care should be taken in test the validity of the parameters, ideally running sanity-checks on identical or near-identical datasets to corroborate that the results are sensible.",Broader Impact,1.0,True,1.0,,"David Alvarez Melis, Nicolo Fusi",f52a7b2610fb4d3f74b4106fb80b233d,https://proceedings.neurips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf,Geometric Dataset Distances via Optimal Transport,Geometric Dataset Distances via Optimal Transport,Optimization,Algorithms -> Multitask and Transfer Learning; Algorithms -> Similarity and Distance Learning; Deep Learning -> Efficient Training Methods,16.0,"
A notion of distance is such a basic and fundamental concept that it is most often used as a primitive
from which other tools and methods derive utility. In the speciﬁc case of the dataset distance we
propose here, it would most likely be used as tool within a machine learning pipeline. Thus, by its very
nature, the prospect of potential impact of this work is broad enough to essentially encompass most
settings where machine learning is used. In this statement, we focus on aspects that are immediate,
tractable, and precise enough to be discussed constructively in this format.

Perhaps the most immediate impact of this work could be through its application in transfer learning.
Improvements in this paradigm can have a myriad outcomes, ranging from societal to environmental,
both within and beyond the machine learning community. Among potential beneﬁcial outcomes, one
that stands out is the environmental impact of making transfer learning more efﬁcient by providing
guidance as to what resources to use for pretraining (§6.2) or choosing optimal data augmentations
(§6.3). This would be particularly relevant for NLP, where the carbon footprint of models has grown
exponentially in recent years, driven largely by pretraining of very large models on massive datasets
[49]. Another beneﬁcial outcome of this speciﬁc use of the distance proposed in this work rests on
the intuition that more efﬁcient transfer learning would could erode or mitigate economic barriers that
currently limit large-scale data pretraining and adaptation to resource-rich entities and institutions.
However, work studying the impact of improved data and method efﬁciency has pointed out that this
intuition is perhaps too optimistic, as there are various unexpected yet feasible negative collateral
consequences of increased efﬁciency, e. g., in terms of privacy, data markets and misuse [51].

We next highlight a few potential failure modes of this work. The modeling approximations used here
to make this notion of distance efﬁciently computable, in particular the use of Gaussian distribution for
modeling same-class collections, might prove too unrealistic in some datasets, leading to unreliable
distance estimation. This, of course, could have negative impact on downstream applications that
would rely on this distance as a sub-component, especially so given how deeply embedded within an
ML pipeline it would be. In order to mitigate such impact, we suggest the practitioner verify how
realistic these modeling assumptions are for the application at hand. On the other hand, despite the
limited number of hyperparameters the computation of this distance relies on, inadequate choices
for these (e. g., the entropy regularization parameter "") might nevertheless lead to unreliable or
imprecise results. Again, care should be taken in test the validity of the parameters, ideally running
sanity-checks on identical or near-identical datasets to corroborate that the results are sensible.

",289,Geometric Dataset Distances via Optimal Transport,https://papers.nips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf,458.0
158,"
Our work focuses on approximating Gaussian processes using a mixture of practical methods and
theoretical analysis to reconﬁgure data in ways that reduce their approximation complexity. As such,
it could have signiﬁcant broader impact by allowing users to more accurately solve practical problems
such as the ones discussed in our introduction, while still providing concrete theoretical guarantees.
While applications of our work to real data could result in ethical considerations, this is an indirect
(and unpredictable) side-effect of our work. Our experimental work uses publicly available datasets
to evaluate the performance of our algorithms; no ethical considerations are raised.","Minh Hoang, Nghia Hoang, Hai Pham, David Woodruff",Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes,1.0,"{'IBM Research', 'Carnegie Mellon University'}",,,{'USA'},False,False,"Our work focuses on approximating Gaussian processes using a mixture of practical methods and theoretical analysis to reconfigure data in ways that reduce their approximation complexity. As such, it could have significant broader impact by allowing users to more accurately solve practical problems such as the ones discussed in our introduction, while still providing concrete theoretical guarantees. While applications of our work to real data could result in ethical considerations, this is an indirect (and unpredictable) side-effect of our work. Our experimental work uses publicly available datasets to evaluate the performance of our algorithms; no ethical considerations are raised.",6 Statement of Broader Impact,1.0,False,1.0,,"Minh Hoang, Nghia Hoang, Hai Pham, David Woodruff",95b431e51fc53692913da5263c214162,https://proceedings.neurips.cc/paper/2020/file/95b431e51fc53692913da5263c214162-Paper.pdf,Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes,Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes,Theory -> Statistical Learning Theory,Algorithms -> Kernel Methods,4.0,"
Our work focuses on approximating Gaussian processes using a mixture of practical methods and
theoretical analysis to reconﬁgure data in ways that reduce their approximation complexity. As such,
it could have signiﬁcant broader impact by allowing users to more accurately solve practical problems
such as the ones discussed in our introduction, while still providing concrete theoretical guarantees.
While applications of our work to real data could result in ethical considerations, this is an indirect
(and unpredictable) side-effect of our work. Our experimental work uses publicly available datasets
to evaluate the performance of our algorithms; no ethical considerations are raised.",158,Revisiting the Sample Complexity of Sparse Spectrum Approximation of Gaussian Processes,https://papers.nips.cc/paper/2020/file/95b431e51fc53692913da5263c214162-Paper.pdf,99.0
78,"
Our “Group Contextual Encoding” can be directly applied to the 3D point cloud scene understanding
tasks including 3D object detection, voxel labeling, and segmentation. Our research can also support
downstream research and applications such as autonomous driving, robotics, and AR/MR. We will
investigate the generalizability of our method to other tasks and frameworks, e.g., Graph Convolution
network, 3D sparse CNNs, where the global context plays a crucial role in these tasks.

On the other hand, this technology may also endanger the employment of human servants and drivers
because they may be replaced by autonomous robots and vehicles, which may cause the potential
social problems. This issue should be taken seriously and measures should be taken for preparation.

","Xu Liu, Chengtao Li, Jian Wang, Jingbo Wang, Boxin Shi, Xiaodong He",Group Contextual Encoding for 3D Point Clouds,1.0,"{'Peking University', 'Carnegie Mellon University', 'JD AI research', 'MIT', 'The University of Tokyo'}",,Vision,"{'Japan', 'USA', 'China'}",False,False,"Our “Group Contextual Encoding” can be directly applied to the 3D point cloud scene understanding tasks including 3D object detection, voxel labeling, and segmentation. Our research can also support downstream research and applications such as autonomous driving, robotics, and AR/MR. We will investigate the generalizability of our method to other tasks and frameworks, e.g., Graph Convolution network, 3D sparse CNNs, where the global context plays a crucial role in these tasks. On the other hand, this technology may also endanger the employment of human servants and drivers because they may be replaced by autonomous robots and vehicles, which may cause the potential social problems. This issue should be taken seriously and measures should be taken for preparation.",Broader Impact,1.0,False,1.0,,"Xu Liu, Chengtao Li, Jian Wang, Jingbo Wang, Boxin Shi, Xiaodong He",9b72e31dac81715466cd580a448cf823,https://proceedings.neurips.cc/paper/2020/file/9b72e31dac81715466cd580a448cf823-Paper.pdf,Group Contextual Encoding for 3D Point Clouds,Group Contextual Encoding for 3D Point Clouds,Applications -> Computer Vision,Applications -> Object Detection; Applications -> Robotics,5.0,"
Our “Group Contextual Encoding” can be directly applied to the 3D point cloud scene understanding
tasks including 3D object detection, voxel labeling, and segmentation. Our research can also support
downstream research and applications such as autonomous driving, robotics, and AR/MR. We will
investigate the generalizability of our method to other tasks and frameworks, e.g., Graph Convolution
network, 3D sparse CNNs, where the global context plays a crucial role in these tasks.

On the other hand, this technology may also endanger the employment of human servants and drivers
because they may be replaced by autonomous robots and vehicles, which may cause the potential
social problems. This issue should be taken seriously and measures should be taken for preparation.

",78,Group Contextual Encoding for 3D Point Clouds,https://papers.nips.cc/paper/2020/file/9b72e31dac81715466cd580a448cf823-Paper.pdf,117.0
81,"Who may beneﬁt from this research? The VR / AR software developers and 3D graphics designers
may beneﬁt from our research. The proposed technique generates single-view clothed human mesh
reconstructions with improved global topology regularities and local surface details. Our method
can beneﬁt various VR / AR applications that involve reconstructing 3D virtual human avatars for
customized user experience, such as conference systems and role-playing games. Moreover, being
able to efﬁciently reconstruct 3D meshes from single-view images is useful for graphics rendering
and 3D designs.

Who may be put at disadvantage from this research? In the long run, some entry-level graphics
artists and designers might be affected. Generally speaking, the 3D gaming and graphics design
industries are moving towards automatic content generation techniques. These techniques are not
meant to replace highly skilled human workers, but to help improve their productivity at work.

What are the consequences of failure of the system? Failed human mesh reconstructions might
bring unpleasant user experience. Typical failure cases as well as possible solutions have also been
discussed in the main paper.

Whether the task/method leverages biases in the data? There might be some biases on human
poses and clothes due to long-tail cases. However, our dataset is already 10× larger than the one used
in the competing methods. More importantly, our mesh collection procedures can be easily expanded
to other domain-speciﬁc scenarios to obtain more human meshes with different shapes, poses and
clothes to compensate for long-tail cases.","Tong He, John Collomosse, Hailin Jin, Stefano Soatto",Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,1.0,"{'UCLA', 'Adobe Research', 'Adobe'}",,Vision,{'USA'},False,False,"Who may benefit from this research? The VR / AR software developers and 3D graphics designers may benefit from our research. The proposed technique generates single-view clothed human mesh reconstructions with improved global topology regularities and local surface details. Our method can benefit various VR / AR applications that involve reconstructing 3D virtual human avatars for customized user experience, such as conference systems and role-playing games. Moreover, being able to efficiently reconstruct 3D meshes from single-view images is useful for graphics rendering and 3D designs. Who may be put at disadvantage from this research? In the long run, some entry-level graphics artists and designers might be affected. Generally speaking, the 3D gaming and graphics design industries are moving towards automatic content generation techniques. These techniques are not meant to replace highly skilled human workers, but to help improve their productivity at work. What are the consequences of failure of the system? Failed human mesh reconstructions might bring unpleasant user experience. Typical failure cases as well as possible solutions have also been discussed in the main paper. Whether the task/method leverages biases in the data? There might be some biases on human poses in the and competing clothes methods. due to long-tail More importantly, cases. However, our mesh our dataset collection is already procedures 10 ⇥ larger can be than easily the expanded one used to other domain-specific scenarios to obtain more human meshes with different shapes, poses and clothes to compensate for long-tail cases.",Broader Impact,1.0,False,1.0,,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",690f44c8c2b7ded579d01abe8fdb6110,https://proceedings.neurips.cc/paper/2020/file/690f44c8c2b7ded579d01abe8fdb6110-Paper.pdf,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,"Applications -> Body Pose, Face, and Gesture Analysis",Algorithms -> Representation Learning; Applications -> Computer Vision,16.0,"Who may beneﬁt from this research? The VR / AR software developers and 3D graphics designers
may beneﬁt from our research. The proposed technique generates single-view clothed human mesh
reconstructions with improved global topology regularities and local surface details. Our method
can beneﬁt various VR / AR applications that involve reconstructing 3D virtual human avatars for
customized user experience, such as conference systems and role-playing games. Moreover, being
able to efﬁciently reconstruct 3D meshes from single-view images is useful for graphics rendering
and 3D designs.

Who may be put at disadvantage from this research? In the long run, some entry-level graphics
artists and designers might be affected. Generally speaking, the 3D gaming and graphics design
industries are moving towards automatic content generation techniques. These techniques are not
meant to replace highly skilled human workers, but to help improve their productivity at work.

What are the consequences of failure of the system? Failed human mesh reconstructions might
bring unpleasant user experience. Typical failure cases as well as possible solutions have also been
discussed in the main paper.

Whether the task/method leverages biases in the data? There might be some biases on human
poses and clothes due to long-tail cases. However, our dataset is already 10× larger than the one used
in the competing methods. More importantly, our mesh collection procedures can be easily expanded
to other domain-speciﬁc scenarios to obtain more human meshes with different shapes, poses and
clothes to compensate for long-tail cases.",81,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,https://papers.nips.cc/paper/2020/file/690f44c8c2b7ded579d01abe8fdb6110-Paper.pdf,244.0
172,"
Research presented in the paper has a potential to positively contribute to a number of practical
applications where establishing temporal correspondence in video is critical, among them pedestrian
safely in automotive settings, patient monitoring in hospitals and elderly care homes, video-based
animal monitoring and 3D reconstruction, etc. However, there is also a potential for the technology
to be used for nefarious purposes, mainly in the area of unauthorized surveillance, especially by autocratic regimes. As partial mitigation, we commit to not entering into any contracts involving
this technology with any government or quasi-governmental agencies of countries with an EIU
Democracy Index [24] score of 4.0 or below (“authoritarian regimes""), or authorizing them to use our
software.

","Allan Jabri, Andrew Owens, Alexei Efros",Space-Time Correspondence as a Contrastive Random Walk,1.0,{'UC Berkeley'},,,{'USA'},False,False,"Research presented in the paper has a potential to positively contribute to a number of practical applications where establishing temporal correspondence in video is critical, among them pedestrian safely in automotive settings, patient monitoring in hospitals and elderly care homes, video-based animal monitoring and 3D reconstruction, etc. However, there is also a potential for the technology to be used for nefarious purposes, mainly in the area of unauthorized surveillance, especially by  autocratic regimes. As partial mitigation, we commit to not entering into any contracts involving this technology with any government or quasi-governmental agencies of countries with an EIU Democracy Index [24] score of 4 . 0 or below (“authoritarian regimes""), or authorizing them to use our software. Acknowledgments. We thank Amir Zamir, Ashish Kumar, Yu Sun, Tim Brooks, Bill Peebles, Dave Epstein, Armand Joulin, and Jitendra Malik for helpful feedback and support. We are also grateful to the wonderful members of VGG for hosting us during a dreamy semester at Oxford. This work would not have been possible without the hospitality of Port Meadow and the swimming pool on Iffley Road. Research was supported, in part, by NSF grant IIS-1633310, the DARPA MCS program, and NSF IIS-1522904. We are grateful for compute resources donated by NVIDIA. AJ is supported by the PD Soros Fellowship.",6 Broader Impact,0.0,False,0.0,,"Allan Jabri, Andrew Owens, Alexei Efros",e2ef524fbf3d9fe611d5a8e90fefdc9c,https://proceedings.neurips.cc/paper/2020/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,Space-Time Correspondence as a Contrastive Random Walk,Space-Time Correspondence as a Contrastive Random Walk,Applications -> Computer Vision,Applications -> Tracking and Motion in Video; Applications -> Video Analysis,11.0,"
Research presented in the paper has a potential to positively contribute to a number of practical
applications where establishing temporal correspondence in video is critical, among them pedestrian
safely in automotive settings, patient monitoring in hospitals and elderly care homes, video-based
animal monitoring and 3D reconstruction, etc. However, there is also a potential for the technology
to be used for nefarious purposes, mainly in the area of unauthorized surveillance, especially by autocratic regimes. As partial mitigation, we commit to not entering into any contracts involving
this technology with any government or quasi-governmental agencies of countries with an EIU
Democracy Index [24] score of 4.0 or below (“authoritarian regimes""), or authorizing them to use our
software.

",172,Space-Time Correspondence as a Contrastive Random Walk,https://papers.nips.cc/paper/2020/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,214.0
214,"Our work is largely motivated by resource constrained health intervention delivery. This setting is
common in both the Global North and South where
community health workers (CHWs) are recruited to
deliver basic care to a cohort of patients or benefactors. In fact, CHWs have been critical to achieving
global health initiatives for over five decades, and evidence shows that CHWs have had a positive impact
in myriad domains including maternal and newborn
health [6, 9], (non-)communicable diseases [6, 29],
and sexual/reproductive health [37] in low-resource
communities across the world [7, 9, 29, 35]. Our modeling has the potential to improve the delivery
of care in these highly resource-constrained settings.
However, a deployment of our system to any setting must be done responsibly.
For instance, the system is designed with the intention of assisting human CHWs plan limited
interventions. However, we also present results that highlight our algorithm’s ability to plan among
thousands of processes at a time, far more than for which a human could independently plan. Just
making this capability available could encourage the automation of applicable interventions via
automated calls or texts, potentially displacing CHW jobs, reducing human contact with patients, and
unfairly limiting care for patients with limited access to technology.
Additionally, users of the system must be dutifully aware that its recommendations will be based
solely on the data entered in the system. In the context of medication adherence monitoring, if the
worker makes an error when entering data, e.g., the patient was adhering (“good” state) but they
accidentally mark the patient as not-adhering (“bad” state), then the algorithm could make the wrong
recommendation about the patient the next day, since its belief of the patient’s adherence could be
very wrong. Finally, our AI system is inherently a blackbox scheduler which would likely be replacing an
interpretable scheduling heuristic. This would limit any user or administrator’s ability to audit
decisions around why certain patients were recommended for intervention. As with any potential
deployment of a blackbox system to a domain that affects the allocation of resources to humans,
system designers should be acutely aware of balance between their needs to be able to perform audits
vs. their need for optimization.","Aditya Mate, Jackson Killian, Haifeng Xu, Andrew Perrault, Milind Tambe",Collapsing Bandits and Their Application to Public Health Intervention,,,,,,,,,,,,,,,,,,,,,,"Our work is largely motivated by resource constrained health intervention delivery. This setting is
common in both the Global North and South where
community health workers (CHWs) are recruited to
deliver basic care to a cohort of patients or benefactors. In fact, CHWs have been critical to achieving
global health initiatives for over five decades, and evidence shows that CHWs have had a positive impact
in myriad domains including maternal and newborn
health [6, 9], (non-)communicable diseases [6, 29],
and sexual/reproductive health [37] in low-resource
communities across the world [7, 9, 29, 35]. Our modeling has the potential to improve the delivery
of care in these highly resource-constrained settings.
However, a deployment of our system to any setting must be done responsibly.
For instance, the system is designed with the intention of assisting human CHWs plan limited
interventions. However, we also present results that highlight our algorithm’s ability to plan among
thousands of processes at a time, far more than for which a human could independently plan. Just
making this capability available could encourage the automation of applicable interventions via
automated calls or texts, potentially displacing CHW jobs, reducing human contact with patients, and
unfairly limiting care for patients with limited access to technology.
Additionally, users of the system must be dutifully aware that its recommendations will be based
solely on the data entered in the system. In the context of medication adherence monitoring, if the
worker makes an error when entering data, e.g., the patient was adhering (“good” state) but they
accidentally mark the patient as not-adhering (“bad” state), then the algorithm could make the wrong
recommendation about the patient the next day, since its belief of the patient’s adherence could be
very wrong. Finally, our AI system is inherently a blackbox scheduler which would likely be replacing an
interpretable scheduling heuristic. This would limit any user or administrator’s ability to audit
decisions around why certain patients were recommended for intervention. As with any potential
deployment of a blackbox system to a domain that affects the allocation of resources to humans,
system designers should be acutely aware of balance between their needs to be able to perform audits
vs. their need for optimization.",214,,https://papers.nips.cc/paper/2020/file/b460cf6b09878b00a3e1ad4c72344ccd-Paper.pdf,
218,"
Adversarial attacks, especially ones under more realistic threat models, pose several important
security, ethical, and privacy risks. In this work, we introduce the NoBox attack setting, which
generalizes many other blackbox transfer settings, and we provide a novel framework to ground and
study attacks theoretically and their transferability to other functions within a class of functions. As
the NoBox threat model represents a more realistic setting for adversarial attacks, our research has
the potential to be used against a class of machine learning models in the wild. In particular, in terms
of risk, malicious actors could use approaches based on our framework to generate attack vectors that
compromise production ML systems or potentially bias them toward speciﬁc outcomes.

As a concrete example, one can consider creating transferrable examples in the physical world, such
as the computer vision systems of autonomous cars. While prior works have shown the possibility of
such adversarial examples —i.e., adversarial trafﬁc signs, we note that there is a signiﬁcant gap in
translating synthetic adversarial examples to adversarial examples that reside in the physical world
[45]. Understanding and analyzing the NoBox transferability of adversarial examples to the physical
world—in order to provide public and academic visibility on these risks—is an important direction for
future research. Based on the known risks of designing new kinds of adversarial attacks—discussed
above—we now outline the ways in which our research is informed by the intent to mitigate these
potential societal risks. For instance, our research demonstrates that one can successfully craft
adversarial attacks even in the challenging NoBox setting. It raises many important considerations
when developing robustness approaches. A straightforward extension is to consider our adversarial
example game (AEG) framework as a tool for training robust models. On the theoretical side,
exploring formal veriﬁcation of neural networks against NoBox adversaries is an exciting direction
for continued exploration. As an application, ML practitioners in the industry may choose to employ
new forms of A/B testing with different types of adversarial examples, of which AEG is one method
to robustify and stress test production systems further. Such an application falls in line with other
general approaches to red teaming AI systems [10] and veriﬁability in AI development. In essence,
the goal of such approaches, including adversarial examples for robustness, is to align AI systems’
failure modes to those found in human decision making.

","Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, Will Hamilton",Adversarial Example Games,1.0,"{'McGill/MILA', 'Mila/McGill', 'McGill', 'MILA', 'Mila'}",,Adversarial examples,{'Canada'},False,False,"Adversarial attacks, especially ones under more realistic threat models, pose several important security, ethical, and privacy risks. In this work, we introduce the NoBox attack setting, which generalizes many other blackbox transfer settings, and we provide a novel framework to ground and study attacks theoretically and their transferability to other functions within a class of functions. As the NoBox threat model represents a more realistic setting for adversarial attacks, our research has the potential to be used against a class of machine learning models in the wild. In particular, in terms of risk, malicious actors could use approaches based on our framework to generate attack vectors that compromise production ML systems or potentially bias them toward specific outcomes. As a concrete example, one can consider creating transferrable examples in the physical world, such as the computer vision systems of autonomous cars. While prior works have shown the possibility of such adversarial examples —i.e., adversarial traffic signs, we note that there is a significant gap in translating synthetic adversarial examples to adversarial examples that reside in the physical world [45]. Understanding and analyzing the NoBox transferability of adversarial examples to the physical world—in order to provide public and academic visibility on these risks—is an important direction for future research. Based on the known risks of designing new kinds of adversarial attacks—discussed above—we now outline the ways in which our research is informed by the intent to mitigate these potential societal risks. For instance, our research demonstrates that one can successfully craft adversarial attacks even in the challenging NoBox setting. It raises many important considerations when developing robustness approaches. A straightforward extension is to consider our adversarial example game (AEG) framework as a tool for training robust models. On the theoretical side, exploring formal verification of neural networks against NoBox adversaries is an exciting direction for continued exploration. As an application, ML practitioners in the industry may choose to employ new forms of A/B testing with different types of adversarial examples, of which AEG is one method to robustify and stress test production systems further. Such an application falls in line with other general approaches to red teaming AI systems [10] and verifiability in AI development. In essence, the goal of such approaches, including adversarial examples for robustness, is to align AI systems’ failure modes to those found in human decision making.",Broader Impact,0.0,False,0.0,,"Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, Will Hamilton",65586803f1435736f42a541d3a924595,https://proceedings.neurips.cc/paper/2020/file/65586803f1435736f42a541d3a924595-Paper.pdf,Adversarial Example Games,Adversarial Example Games,Algorithms -> Adversarial Learning,Deep Learning -> Adversarial Networks,15.0,"
Adversarial attacks, especially ones under more realistic threat models, pose several important
security, ethical, and privacy risks. In this work, we introduce the NoBox attack setting, which
generalizes many other blackbox transfer settings, and we provide a novel framework to ground and
study attacks theoretically and their transferability to other functions within a class of functions. As
the NoBox threat model represents a more realistic setting for adversarial attacks, our research has
the potential to be used against a class of machine learning models in the wild. In particular, in terms
of risk, malicious actors could use approaches based on our framework to generate attack vectors that
compromise production ML systems or potentially bias them toward speciﬁc outcomes.

As a concrete example, one can consider creating transferrable examples in the physical world, such
as the computer vision systems of autonomous cars. While prior works have shown the possibility of
such adversarial examples —i.e., adversarial trafﬁc signs, we note that there is a signiﬁcant gap in
translating synthetic adversarial examples to adversarial examples that reside in the physical world
[45]. Understanding and analyzing the NoBox transferability of adversarial examples to the physical
world—in order to provide public and academic visibility on these risks—is an important direction for
future research. Based on the known risks of designing new kinds of adversarial attacks—discussed
above—we now outline the ways in which our research is informed by the intent to mitigate these
potential societal risks. For instance, our research demonstrates that one can successfully craft
adversarial attacks even in the challenging NoBox setting. It raises many important considerations
when developing robustness approaches. A straightforward extension is to consider our adversarial
example game (AEG) framework as a tool for training robust models. On the theoretical side,
exploring formal veriﬁcation of neural networks against NoBox adversaries is an exciting direction
for continued exploration. As an application, ML practitioners in the industry may choose to employ
new forms of A/B testing with different types of adversarial examples, of which AEG is one method
to robustify and stress test production systems further. Such an application falls in line with other
general approaches to red teaming AI systems [10] and veriﬁability in AI development. In essence,
the goal of such approaches, including adversarial examples for robustness, is to align AI systems’
failure modes to those found in human decision making.

",218,Adversarial Example Games,https://papers.nips.cc/paper/2020/file/65586803f1435736f42a541d3a924595-Paper.pdf,390.0
274,"
As many complex real-world problems can be formulated as cooperative multi-agent games, this work
provides an effective approach to these problems. For example, decentralized agents can be applied to
network routing optimization to speed up transmission, trafﬁc management with autonomous vehicles
to maximize trafﬁc ﬂow, and efﬁcient package delivery with swarms of drones to reduce delivery costs.
However, since our method relies on deep neural networks to implicitly attribute shared outcomes
of the agent group to the individual agents, it faces the “black box problem” where behaviors of the
individual agents may not be rational or interpretable from the human perspective. Furthermore, when
maximizing a shared reward in multi-agent cooperative settings without considering the status of the
individual agents, ethical issues may arise when the optimal joint actions require sacriﬁcing certain
agents. Using the task of trafﬁc management with autonomous vehicles as an example, maximizing
the total trafﬁc volume could lead to indeﬁnite delays for a subset of the vehicles.

","Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, Yuk Ying Chung",Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning,1.0,"{'University of Sydney', 'The University of Sydney'}",,Reinforcement learning and planning,{'Australia'},False,False,"As many complex real-world problems can be formulated as cooperative multi-agent games, this work provides an effective approach to these problems. For example, decentralized agents can be applied to network routing optimization to speed up transmission, traffic management with autonomous vehicles to maximize traffic flow, and efficient package delivery with swarms of drones to reduce delivery costs. However, since our method relies on deep neural networks to implicitly attribute shared outcomes of the agent group to the individual agents, it faces the “black box problem” where behaviors of the individual agents may not be rational or interpretable from the human perspective. Furthermore, when maximizing a shared reward in multi-agent cooperative settings without considering the status of the individual agents, ethical issues may arise when the optimal joint actions require sacrificing certain agents. Using the task of traffic management with autonomous vehicles as an example, maximizing the total traffic volume could lead to indefinite delays for a subset of the vehicles.",Broader Impact,0.0,False,0.0,,"Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, Yuk Ying Chung",8977ecbb8cb82d77fb091c7a7f186163,https://proceedings.neurips.cc/paper/2020/file/8977ecbb8cb82d77fb091c7a7f186163-Paper.pdf,Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning,Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning,Reinforcement Learning and Planning -> Multi-Agent RL,,5.0,"
As many complex real-world problems can be formulated as cooperative multi-agent games, this work
provides an effective approach to these problems. For example, decentralized agents can be applied to
network routing optimization to speed up transmission, trafﬁc management with autonomous vehicles
to maximize trafﬁc ﬂow, and efﬁcient package delivery with swarms of drones to reduce delivery costs.
However, since our method relies on deep neural networks to implicitly attribute shared outcomes
of the agent group to the individual agents, it faces the “black box problem” where behaviors of the
individual agents may not be rational or interpretable from the human perspective. Furthermore, when
maximizing a shared reward in multi-agent cooperative settings without considering the status of the
individual agents, ethical issues may arise when the optimal joint actions require sacriﬁcing certain
agents. Using the task of trafﬁc management with autonomous vehicles as an example, maximizing
the total trafﬁc volume could lead to indeﬁnite delays for a subset of the vehicles.

",274,Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning,https://papers.nips.cc/paper/2020/file/8977ecbb8cb82d77fb091c7a7f186163-Paper.pdf,160.0
27,"This paper proposes a novel method of adapting tools in applied mathematics to enhance the learn-
ability of deep learning models. Even though our methodology is generally applicable to any complex
modern data, it is not tuned to a speciﬁc application that might improperly incur direct societal/ethical
consequences. So the broader impact discussion is not needed for our work.","Kwangho Kim, Jisu Kim, Manzil Zaheer, Joon Kim, Frederic Chazal, Larry Wasserman",PLLay: Efficient Topological Layer based on Persistent Landscapes,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a novel method of adapting tools in applied mathematics to enhance the learn-
ability of deep learning models. Even though our methodology is generally applicable to any complex
modern data, it is not tuned to a speciﬁc application that might improperly incur direct societal/ethical
consequences. So the broader impact discussion is not needed for our work.",27,,https://papers.nips.cc/paper/2020/file/b803a9254688e259cde2ec0361c8abe4-Paper.pdf,
164,"
We believe that this work has the potential to lead to a net-positive change in the reinforcement
learning community and more broadly in society as a whole. Our work enables researchers to
represent the uncertainty in memories due to resource constraints and perform well in the face of
such constraints by prioritizing the knowledge that really matters. While our work is preliminary,
we believe that furthering this line of work may prove to be highly beneﬁcial in reducing the overall
carbon footprint of the artiﬁcial intelligence (AI) industry, which has recently come under scrutiny
for the jarring energy consumption of several common large AI models that produce up to ﬁve times
as much CO2 than an average American car does in its lifetime [51, 52].

In terms of ethical aspects, our method is neutral per se. The advancement of energy-efﬁcient
algorithms may enable autonomous agents to function for long hours in remote areas, the applications
for which could be used for both constructive and destructive things alike, e.g. they may be deployed
for rescue missions [53] or weaponized for military applications [54, 55], but this holds true for any
RL agent.

","Nisheet Patel, Luigi Acerbi, Alexandre Pouget",Dynamic allocation of limited memory resources in reinforcement learning,1.0,"{'University of Geneva', 'University of Helsinki'}",,Neuroscience and cognitive science,"{'Finland', 'Switzerland'}",False,False,"We believe that this work has the potential to lead to a net-positive change in the reinforcement learning community and more broadly in society as a whole. Our work enables researchers to represent the uncertainty in memories due to resource constraints and perform well in the face of such constraints by prioritizing the knowledge that really matters. While our work is preliminary, we believe that furthering this line of work may prove to be highly beneficial in reducing the overall carbon footprint of the artificial intelligence (AI) industry, which has recently come under scrutiny for the jarring energy consumption of several common large AI models that produce up to five times as much CO 2 than an average American car does in its lifetime [51, 52]. In terms of ethical aspects, our method is neutral per se. The advancement of energy-efficient algorithms may enable autonomous agents to function for long hours in remote areas, the applications for which could be used for both constructive and destructive things alike, e.g. they may be deployed for rescue missions [53] or weaponized for military applications [54, 55], but this holds true for any RL agent.",Broader Impact,0.0,False,0.0,,"Nisheet Patel, Luigi Acerbi, Alexandre Pouget",c4fac8fb3c9e17a2f4553a001f631975,https://proceedings.neurips.cc/paper/2020/file/c4fac8fb3c9e17a2f4553a001f631975-Paper.pdf,Dynamic allocation of limited memory resources in reinforcement learning,Dynamic allocation of limited memory resources in reinforcement learning,Neuroscience and Cognitive Science -> Memory,Neuroscience and Cognitive Science -> Cognitive Science; Neuroscience and Cognitive Science -> Neuroscience; Reinforcement Learning and Planning -> Markov Decision Processes; Reinforcement Learning and Planning -> Model-Based RL; Reinforcement Learning and Planning -> Planning; Reinforcement Learning and Planning -> Reinforcement Learning,5.0,"
We believe that this work has the potential to lead to a net-positive change in the reinforcement
learning community and more broadly in society as a whole. Our work enables researchers to
represent the uncertainty in memories due to resource constraints and perform well in the face of
such constraints by prioritizing the knowledge that really matters. While our work is preliminary,
we believe that furthering this line of work may prove to be highly beneﬁcial in reducing the overall
carbon footprint of the artiﬁcial intelligence (AI) industry, which has recently come under scrutiny
for the jarring energy consumption of several common large AI models that produce up to ﬁve times
as much CO2 than an average American car does in its lifetime [51, 52].

In terms of ethical aspects, our method is neutral per se. The advancement of energy-efﬁcient
algorithms may enable autonomous agents to function for long hours in remote areas, the applications
for which could be used for both constructive and destructive things alike, e.g. they may be deployed
for rescue missions [53] or weaponized for military applications [54, 55], but this holds true for any
RL agent.

",164,Dynamic allocation of limited memory resources in reinforcement learning,https://papers.nips.cc/paper/2020/file/c4fac8fb3c9e17a2f4553a001f631975-Paper.pdf,192.0
252,"
Our work discusses methods of improving meta-learning through meta-augmentation at the task level,
and does not target any speciﬁc application of meta-learning. The learning algorithms meta-learning
generates are ones learned from the data in train-time tasks. It is possible these approaches inject bias
into not only how models perform after training, but also inject bias into the training procedure itself.
Biased learners can result in biased model outcomes even when the support set presented at test time
is unbiased, and this may not be straightforward to detect because the behavior of the learner occurs
upstream of actual model predictions. We believe our work is a positive step towards mitigating bias
in meta-learning algorithms, by helping avoid overﬁtting to certain parts of the inputs.

","Janarthanan Rajendran, Alexander Irpan, Eric Jang",Meta-Learning Requires Meta-Augmentation,1.0,"{'Google Brain', 'University of Michigan'}",,"Learning with limited supervision (e.g., unsupervised learning, active learning, few-shot, meta-learning, transfer learning, etc.)",{'USA'},False,False,"Our work discusses methods of improving meta-learning through meta-augmentation at the task level, and does not target any specific application of meta-learning. The learning algorithms meta-learning generates are ones learned from the data in train-time tasks. It is possible these approaches inject bias into not only how models perform after training, but also inject bias into the training procedure itself . Biased learners can result in biased model outcomes even when the support set presented at test time is unbiased, and this may not be straightforward to detect because the behavior of the learner occurs upstream of actual model predictions. We believe our work is a positive step towards mitigating bias in meta-learning algorithms, by helping avoid overfitting to certain parts of the inputs.",Broader Impact,1.0,False,1.0,,"Janarthanan Rajendran, Alexander Irpan, Eric Jang",3e5190eeb51ebe6c5bbc54ee8950c548,https://proceedings.neurips.cc/paper/2020/file/3e5190eeb51ebe6c5bbc54ee8950c548-Paper.pdf,Meta-Learning Requires Meta-Augmentation,Meta-Learning Requires Meta-Augmentation,Algorithms -> Meta-Learning,Algorithms -> Few-Shot Learning,5.0,"
Our work discusses methods of improving meta-learning through meta-augmentation at the task level,
and does not target any speciﬁc application of meta-learning. The learning algorithms meta-learning
generates are ones learned from the data in train-time tasks. It is possible these approaches inject bias
into not only how models perform after training, but also inject bias into the training procedure itself.
Biased learners can result in biased model outcomes even when the support set presented at test time
is unbiased, and this may not be straightforward to detect because the behavior of the learner occurs
upstream of actual model predictions. We believe our work is a positive step towards mitigating bias
in meta-learning algorithms, by helping avoid overﬁtting to certain parts of the inputs.

",252,Meta-Learning Requires Meta-Augmentation,https://papers.nips.cc/paper/2020/file/3e5190eeb51ebe6c5bbc54ee8950c548-Paper.pdf,124.0
194,"
We propose a fundamentally novel method to implicitly learn the geometric information of a manifold
by explicitly learning its associated heat kernel, which is the solution of heat equation with initial con-
ditions given. Our proposed method is general and can be applied in many down-stream applications.
Speciﬁcally, it could be used to improve many kernel-related algorithms and applications. It may also
inspire researchers in deep learning to borrow ideas from other ﬁelds (mathematics, physics, etc.)
and apply them to their own research. This can beneﬁt both ﬁelds and thus promote interdisciplinary
research.
","Yufan Zhou, Changyou Chen, Jinhui Xu",Learning Manifold Implicitly via Explicit Heat-Kernel Learning,1.0,"{'University at Buffalo', 'SUNY at Buffalo'}",,Deep learning,{'USA'},False,False,"We propose a fundamentally novel method to implicitly learn the geometric information of a manifold by explicitly learning its associated heat kernel, which is the solution of heat equation with initial conditions given. Our proposed method is general and can be applied in many down-stream applications. Specifically, it could be used to improve many kernel-related algorithms and applications. It may also inspire researchers in deep learning to borrow ideas from other fields (mathematics, physics, etc.) and apply them to their own research. This can benefit both fields and thus promote interdisciplinary research.",Broader Impact,0.0,False,0.0,,"Yufan Zhou, Changyou Chen, Jinhui Xu",05e2a0647e260c355dd2b2175edb45b8,https://proceedings.neurips.cc/paper/2020/file/05e2a0647e260c355dd2b2175edb45b8-Paper.pdf,Learning Manifold Implicitly via Explicit Heat-Kernel Learning,Learning Manifold Implicitly via Explicit Heat-Kernel Learning,Algorithms -> Nonlinear Dimensionality Reduction and Manifold Learning,Algorithms -> Unsupervised Learning; Deep Learning -> Adversarial Networks; Deep Learning -> Generative Models,6.0,"
We propose a fundamentally novel method to implicitly learn the geometric information of a manifold
by explicitly learning its associated heat kernel, which is the solution of heat equation with initial con-
ditions given. Our proposed method is general and can be applied in many down-stream applications.
Speciﬁcally, it could be used to improve many kernel-related algorithms and applications. It may also
inspire researchers in deep learning to borrow ideas from other ﬁelds (mathematics, physics, etc.)
and apply them to their own research. This can beneﬁt both ﬁelds and thus promote interdisciplinary
research.
",194,Learning Manifold Implicitly via Explicit Heat-Kernel Learning,https://papers.nips.cc/paper/2020/file/05e2a0647e260c355dd2b2175edb45b8-Paper.pdf,92.0
15,"
Automatic Machine Learning (AutoML) aims to build a better machine learning model in a data-
driven and automated manner, compensating for the lack of machine learning experts and lowering the
threshold of various areas of machine learning to help all the amateurs to use machine learning without
any hassle. These days, many companies, like Google and Facebook, are using AutoML to build
machine learning models for handling different businesses automatically. They especially leverage the
AutoML to automatically build Deep Neural Networks for solving various tasks, including computer
vision, natural language processing, autonomous driving, and so on. AutoML is an up-and-coming
tool to take advantage of the extracted data to ﬁnd the solutions automatically.

This paper focuses on the Neural Architecture Search (NAS) of AutoML, and it is the ﬁrst attempt
to enhance the intelligent exploration of differentiable One-Shot NAS in the latent space. The
experimental results demonstrate the importance of introducing uncertainty into neural architecture
search, and point out a promising research direction in the NAS community.

It is worth notice that NAS is in its infancy, and it is still very challenging to use it to complete
automation of a speciﬁc business function like marketing analytics, customer behavior, or other
customer analytics.

","Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan Ge, Steven Su",Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement,1.0,"{'University of Technology Sydney', 'Monash University', 'Beijing Institute of Technology'}",,AutoML,"{'Australia', 'China'}",False,False,"Automatic Machine Learning (AutoML) aims to build a better machine learning model in a data- driven and automated manner, compensating for the lack of machine learning experts and lowering the threshold of various areas of machine learning to help all the amateurs to use machine learning without any hassle. These days, many companies, like Google and Facebook, are using AutoML to build machine learning models for handling different businesses automatically. They especially leverage the AutoML to automatically build Deep Neural Networks for solving various tasks, including computer vision, natural language processing, autonomous driving, and so on. AutoML is an up-and-coming tool to take advantage of the extracted data to find the solutions automatically. This paper focuses on the Neural Architecture Search (NAS) of AutoML, and it is the first attempt to enhance the intelligent exploration of differentiable One-Shot NAS in the latent space. The experimental results demonstrate the importance of introducing uncertainty into neural architecture search, and point out a promising research direction in the NAS community. It is worth notice that NAS is in its infancy, and it is still very challenging to use it to complete automation of a specific business function like marketing analytics, customer behavior, or other customer analytics.",Broader Impact,0.0,False,0.0,,"Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan Ge, Steven Su",9a96a2c73c0d477ff2a6da3bf538f4f4,https://proceedings.neurips.cc/paper/2020/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf,Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement,Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement,Algorithms -> AutoML,Algorithms -> Online Learning; Algorithms -> Uncertainty Estimation; Deep Learning -> CNN Architectures; Deep Learning -> Deep Autoencoders,7.0,"
Automatic Machine Learning (AutoML) aims to build a better machine learning model in a data-
driven and automated manner, compensating for the lack of machine learning experts and lowering the
threshold of various areas of machine learning to help all the amateurs to use machine learning without
any hassle. These days, many companies, like Google and Facebook, are using AutoML to build
machine learning models for handling different businesses automatically. They especially leverage the
AutoML to automatically build Deep Neural Networks for solving various tasks, including computer
vision, natural language processing, autonomous driving, and so on. AutoML is an up-and-coming
tool to take advantage of the extracted data to ﬁnd the solutions automatically.

This paper focuses on the Neural Architecture Search (NAS) of AutoML, and it is the ﬁrst attempt
to enhance the intelligent exploration of differentiable One-Shot NAS in the latent space. The
experimental results demonstrate the importance of introducing uncertainty into neural architecture
search, and point out a promising research direction in the NAS community.

It is worth notice that NAS is in its infancy, and it is still very challenging to use it to complete
automation of a speciﬁc business function like marketing analytics, customer behavior, or other
customer analytics.

",15,Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement,https://papers.nips.cc/paper/2020/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf,203.0
50,"In the era of big data, business providers, data scientists, and governments try to explore opportu-
nities in the large scale and high-dimensional datasets. Nevertheless, several major computational
challenges arise and prevent practitioners from constructing effective algorithms or tools to analyze
their datasets. Dimensionality reduction (DR) plays an essential role in supervised and unsupervised
learning tasks when the datasets are high dimensional. One beneﬁt of reducing the data dimension
before classiﬁcation or clustering is to save storage and reduce computational cost for the later steps,
however, the DR technique itself can be costly. We study a recently proposed and promising DR
technique, the Wasserstein discriminant analysis, and propose a different formulation that could
achieve comparable or better results with less computational cost. We also analyze the problem
from a different perspective that was originated from electronic structure calculations, which could
be of interest to a broader audience in the machine learning community.","Hexuan Liu, Yunfeng Cai, You-Lin Chen, Ping Li",Ratio Trace Formulation of Wasserstein Discriminant Analysis,1.0,"{'Department of Statistics, University of Chicago', 'University of Washington', 'Baidu Research', 'Baidu Research USA'}",,"Core machine learning methods (e.g., supervised learning, ranking, clustering, metric learning, etc.)","{'USA', 'China'}",False,False,"In the era of big data, business providers, data scientists, and governments try to explore opportu- nities in the large scale and high-dimensional datasets. Nevertheless, several major computational challenges arise and prevent practitioners from constructing effective algorithms or tools to analyze their datasets. Dimensionality reduction (DR) plays an essential role in supervised and unsupervised learning tasks when the datasets are high dimensional. One benefit of reducing the data dimension before classification or clustering is to save storage and reduce computational cost for the later steps, however, the DR technique itself can be costly. We study a recently proposed and promising DR technique, the Wasserstein discriminant analysis, and propose a different formulation that could achieve comparable or better results with less computational cost. We also analyze the problem from a different perspective that was originated from electronic structure calculations, which could be of interest to a broader audience in the machine learning community.",Broader Impact,1.0,False,1.0,,"Hexuan Liu, Yunfeng Cai, You-Lin Chen, Ping Li",c37f9e1283cbd4a6edfd778fc8b1c652,https://proceedings.neurips.cc/paper/2020/file/c37f9e1283cbd4a6edfd778fc8b1c652-Paper.pdf,Ratio Trace Formulation of Wasserstein Discriminant Analysis,Ratio Trace Formulation of Wasserstein Discriminant Analysis,"Algorithms -> Components Analysis (e.g., CCA, ICA, LDA, PCA)",Algorithms -> Classification; Algorithms -> Clustering,6.0,"In the era of big data, business providers, data scientists, and governments try to explore opportu-
nities in the large scale and high-dimensional datasets. Nevertheless, several major computational
challenges arise and prevent practitioners from constructing effective algorithms or tools to analyze
their datasets. Dimensionality reduction (DR) plays an essential role in supervised and unsupervised
learning tasks when the datasets are high dimensional. One beneﬁt of reducing the data dimension
before classiﬁcation or clustering is to save storage and reduce computational cost for the later steps,
however, the DR technique itself can be costly. We study a recently proposed and promising DR
technique, the Wasserstein discriminant analysis, and propose a different formulation that could
achieve comparable or better results with less computational cost. We also analyze the problem
from a different perspective that was originated from electronic structure calculations, which could
be of interest to a broader audience in the machine learning community.",50,Ratio Trace Formulation of Wasserstein Discriminant Analysis,https://papers.nips.cc/paper/2020/file/c37f9e1283cbd4a6edfd778fc8b1c652-Paper.pdf,152.0
187,"
As this is a theoretical contribution, we do not envision that our direct results will have a tangible
societal impact. Our broader line of inquiry could impact a line of thinking about how to design more
sample-efficient algorithms for multi-agent reinforcement learning, which could be useful towards
making artificial intelligence more resource and energy efficient.","Yu Bai, Chi Jin, Tiancheng Yu",Near-Optimal Reinforcement Learning with Self-Play,1.0,"{'Salesforce Research', 'Princeton University', 'MIT '}",True,Reinforcement learning and planning,{'USA'},False,False,"As this is a theoretical contribution, we do not envision that our direct results will have a tangible societal impact. Our broader line of inquiry could impact a line of thinking about how to design more sample-efficient algorithms for multi-agent reinforcement learning, which could be useful towards making artificial intelligence more resource and energy efficient.",Broader Impact,1.0,False,1.0,True,"Yu Bai, Chi Jin, Tiancheng Yu",172ef5a94b4dd0aa120c6878fc29f70c,https://proceedings.neurips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf,Near-Optimal Reinforcement Learning with Self-Play,Near-Optimal Reinforcement Learning with Self-Play,Reinforcement Learning and Planning -> Reinforcement Learning,Reinforcement Learning and Planning -> Exploration; Theory -> Statistical Learning Theory,2.0,"
As this is a theoretical contribution, we do not envision that our direct results will have a tangible
societal impact. Our broader line of inquiry could impact a line of thinking about how to design more
sample-efficient algorithms for multi-agent reinforcement learning, which could be useful towards
making artificial intelligence more resource and energy efficient.",187,Near-Optimal Reinforcement Learning with Self-Play,https://papers.nips.cc/paper/2020/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf,55.0
32,"Data centers draw increasing amounts of the total energy we consume, and increasing applications
of machine learning mean that model-ﬁtting and parameter exploration require a larger and larger
proportion of their energy expenditures [1, 14, 30]. Indeed, as Asi and Duchi [1] note, the energy to
train and tune some models is roughly on the scale of driving thousands of cars from San Francisco
to Los Angeles, while training a modern transformer network (with architecture search) generates
roughly six times the total CO2 of an average car’s lifetime [30]. It is thus centrally important to
build more efﬁcient and robust methods, which allow us to avoid wasteful hyperparameter search but
simply work.

A major challenge in building better algorithms is that fundamental physical limits have forced CPU
speed and energy to essentially plateau; only by parallelization can we harness both increasing speed
and reduce the energy to ﬁt models [14]. In this context, our methods take a step toward reducing the
energy and overhead to perform machine learning.

Taking a step farther back, we believe optimization and model-ﬁtting research in machine learning
should refocus its attention: rather than developing algorithms that, with appropriate hyperparameter
tuning, achieve state-of-the-art accuracy for a given dataset, we should evaluate algorithms by
whether they robustly work. This would allow a more careful consideration of an algorithms’ costs
and beneﬁts: is it worth 2× faster training, for appropriate hyperparameters, if one has to spend
25× as much time to ﬁnd the appropriate algorithmic hyperparameters? Even more, as Strubell et al.
[30] point out, the extraordinary costs of hyperparameter tuning for ﬁtting large-scale models price
many researchers out of making progress on certain frontiers; to the extent that we can mitigate these
challenges, we will allow more equity in who can help machine learning progress.
","Hilal Asi, Karan Chadha, Gary Cheng, John C. Duchi",Minibatch Stochastic Approximate Proximal Point Methods,1.0,"{'Stanford', 'Stanford University'}",,Optimization Methods (continuous or discrete),{'USA'},False,False,"Data centers draw increasing amounts of the total energy we consume, and increasing applications of machine learning mean that model-fitting and parameter exploration require a larger and larger proportion of their energy expenditures [1, 14, 30]. Indeed, as Asi and Duchi [1] note, the energy to train and tune some models is roughly on the scale of driving thousands of cars from San Francisco to Los Angeles, while training a modern transformer network (with architecture search) generates roughly six times the total CO 2 of an average car’s lifetime [30]. It is thus centrally important to build more efficient and robust methods, which allow us to avoid wasteful hyperparameter search but simply work. A major challenge in building better algorithms is that fundamental physical limits have forced CPU speed and energy to essentially plateau; only by parallelization can we harness both increasing speed and reduce the energy to fit models [14]. In this context, our methods take a step toward reducing the energy and overhead to perform machine learning. Taking a step farther back, we believe optimization and model-fitting research in machine learning should refocus its attention: rather than developing algorithms that, with appropriate hyperparameter tuning, achieve state-of-the-art accuracy for a given dataset, we should evaluate algorithms by whether they robustly work. This would allow a more careful consideration of an algorithms’ costs and benefits: is it worth 2 × faster training, for appropriate hyperparameters, if one has to spend 25 × as much time to find the appropriate algorithmic hyperparameters? Even more, as Strubell et al. [30] point out, the extraordinary costs of hyperparameter tuning for fitting large-scale models price many researchers out of making progress on certain frontiers; to the extent that we can mitigate these challenges, we will allow more equity in who can help machine learning progress.",Broader Impact,0.0,False,0.0,,"Hilal Asi, Karan Chadha, Gary Cheng, John C. Duchi",fa2246fa0fdf0d3e270c86767b77ba1b,https://proceedings.neurips.cc/paper/2020/file/fa2246fa0fdf0d3e270c86767b77ba1b-Paper.pdf,Minibatch Stochastic Approximate Proximal Point Methods,Minibatch Stochastic Approximate Proximal Point Methods,Optimization -> Convex Optimization,Optimization -> Stochastic Optimization,9.0,"Data centers draw increasing amounts of the total energy we consume, and increasing applications
of machine learning mean that model-ﬁtting and parameter exploration require a larger and larger
proportion of their energy expenditures [1, 14, 30]. Indeed, as Asi and Duchi [1] note, the energy to
train and tune some models is roughly on the scale of driving thousands of cars from San Francisco
to Los Angeles, while training a modern transformer network (with architecture search) generates
roughly six times the total CO2 of an average car’s lifetime [30]. It is thus centrally important to
build more efﬁcient and robust methods, which allow us to avoid wasteful hyperparameter search but
simply work.

A major challenge in building better algorithms is that fundamental physical limits have forced CPU
speed and energy to essentially plateau; only by parallelization can we harness both increasing speed
and reduce the energy to ﬁt models [14]. In this context, our methods take a step toward reducing the
energy and overhead to perform machine learning.

Taking a step farther back, we believe optimization and model-ﬁtting research in machine learning
should refocus its attention: rather than developing algorithms that, with appropriate hyperparameter
tuning, achieve state-of-the-art accuracy for a given dataset, we should evaluate algorithms by
whether they robustly work. This would allow a more careful consideration of an algorithms’ costs
and beneﬁts: is it worth 2× faster training, for appropriate hyperparameters, if one has to spend
25× as much time to ﬁnd the appropriate algorithmic hyperparameters? Even more, as Strubell et al.
[30] point out, the extraordinary costs of hyperparameter tuning for ﬁtting large-scale models price
many researchers out of making progress on certain frontiers; to the extent that we can mitigate these
challenges, we will allow more equity in who can help machine learning progress.
",32,Minibatch Stochastic Approximate Proximal Point Methods,https://papers.nips.cc/paper/2020/file/fa2246fa0fdf0d3e270c86767b77ba1b-Paper.pdf,301.0
18,"
Although several studies have investigated communication between populations of neurons, task-
related communication has been ignored. This is of fundamental importance in neuroscience, and we
show that it can be achieved simply by extending the previous method. We believe our methods will
be beneﬁcial to the neuroscientists who will investigate interaction among multiple brain areas in
terms of speciﬁc task parameter of interest.

","Yu Takagi, Steven Kennerley, Jun-ichiro Hirayama, Laurence Hunt",Demixed shared component analysis of neural population data from multiple brain areas,1.0,"{'AIST', 'University of Oxford'}",,,{'UK'},False,False,"Although several studies have investigated communication between populations of neurons, task- related communication has been ignored. This is of fundamental importance in neuroscience, and we show that it can be achieved simply by extending the previous method. We believe our methods will be beneficial to the neuroscientists who will investigate interaction among multiple brain areas in terms of specific task parameter of interest.",Broader Impact,0.0,False,0.0,,"Yu Takagi, Steven Kennerley, Jun-ichiro Hirayama, Laurence Hunt",44ece762ae7e41e3a0b1301488907eaa,https://proceedings.neurips.cc/paper/2020/file/44ece762ae7e41e3a0b1301488907eaa-Paper.pdf,Demixed shared component analysis of neural population data from multiple brain areas,Demixed shared component analysis of neural population data from multiple brain areas,Neuroscience and Cognitive Science,Neuroscience and Cognitive Science -> Cognitive Science; Neuroscience and Cognitive Science -> Neuroscience; Neuroscience and Cognitive Science -> Perception,3.0,"
Although several studies have investigated communication between populations of neurons, task-
related communication has been ignored. This is of fundamental importance in neuroscience, and we
show that it can be achieved simply by extending the previous method. We believe our methods will
be beneﬁcial to the neuroscientists who will investigate interaction among multiple brain areas in
terms of speciﬁc task parameter of interest.

",18,Demixed shared component analysis of neural population data from multiple brain areas,https://papers.nips.cc/paper/2020/file/44ece762ae7e41e3a0b1301488907eaa-Paper.pdf,63.0
75,"Our work ﬁts within the broad direction of research concerning safety issues in AI/ML at large. With
the recent radical advances in machine learning, ML-assisted decision making is fast becoming an
intrinsic part of the design of systems and services that billions of people around the world use every
day. And not surprisingly, investigating the vulnerability of existing learning models and robustness
against manipulation attacks are becoming critically important in the light of trustworthy learning
paradigm. Hence, there has been a surge of interest in making learning models that are robust against
adversarial attacks for both applied ML such as supervised learning and deep learning, and theoretical
ML such as reinforcement learning and multi-armed bandits. This is critically important for society,
since the ML algorithms are being adopted more and more in safety-critical domains across sciences,
businesses, and governments that impact people’s daily lives. Last, we see no ethical concerns related
to this paper.","lin yang, Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John C. S. Lui, Wing Shing Wong",Adversarial Bandits with Corruptions: Regret Lower Bound and No-regret Algorithm,1.0,"{'The Chinese University of Hong Kong', 'University of Copenhagen', 'UMass', 'UMass Amherst'}",,Reinforcement learning and planning,"{'Denmark', 'USA', 'China'}",False,False,"Our work fits within the broad direction of research concerning safety issues in AI/ML at large. With the recent radical advances in machine learning, ML-assisted decision making is fast becoming an intrinsic part of the design of systems and services that billions of people around the world use every day. And not surprisingly, investigating the vulnerability of existing learning models and robustness against manipulation attacks are becoming critically important in the light of trustworthy learning paradigm . Hence, there has been a surge of interest in making learning models that are robust against adversarial attacks for both applied ML such as supervised learning and deep learning, and theoretical ML such as reinforcement learning and multi-armed bandits. This is critically important for society, since the ML algorithms are being adopted more and more in safety-critical domains across sciences, businesses, and governments that impact people’s daily lives. Last, we see no ethical concerns related to this paper.",7 Broader Impacts,0.0,False,0.0,,"lin yang, Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John C. S. Lui, Wing Shing Wong",e655c7716a4b3ea67f48c6322fc42ed6,https://proceedings.neurips.cc/paper/2020/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf,Adversarial Bandits with Corruptions: Regret Lower Bound and No-regret Algorithm,Adversarial Bandits with Corruptions,Algorithms -> Bandit Algorithms,Algorithms -> Online Learning,6.0,"Our work ﬁts within the broad direction of research concerning safety issues in AI/ML at large. With
the recent radical advances in machine learning, ML-assisted decision making is fast becoming an
intrinsic part of the design of systems and services that billions of people around the world use every
day. And not surprisingly, investigating the vulnerability of existing learning models and robustness
against manipulation attacks are becoming critically important in the light of trustworthy learning
paradigm. Hence, there has been a surge of interest in making learning models that are robust against
adversarial attacks for both applied ML such as supervised learning and deep learning, and theoretical
ML such as reinforcement learning and multi-armed bandits. This is critically important for society,
since the ML algorithms are being adopted more and more in safety-critical domains across sciences,
businesses, and governments that impact people’s daily lives. Last, we see no ethical concerns related
to this paper.",75,Adversarial Bandits with Corruptions: Regret Lower Bound and No-regret Algorithm,https://papers.nips.cc/paper/2020/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf,155.0
80,"This research does not involve any issues directly regarding ethical aspects and future societal
consequences. In the future, our approach presented in this paper might be applied in different
domains, e.g., medicine and life science, where ethical aspects and societal consequences might have
to be considered.","Maksymilian Wojtas, Ke Chen",Feature Importance Ranking for Deep Learning,1.0,"{'University of Manchester', 'The University of Manchester'}",True,Deep learning,{'UK'},False,False,"This research does not involve any issues directly regarding ethical aspects and future societal consequences. In the future, our approach presented in this paper might be applied in different domains, e.g., medicine and life science, where ethical aspects and societal consequences might have to be considered.",Broader Impact,0.0,False,0.0,True,"Maksymilian Wojtas, Ke Chen",36ac8e558ac7690b6f44e2cb5ef93322,https://proceedings.neurips.cc/paper/2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,Feature Importance Ranking for Deep Learning,Feature Importance Ranking for Deep Learning,Deep Learning,"Deep Learning -> Supervised Deep Networks; Deep Learning -> Visualization, Interpretability, and Explainability",2.0,"This research does not involve any issues directly regarding ethical aspects and future societal
consequences. In the future, our approach presented in this paper might be applied in different
domains, e.g., medicine and life science, where ethical aspects and societal consequences might have
to be considered.",80,Feature Importance Ranking for Deep Learning,https://papers.nips.cc/paper/2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,46.0
85,"In general this work does not present any foreseeable speciﬁc societal consequence in the authors’
joint opinion.

This is foundational research in regret-bounded online learning. As such it is not targeted towards
any particular application area. Although this research may have societal impact for good or for ill in
the future, we cannot foresee the shape and the extent.","Mark Herbster, Stephen Pasteris, Lisa Tse",Online Matrix Completion with Side Information,1.0,{'University College London'},False,Theory (including computational and statistical analyses),{'UK'},True,False,"In general this work does not present any foreseeable specific societal consequence in the authors’ joint opinion. This is foundational research in regret-bounded online learning. As such it is not targeted towards any particular application area. Although this research may have societal impact for good or for ill in the future, we cannot foresee the shape and the extent.",Broader Impact,0.0,True,0.0,True,"Mark Herbster, Stephen Pasteris, Lisa Tse",eb06b9db06012a7a4179b8f3cb5384d3,https://proceedings.neurips.cc/paper/2020/file/eb06b9db06012a7a4179b8f3cb5384d3-Paper.pdf,Online Matrix Completion with Side Information,Online Matrix Completion with Side Information,Algorithms -> Online Learning,,4.0,"In general this work does not present any foreseeable speciﬁc societal consequence in the authors’
joint opinion.

This is foundational research in regret-bounded online learning. As such it is not targeted towards
any particular application area. Although this research may have societal impact for good or for ill in
the future, we cannot foresee the shape and the extent.",85,Online Matrix Completion with Side Information,https://papers.nips.cc/paper/2020/file/eb06b9db06012a7a4179b8f3cb5384d3-Paper.pdf,59.0
110,"This work does not present any foreseeable societal consequence. A broader
impact discussion is not applicable.","Jiajin Li, Caihua Chen, Anthony Man-Cho So",Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine,1.0,"{'The Chinese University of Hong Kong', 'Nanjing University', 'CUHK'}",False,Optimization Methods (continuous or discrete),{'China'},True,False,This work does not present any foreseeable societal consequence. A broader impact discussion is not applicable.,Broader Impact,0.0,True,0.0,True,"Jiajin Li, Caihua Chen, Anthony Man-Cho So",2974788b53f73e7950e8aa49f3a306db,https://proceedings.neurips.cc/paper/2020/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine,Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine,Optimization,Algorithms -> Classification; Optimization -> Convex Optimization,2.0,"This work does not present any foreseeable societal consequence. A broader
impact discussion is not applicable.",110,Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine,https://papers.nips.cc/paper/2020/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,16.0
112,"
This work touches upon a very old problem dating back to 1933 and the work of [39]. Therefore, we
don’t anticipate any new societal impacts or ethical aspects, that are not well understood by now.

",Vrettos Moulos,Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian Rewards,1.0,{'UC Berkeley'},True,Reinforcement learning and planning,{'USA'},False,False,"This work touches upon a very old problem dating back to 1933 and the work of [39]. Therefore, we don’t anticipate any new societal impacts or ethical aspects, that are not well understood by now.",Statement of Broader Impact,0.0,False,0.0,True,Vrettos Moulos,597c7b407a02cc0a92167e7a371eca25,https://proceedings.neurips.cc/paper/2020/file/597c7b407a02cc0a92167e7a371eca25-Paper.pdf,Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian Rewards,Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian Rewards,Algorithms -> Bandit Algorithms,Theory -> Large Deviations and Asymptotic Analysis,2.0,"
This work touches upon a very old problem dating back to 1933 and the work of [39]. Therefore, we
don’t anticipate any new societal impacts or ethical aspects, that are not well understood by now.

",112,Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian Rewards,https://papers.nips.cc/paper/2020/file/597c7b407a02cc0a92167e7a371eca25-Paper.pdf,35.0
138,"
This work develops a new framework that can grow neural networks simply and efﬁciently, which
can be generally used in various applications that using neural networks and positively enhance their
capacity and performance. In particular, we anticipate it can be applied on devices which have hard
memory/computation constraint, i.e. mobile devices or robots. Our work does not have any negative
societal impacts.","Lemeng Wu, Bo Liu, Peter Stone, Qiang Liu",Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks,1.0,"{'University of Texas at Austin', 'The University of Texas at Austin', 'UT Austin'}",,AutoML,{'USA'},False,False,"This work develops a new framework that can grow neural networks simply and efficiently, which can be generally used in various applications that using neural networks and positively enhance their capacity and performance. In particular, we anticipate it can be applied on devices which have hard memory/computation constraint, i.e. mobile devices or robots. Our work does not have any negative societal impacts.",Broader Impact,0.0,False,0.0,,"Lemeng Wu, Bo Liu, Peter Stone, Qiang Liu",fdbe012e2e11314b96402b32c0df26b7,https://proceedings.neurips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf,Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks,Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks,Deep Learning -> Optimization for Deep Networks,Algorithms -> Continual Learning,3.0,"
This work develops a new framework that can grow neural networks simply and efﬁciently, which
can be generally used in various applications that using neural networks and positively enhance their
capacity and performance. In particular, we anticipate it can be applied on devices which have hard
memory/computation constraint, i.e. mobile devices or robots. Our work does not have any negative
societal impacts.",138,Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks,https://papers.nips.cc/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf,62.0
163,"
This work does not present any foreseeable societal consequence.

","Neng Wan, Dapeng Li, NAIRA HOVAKIMYAN",f-Divergence Variational Inference,,,,,,,,,,,,,,,,,,,,,,"
This work does not present any foreseeable societal consequence.

",163,,https://papers.nips.cc/paper/2020/file/c928d86ff00aeb89a39bd4a80e652a38-Paper.pdf,
166,"
Any protocol that quantifies perception could potentially be turned into a diagnostic tool. Apart from
that, our work does not present any foreseeable societal consequence.","Jonathan Vacher, Aida Davila, Adam Kohn, Ruben  Coen-Cagli",Texture Interpolation for Probing Visual Perception,1.0,{'Albert Einstein College of Medicine'},True,Vision,{'USA'},False,False,"Any protocol that quantifies perception could potentially be turned into a diagnostic tool. Apart from that, our work does not present any foreseeable societal consequence.",Broader Impact,0.0,False,0.0,True,"Jonathan Vacher, Aida Davila, Adam Kohn, Ruben  Coen-Cagli",fba9d88164f3e2d9109ee770223212a0,https://proceedings.neurips.cc/paper/2020/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf,Texture Interpolation for Probing Visual Perception,Texture Interpolation for Probing Visual Perception,Neuroscience and Cognitive Science -> Visual Perception,Applications -> Computer Vision; Neuroscience and Cognitive Science -> Cognitive Science,2.0,"
Any protocol that quantifies perception could potentially be turned into a diagnostic tool. Apart from
that, our work does not present any foreseeable societal consequence.",166,Texture Interpolation for Probing Visual Perception,https://papers.nips.cc/paper/2020/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf,25.0
170,"
We propose a general framework for training agents to tackle a class of Multilevel Budgeted Combi-
natorial problems. Such models are widely used in Economics and Operations Research. In this study,
we particularly focused on the Multilevel Critical Node problem (MCN). Regarding the usefulness of
such problem for practical scenarios, the MCN could ﬁt on several applications, e.g.
to limit the fake news spread in social networks or in cyber security for the protection of a botnet against malware
injections [1]. Thus, this could represent a step towards the design of more robust networks, but could
also be used to identify their critical weaknesses for malicious agents. We do not anticipate that our
work will advantage or disadvantage any particular group.

","Adel Nabli, Margarida Carvalho",Curriculum learning for multilevel budgeted combinatorial problems,1.0,{'Université de Montréal'},,Optimization Methods (continuous or discrete),{'Canada'},False,False,"We propose a general framework for training agents to tackle a class of Multilevel Budgeted Combinatorial problems. Such models are widely used in Economics and Operations Research. In this study, we particularly focused on the Multilevel Critical Node problem (MCN). Regarding the usefulness of such problem for practical scenarios, the MCN could fit on several applications, e.g. to limit the fake news spread in social networks or in cyber security for the protection of a botnet against malware injections [1]. Thus, this could represent a step towards the design of more robust networks, but could also be used to identify their critical weaknesses for malicious agents. We do not anticipate that our work will advantage or disadvantage any particular group.",Broader Impact,0.0,False,0.0,,"Adel Nabli, Margarida Carvalho",4eb7d41ae6005f60fe401e56277ebd4e,https://proceedings.neurips.cc/paper/2020/file/4eb7d41ae6005f60fe401e56277ebd4e-Paper.pdf,Curriculum learning for multilevel budgeted combinatorial problems,Curriculum learning for multilevel budgeted combinatorial problems,Optimization -> Discrete Optimization,Reinforcement Learning and Planning -> Multi-Agent RL; Reinforcement Learning and Planning -> Reinforcement Learning; Theory -> Game Theory and Computational Economics,6.0,"
We propose a general framework for training agents to tackle a class of Multilevel Budgeted Combi-
natorial problems. Such models are widely used in Economics and Operations Research. In this study,
we particularly focused on the Multilevel Critical Node problem (MCN). Regarding the usefulness of
such problem for practical scenarios, the MCN could ﬁt on several applications, e.g.
to limit the fake news spread in social networks or in cyber security for the protection of a botnet against malware
injections [1]. Thus, this could represent a step towards the design of more robust networks, but could
also be used to identify their critical weaknesses for malicious agents. We do not anticipate that our
work will advantage or disadvantage any particular group.

",170,Curriculum learning for multilevel budgeted combinatorial problems,https://papers.nips.cc/paper/2020/file/4eb7d41ae6005f60fe401e56277ebd4e-Paper.pdf,120.0
174,"
The results of this paper improves the performance of stochastic gradient descent with momentum as
well as its multistage version. Our study will also benefit the machine learning community. We do
not believe that the results in this work will cause any ethical issue, or put anyone at a disadvantage
in our society.","Yanli Liu, Yuan Gao, Wotao Yin",An Improved Analysis of Stochastic Gradient Descent with Momentum,1.0,"{'UCLA', 'Columbia University', 'Alibaba US, DAMO Academy'}",True,Optimization Methods (continuous or discrete),"{'USA', 'China'}",False,False,"The results of this paper improves the performance of stochastic gradient descent with momentum as well as its multistage version. Our study will also benefit the machine learning community. We do not believe that the results in this work will cause any ethical issue, or put anyone at a disadvantage in our society.",Broader Impact,1.0,False,1.0,False,"Yanli Liu, Yuan Gao, Wotao Yin",d3f5d4de09ea19461dab00590df91e4f,https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf,An Improved Analysis of Stochastic Gradient Descent with Momentum,An Improved Analysis of Stochastic Gradient Descent with Momentum,Optimization -> Stochastic Optimization,Optimization -> Convex Optimization; Optimization -> Non-Convex Optimization,3.0,"
The results of this paper improves the performance of stochastic gradient descent with momentum as
well as its multistage version. Our study will also benefit the machine learning community. We do
not believe that the results in this work will cause any ethical issue, or put anyone at a disadvantage
in our society.",174,An Improved Analysis of Stochastic Gradient Descent with Momentum,https://papers.nips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf,53.0
224,"
Our research has the nature of basic science — we are working on foundational improvements to
reinforcement learning algorithms. We are not targeting any specific applications, and it is hard to
foresee any societal consequences beyond those brought about by advancing the state of our knowledge
of machine learning.","Roshan Shariff, Csaba Szepesvari",Efficient Planning in Large MDPs with Weak Linear Function Approximation,1.0,"{'University of Alberta', 'DeepMind / University of Alberta'}",False,Reinforcement learning and planning,"{'Canada', 'UK'}",False,False,"Our research has the nature of basic science — we are working on foundational improvements to reinforcement learning algorithms. We are not targeting any specific applications, and it is hard to foresee any societal consequences beyond those brought about by advancing the state of our knowledge of machine learning.",Broader Impact,1.0,False,1.0,True,"Roshan Shariff, Csaba Szepesvari",de07edeeba9f475c9395959494cd8f64,https://proceedings.neurips.cc/paper/2020/file/de07edeeba9f475c9395959494cd8f64-Paper.pdf,Efficient Planning in Large MDPs with Weak Linear Function Approximation,Efficient Planning in Large MDPs with Weak Linear Function Approximation,Reinforcement Learning and Planning,Reinforcement Learning and Planning -> Decision and Control; Reinforcement Learning and Planning -> Markov Decision Processes; Reinforcement Learning and Planning -> Planning; Reinforcement Learning and Planning -> Reinforcement Learning; Theory,2.0,"
Our research has the nature of basic science — we are working on foundational improvements to
reinforcement learning algorithms. We are not targeting any specific applications, and it is hard to
foresee any societal consequences beyond those brought about by advancing the state of our knowledge
of machine learning.",224,Efficient Planning in Large MDPs with Weak Linear Function Approximation,https://papers.nips.cc/paper/2020/file/de07edeeba9f475c9395959494cd8f64-Paper.pdf,49.0
261,"
As we can foresee, with the development of 5G communication, video-based media industry will
develop fast in the future. The advancement of accurate and fast semi-supervised segmentation will
be helpful in modern video editing software and provide real-time online segmentation solution to
stream media in video live applications. Consequently the online user experience can be improved.
However, there also exists the risk that video segmentation technology is utilized in the scenario of
illegal shoot and malicious edit, thus the personal privacy are more likely to be exposed and tracked.

","Yuxi Li, Ning Xu, Jinlong Peng, John See, Weiyao Lin",Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation,1.0,"{'Multimedia University', 'Shanghai Jiao Tong university', 'Adobe Research', 'Tencent Youtu Lab', 'Shanghai Jiao Tong University'}",,Vision,"{'Malaysia', 'USA', 'China'}",False,False,"As we can foresee, with the development of 5G communication, video-based media industry will develop fast in the future. The advancement of accurate and fast semi-supervised segmentation will be helpful in modern video editing software and provide real-time online segmentation solution to stream media in video live applications. Consequently the online user experience can be improved. However, there also exists the risk that video segmentation technology is utilized in the scenario of illegal shoot and malicious edit, thus the personal privacy are more likely to be exposed and tracked.",Broader Impact,1.0,False,1.0,,"Yuxi Li, Ning Xu, Jinlong Peng, John See, Weiyao Lin",0d5bd023a3ee11c7abca5b42a93c4866,https://proceedings.neurips.cc/paper/2020/file/0d5bd023a3ee11c7abca5b42a93c4866-Paper.pdf,Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation,Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation,Applications -> Computer Vision,Applications -> Video Analysis,4.0,"
As we can foresee, with the development of 5G communication, video-based media industry will
develop fast in the future. The advancement of accurate and fast semi-supervised segmentation will
be helpful in modern video editing software and provide real-time online segmentation solution to
stream media in video live applications. Consequently the online user experience can be improved.
However, there also exists the risk that video segmentation technology is utilized in the scenario of
illegal shoot and malicious edit, thus the personal privacy are more likely to be exposed and tracked.

",261,Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation,https://papers.nips.cc/paper/2020/file/0d5bd023a3ee11c7abca5b42a93c4866-Paper.pdf,89.0
294,"
This work does not present any foreseeable societal consequence.","Xiang Wang, Chenwei Wu, Jason D. Lee, Tengyu Ma, Rong Ge",Beyond Lazy Training for Over-parameterized Tensor Decomposition,1.0,"{'Stanford University', 'Princeton University', 'Duke University'}",False,Theory (including computational and statistical analyses),{'USA'},False,False,This work does not present any foreseeable societal consequence.,Broader Impact,0.0,False,0.0,True,"Xiang Wang, Chenwei Wu, Jason D. Lee, Tengyu Ma, Rong Ge",f9d3a954de63277730a1c66d8b38dee3,https://proceedings.neurips.cc/paper/2020/file/f9d3a954de63277730a1c66d8b38dee3-Paper.pdf,Beyond Lazy Training for Over-parameterized Tensor Decomposition,Beyond Lazy Training for Over-parameterized Tensor Decomposition,Optimization -> Non-Convex Optimization,Applications -> Matrix and Tensor Factorization,1.0,"
This work does not present any foreseeable societal consequence.",294,Beyond Lazy Training for Over-parameterized Tensor Decomposition,https://papers.nips.cc/paper/2020/file/f9d3a954de63277730a1c66d8b38dee3-Paper.pdf,9.0
