doc_id,text,IsPatentCited,abstract,aff_affiliations,aff_authorids,aff_paperid,authors,citationCount,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,publicationDate,publicationTypes,referenceCount,title,url,venue,year
1965447681,"Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.",1.0,"Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.","['Dept. of Computer Sci. University of Toronto', 'Dept. of Computer Sci. University of Toronto']","['2123357461', '2130130894']",1965447681.0,"{'offset': 0, 'data': [{'authorId': '33222621', 'url': 'https://www.semanticscholar.org/author/33222621', 'name': 'Mike Wu', 'affiliations': [], 'homepage': None, 'paperCount': 20, 'citationCount': 1125}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",527.0,"{'MAG': '1965447681', 'DBLP': 'conf/uist/WuB03', 'DOI': '10.1145/964696.964718', 'CorpusId': 207583051}",['Computer Science'],30.0,True,{'pages': '193-202'},11/2/2003,['JournalArticle'],33.0,Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays,https://www.semanticscholar.org/paper/bbd13d2f92aeb08d73978e8958a12f793f175b8c,UIST,2003
1977150168,"Computer sliders are a generic user input mechanism for specifying a numeric value from a range. For data visualization, the effectiveness of sliders may be increased by using the space inside the slider as• an interactive color scale,• a barplot for discrete data, and• a density plot for continuous data.The idea is to show the selected values in relation to the data and its distribution. Furthermore, the selection mechanism may be generalized using a painting metaphor to specify arbitrary, disconnected intervals while maintaining an intuitive user-interface.",1.0,"Computer sliders are a generic user input mechanism for specifying a numeric value from a range. For data visualization, the effectiveness of sliders may be increased by using the space inside the slider as• an interactive color scale,• a barplot for discrete data, and• a density plot for continuous data.The idea is to show the selected values in relation to the data and its distribution. Furthermore, the selection mechanism may be generalized using a painting metaphor to specify arbitrary, disconnected intervals while maintaining an intuitive user-interface.","['AT&T Bell Laboratories - Room IHC 1G-351, 1000 E. Warrenville Road, Naperville, Illinois']",['2230662789'],1977150168.0,"{'offset': 0, 'data': [{'authorId': '34692876', 'url': 'https://www.semanticscholar.org/author/34692876', 'name': 'S. Eick', 'affiliations': [], 'homepage': None, 'paperCount': 112, 'citationCount': 5091}]}",123.0,"{'MAG': '64851601', 'DBLP': 'conf/uist/Eick94', 'DOI': '10.1145/192426.192472', 'CorpusId': 14230987}",['Computer Science'],5.0,False,{'pages': '119-120'},11/2/1994,['JournalArticle'],10.0,Data visualization sliders,https://www.semanticscholar.org/paper/540232bb2196b622251126b7981c17bba66b5cbe,UIST,1994
1984575025,"This research explores distributed sensing techniques for mobile devices using synchronous gestures. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.",1.0,"This research explores distributed sensing techniques for mobile devices using synchronous gestures. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.","['Microsoft Research, One Microsoft Way Redmond, WA#TAB#']",['1560725665'],1984575025.0,"{'offset': 0, 'data': [{'authorId': '1738072', 'url': 'https://www.semanticscholar.org/author/1738072', 'name': 'K. Hinckley', 'affiliations': [], 'homepage': None, 'paperCount': 163, 'citationCount': 9081}]}",299.0,"{'MAG': '1984575025', 'DBLP': 'conf/uist/Hinckley03', 'DOI': '10.1145/964696.964713', 'CorpusId': 10639720}",['Computer Science'],18.0,True,{'pages': '149-158'},11/2/2003,['JournalArticle'],32.0,Synchronous gestures for multiple persons and computers,https://www.semanticscholar.org/paper/c24047d431d0c1d1881e83bbf0caf1fd86ffc2ee,UIST,2003
1984975460,"While onboard navigation systems are gaining in importance, maps are still the medium of choice for laying out a route to a destination and for way finding. However, even with a map, one is almost always more comfortable navigating a route the second time due to the visual memory of the route. To make the first time navigating a route feel more familiar, we present a system that integrates a map with a video automatically constructed from panoramic imagery captured at close intervals along the route. The routing information is used to create a variable speed video depicting the route. During playback of the video, the frame and field of view are dynamically modulated to highlight salient features along the route and connect them back to the map. A user interface is demonstrated to allow exploration of the combined map, video, and textual driving directions. We discuss the construction of the hybrid map and video interface. Finally, we report the results of a study that provides evidence of the effectiveness of such a system for route following.",1.0,"While onboard navigation systems are gaining in importance, maps are still the medium of choice for laying out a route to a destination and for way finding. However, even with a map, one is almost always more comfortable navigating a route the second time due to the visual memory of the route. To make the first time navigating a route feel more familiar, we present a system that integrates a map with a video automatically constructed from panoramic imagery captured at close intervals along the route. The routing information is used to create a variable speed video depicting the route. During playback of the video, the frame and field of view are dynamically modulated to highlight salient features along the route and connect them back to the map. A user interface is demonstrated to allow exploration of the combined map, video, and textual driving directions. We discuss the construction of the hybrid map and video interface. Finally, we report the results of a study that provides evidence of the effectiveness of such a system for route following.","['Microsoft, Redmond WA, USA', 'University of Konstanz, Konstanz, GERMANY', 'Microsoft, Redmond WA, USA', 'Microsoft, Redmond WA, USA', 'University of Konstanz, Konstanz, GERMANY']","['1794776656', '1942387931', '2124876992', '3049172691', '74604451']",1984975460.0,"{'offset': 0, 'data': [{'authorId': '48951491', 'url': 'https://www.semanticscholar.org/author/48951491', 'name': 'Billy Chen', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 1289}, {'authorId': '2466324', 'url': 'https://www.semanticscholar.org/author/2466324', 'name': 'B. Neubert', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 1072}, {'authorId': '20592981', 'url': 'https://www.semanticscholar.org/author/20592981', 'name': 'E. Ofek', 'affiliations': [], 'homepage': None, 'paperCount': 130, 'citationCount': 6658}, {'authorId': '1850438', 'url': 'https://www.semanticscholar.org/author/1850438', 'name': 'O. Deussen', 'affiliations': [], 'homepage': None, 'paperCount': 274, 'citationCount': 8075}, {'authorId': '1400248273', 'url': 'https://www.semanticscholar.org/author/1400248273', 'name': 'Michael F. Cohen', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 20971}]}",24.0,"{'MAG': '1984975460', 'DBLP': 'conf/uist/ChenNODC09', 'DOI': '10.1145/1622176.1622218', 'CorpusId': 7041529}",['Computer Science'],1.0,True,{'pages': '223-232'},10/4/2009,['JournalArticle'],17.0,Integrated videos and maps for driving directions,https://www.semanticscholar.org/paper/052cddeef87d81cf3c4f42ff56c16273fb67d2fa,UIST,2009
1985543789,"In this paper we propose a novel way of supporting occasional meetings that take place in unfamiliar public places, which promotes lightweight, visible and fluid collaboration. Our central idea is that the sharing and exchange of information occurs across public surfaces that users can easily access and interact with. To this end, we designed and implemented Dynamo, a communal multi-user interactive surface. The surface supports the cooperative sharing and exchange of a wide range of media that can be brought to the surface by users that are remote from their familiar organizational settings.",1.0,"In this paper we propose a novel way of supporting occasional meetings that take place in unfamiliar public places, which promotes lightweight, visible and fluid collaboration. Our central idea is that the sharing and exchange of information occurs across public surfaces that users can easily access and interact with. To this end, we designed and implemented Dynamo, a communal multi-user interactive surface. The surface supports the cooperative sharing and exchange of a wide range of media that can be brought to the surface by users that are remote from their familiar organizational settings.","['The Mixed Reality Lab, University of Nottingham, Nottingham, UK NG8 1BB', 'The Interact Lab, COGS, University of Sussex, Sussex, UK', 'The Mixed Reality Lab, University of Nottingham, Nottingham, UK NG8 1BB', 'The Interact Lab, COGS, University of Sussex, Sussex, UK', 'The Interact Lab, COGS, University of Sussex, Sussex, UK']","['1980670276', '2039523318', '2098553916', '2144042109', '2166804211']",1985543789.0,"{'offset': 0, 'data': [{'authorId': '79406746', 'url': 'https://www.semanticscholar.org/author/79406746', 'name': 'S. Izadi', 'affiliations': [], 'homepage': None, 'paperCount': 212, 'citationCount': 20595}, {'authorId': '2967611', 'url': 'https://www.semanticscholar.org/author/2967611', 'name': 'Harry Brignull', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 1215}, {'authorId': '145282573', 'url': 'https://www.semanticscholar.org/author/145282573', 'name': 'T. Rodden', 'affiliations': [], 'homepage': None, 'paperCount': 378, 'citationCount': 15105}, {'authorId': '1685816', 'url': 'https://www.semanticscholar.org/author/1685816', 'name': 'Y. Rogers', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 21895}, {'authorId': '33347932', 'url': 'https://www.semanticscholar.org/author/33347932', 'name': 'M. Underwood', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 363}]}",291.0,"{'DBLP': 'conf/uist/IzadiBRRU03', 'MAG': '1985543789', 'DOI': '10.1145/964696.964714', 'CorpusId': 8908560}",['Computer Science'],18.0,False,{'pages': '159-168'},11/2/2003,['JournalArticle'],30.0,Dynamo: a public interactive surface supporting the cooperative sharing and exchange of media,https://www.semanticscholar.org/paper/9b740418b8c689b13a153c50055a3b329480cbca,UIST,2003
1991211783,"Spatial augmented reality (SAR) promises the integration of digital information in the real (physical) world through projection. In this doctoral symposium paper, I propose different tools to improve speed or ease the drawing by projecting photos, virtual construction lines and interactive 3D scenes. After describing the tools, I explain some future challenges to explore such as the creation of tools which helps to create drawings that are ""difficult"" to achieve for a human being, but easy to do by a computer. Furthermore, I propose some insights for the creation of digital games and programs which can take full advantages of physical drawings.",0.0,"Spatial augmented reality (SAR) promises the integration of digital information in the real (physical) world through projection. In this doctoral symposium paper, I propose different tools to improve speed or ease the drawing by projecting photos, virtual construction lines and interactive 3D scenes. After describing the tools, I explain some future challenges to explore such as the creation of tools which helps to create drawings that are ""difficult"" to achieve for a human being, but easy to do by a computer. Furthermore, I propose some insights for the creation of digital games and programs which can take full advantages of physical drawings.","['Inria Bordeaux Sud-Ouest, Bordeaux, France.', 'Bordeaux University, Bordeaux, France']","['1311744216', '432195661']",1991211783.0,"{'offset': 0, 'data': [{'authorId': '3421303', 'url': 'https://www.semanticscholar.org/author/3421303', 'name': 'Jérémy Laviole', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 249}, {'authorId': '2281511', 'url': 'https://www.semanticscholar.org/author/2281511', 'name': 'M. Hachet', 'affiliations': [], 'homepage': None, 'paperCount': 150, 'citationCount': 2396}]}",7.0,"{'DBLP': 'conf/uist/LavioleH12a', 'MAG': '1991211783', 'DOI': '10.1145/2380296.2380316', 'CorpusId': 6094073}",['Computer Science'],0.0,True,{'pages': '43-46'},10/7/2012,['JournalArticle'],15.0,Spatial augmented reality to enhance physical artistic creation,https://www.semanticscholar.org/paper/fff5e7c3a4ab929331854e3d6ba15cae0a4a3497,UIST,2012
1993608429,"We have previously developed a collaborative infrastructure called SCAPE - an acronym for Stereoscopic Collaboration in Augmented and Projective Environments - that integrates the traditionally separate paradigms of virtual and augmented reality. In this paper, we extend SCAPE by formalizing its underlying mathematical framework and detailing three augmented Widgets constructed via this framework: CoCylinder, Magnifier, and CoCube. These devices promote intuitive ways of selecting, examining, and sharing synthetic objects, and retrieving associated documentary text. Finally we present a testbed application to showcase SCAPE's capabilities for interaction in large, augmented virtual environments.",1.0,"We have previously developed a collaborative infrastructure called SCAPE - an acronym for Stereoscopic Collaboration in Augmented and Projective Environments - that integrates the traditionally separate paradigms of virtual and augmented reality. In this paper, we extend SCAPE by formalizing its underlying mathematical framework and detailing three augmented Widgets constructed via this framework: CoCylinder, Magnifier, and CoCube. These devices promote intuitive ways of selecting, examining, and sharing synthetic objects, and retrieving associated documentary text. Finally we present a testbed application to showcase SCAPE's capabilities for interaction in large, augmented virtual environments.","['Information & Computer Sciences, University of Hawaii, Honolulu, HI#TAB#', 'Beckman Inst., Univ. of Illinois, Urbana, IL', 'Beckman Inst., Univ. of Illinois, Urbana, IL']","['2120873243', '2161407021', '2341421681']",1993608429.0,"{'offset': 0, 'data': [{'authorId': '37980155', 'url': 'https://www.semanticscholar.org/author/37980155', 'name': 'L. D. Brown', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 584}, {'authorId': '143768640', 'url': 'https://www.semanticscholar.org/author/143768640', 'name': 'H. Hua', 'affiliations': [], 'homepage': None, 'paperCount': 187, 'citationCount': 3888}, {'authorId': '40665566', 'url': 'https://www.semanticscholar.org/author/40665566', 'name': 'C. Gao', 'affiliations': [], 'homepage': None, 'paperCount': 30, 'citationCount': 922}]}",304.0,"{'MAG': '1993608429', 'DBLP': 'conf/uist/BrownHG03', 'DOI': '10.1145/964696.964697', 'CorpusId': 16182440}",['Computer Science'],4.0,False,{'pages': '1-10'},11/2/2003,['JournalArticle'],36.0,A widget framework for augmented interaction in SCAPE,https://www.semanticscholar.org/paper/eb9639cce59d38265e775c1e8593edf0d9ea4071,UIST,2003
1995004236,"In this paper, we describe Kimura, an augmented office environment to support common multitasking practices. Previous systems, such as Rooms, limit users by constraining the interaction to the desktop monitor. In Kimura, we leverage interactive projected peripheral displays to support the perusal, manipulation and awareness of background activities. Furthermore, each activity is represented by a montage comprised of images from current and past interaction on the desktop. These montages help remind the user of past actions, and serve as a springboard for ambient context-aware reminders and notifications.",1.0,"In this paper, we describe Kimura, an augmented office environment to support common multitasking practices. Previous systems, such as Rooms, limit users by constraining the interaction to the desktop monitor. In Kimura, we leverage interactive projected peripheral displays to support the perusal, manipulation and awareness of background activities. Furthermore, each activity is represented by a montage comprised of images from current and past interaction on the desktop. These montages help remind the user of past actions, and serve as a springboard for ambient context-aware reminders and notifications.","['Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'University of Aarhus, Aabogade 34A, 8200 Aarhus N, Denmark#TAB#']","['18171182', '2022470897', '2028099635', '2108490771', '211303378', '2781355347']",1995004236.0,"{'offset': 0, 'data': [{'authorId': '145931052', 'url': 'https://www.semanticscholar.org/author/145931052', 'name': 'B. MacIntyre', 'affiliations': [], 'homepage': None, 'paperCount': 151, 'citationCount': 11664}, {'authorId': '1752751', 'url': 'https://www.semanticscholar.org/author/1752751', 'name': 'Elizabeth D. Mynatt', 'affiliations': [], 'homepage': None, 'paperCount': 262, 'citationCount': 13114}, {'authorId': '1804450', 'url': 'https://www.semanticscholar.org/author/1804450', 'name': 'Stephen Voida', 'affiliations': [], 'homepage': None, 'paperCount': 81, 'citationCount': 2032}, {'authorId': '1781815', 'url': 'https://www.semanticscholar.org/author/1781815', 'name': 'K. M. Hansen', 'affiliations': [], 'homepage': None, 'paperCount': 106, 'citationCount': 2057}, {'authorId': '3126077', 'url': 'https://www.semanticscholar.org/author/3126077', 'name': 'J. Tullio', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 1137}, {'authorId': '32130173', 'url': 'https://www.semanticscholar.org/author/32130173', 'name': 'G. Corso', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 573}]}",206.0,"{'DBLP': 'conf/uist/MacIntyreMVHTC01', 'MAG': '1995004236', 'DOI': '10.1145/502348.502355', 'CorpusId': 5976911}",['Computer Science'],8.0,False,{'pages': '41-50'},11/11/2001,['JournalArticle'],30.0,Support for multitasking and background awareness using interactive peripheral displays,https://www.semanticscholar.org/paper/4354e022e9afeda60829579142d6062d2913a29f,UIST,2001
1996202517,"UIST is the premier forum on innovative engineering of the human-computer interface. The symposium brings together user-interface researchers and practitioners with an interest in techniques, tools, and technology for constructing quality, innovative user interfaces. UIST grew out of a series of SIGGRAPH-supported workshops on User Interface Management Systems (UIMS).",0.0,"UIST is the premier forum on innovative engineering of the human-computer interface. The symposium brings together user-interface researchers and practitioners with an interest in techniques, tools, and technology for constructing quality, innovative user interfaces. UIST grew out of a series of SIGGRAPH-supported workshops on User Interface Management Systems (UIMS).",['Université Paris-Sud'],['2674390412'],1996202517.0,"{'offset': 0, 'data': [{'authorId': '1401678555', 'url': 'https://www.semanticscholar.org/author/1401678555', 'name': 'M. Beaudouin-Lafon', 'affiliations': [], 'homepage': None, 'paperCount': 177, 'citationCount': 7255}]}",0.0,"{'MAG': '1996202517', 'DBLP': 'conf/uist/2002', 'DOI': '10.1145/571985', 'CorpusId': 45681232}",['Computer Science'],0.0,False,"{'name': '', 'volume': ''}",10/27/2002,['Conference'],0.0,"Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology, Paris, France, October 27-30, 2002",https://www.semanticscholar.org/paper/f95e1a99de397345a6c4943234fa5692e304e3b8,UIST,2002
1998246765,"We present the iRing, an intelligent input ring device developed for measuring finger gestures and external input. iRing recognizes rotation, finger bending, and external force via an infrared (IR) reflection sensor that leverages skin characteristics such as reflectance and softness. Furthermore, iRing allows using a push and stroke input method, which is popular in touch displays. The ring design has potential to be used as a wearable controller because its accessory shape is socially acceptable, easy to install, and safe, and iRing does not require extra devices. We present examples of iRing applications and discuss its validity as an inexpensive wearable interface and as a human sensing device.",1.0,"We present the iRing, an intelligent input ring device developed for measuring finger gestures and external input. iRing recognizes rotation, finger bending, and external force via an infrared (IR) reflection sensor that leverages skin characteristics such as reflectance and softness. Furthermore, iRing allows using a push and stroke input method, which is popular in touch displays. The ring design has potential to be used as a wearable controller because its accessory shape is socially acceptable, easy to install, and safe, and iRing does not require extra devices. We present examples of iRing applications and discuss its validity as an inexpensive wearable interface and as a human sensing device.","['Keio University, YOKOHAMA, KANAGAWA, Japan', 'Keio University, YOKOHAMA, KANAGAWA, Japan', 'Keio University, YOKOHAMA, KANAGAWA, Japan', 'Keio University, YOKOHAMA, KANAGAWA, Japan']","['2094771307', '2112088904', '2155357175', '2810579644']",1998246765.0,"{'offset': 0, 'data': [{'authorId': '2107306', 'url': 'https://www.semanticscholar.org/author/2107306', 'name': 'Masa Ogata', 'affiliations': [], 'homepage': None, 'paperCount': 35, 'citationCount': 508}, {'authorId': '1799242', 'url': 'https://www.semanticscholar.org/author/1799242', 'name': 'Yuta Sugiura', 'affiliations': [], 'homepage': None, 'paperCount': 175, 'citationCount': 1137}, {'authorId': '145978249', 'url': 'https://www.semanticscholar.org/author/145978249', 'name': 'Hirotaka Osawa', 'affiliations': [], 'homepage': None, 'paperCount': 133, 'citationCount': 718}, {'authorId': '1752970', 'url': 'https://www.semanticscholar.org/author/1752970', 'name': 'M. Imai', 'affiliations': [], 'homepage': None, 'paperCount': 267, 'citationCount': 3734}]}",86.0,"{'DBLP': 'conf/uist/OgataSOI12', 'MAG': '1998246765', 'DOI': '10.1145/2380116.2380135', 'CorpusId': 18774243}",['Computer Science'],5.0,False,{'name': 'Proceedings of the 25th annual ACM symposium on User interface software and technology'},10/7/2012,"['Book', 'JournalArticle', 'Conference']",8.0,iRing: intelligent ring using infrared reflection,https://www.semanticscholar.org/paper/869156d56cf1ce9c9c9cbc20aa83b6e6f320566d,UIST,2012
2005178383,"Current touchscreen technology does not provide adequate haptic feedback to the user. Mostly haptic feedback solutions for touchscreens involve either a) deforming the surface layers screen itself or b) placing actuators under the screen to vibrate it. This means that we have only limited control over where on the screen the feedback feels like it is coming from, and that we are limited to feedback that feels like movement up and down, orthogonal to the screen. In this work I demonstrate a novel technique for haptic feedback: sawtooth planar waves. In a series of paper Canny & Reznick showed that sawtooth planar waves could be used for object manipulation. Here that technique is applied to haptic feedback. By varying the input waves, from 1 one to 4 actuators, it is possible to provide feelings of motion in any planar direction to a finger at one point on the screen while providing a different sensation, or none at all, to fingers placed at several other points on the screen.",0.0,"Current touchscreen technology does not provide adequate haptic feedback to the user. Mostly haptic feedback solutions for touchscreens involve either a) deforming the surface layers screen itself or b) placing actuators under the screen to vibrate it. This means that we have only limited control over where on the screen the feedback feels like it is coming from, and that we are limited to feedback that feels like movement up and down, orthogonal to the screen. In this work I demonstrate a novel technique for haptic feedback: sawtooth planar waves. In a series of paper Canny & Reznick showed that sawtooth planar waves could be used for object manipulation. Here that technique is applied to haptic feedback. By varying the input waves, from 1 one to 4 actuators, it is possible to provide feelings of motion in any planar direction to a finger at one point on the screen while providing a different sensation, or none at all, to fingers placed at several other points on the screen.","['Nokia, Sunnyvale, California, USA']",['2159473637'],2005178383.0,"{'offset': 0, 'data': [{'authorId': '51921005', 'url': 'https://www.semanticscholar.org/author/51921005', 'name': 'Jofish Kaye', 'affiliations': [], 'homepage': None, 'paperCount': 68, 'citationCount': 3278}]}",7.0,"{'MAG': '2005178383', 'DBLP': 'conf/uist/Kaye12', 'DOI': '10.1145/2380296.2380300', 'CorpusId': 34405124}",['Computer Science'],4.0,False,{'pages': '5-6'},10/7/2012,['JournalArticle'],5.0,Sawtooth planar waves for haptic feedback,https://www.semanticscholar.org/paper/b7c521b99240a47af65813747f9102de6f4a8255,UIST,2012
2007262359,"Creating Interactive Techniques by Symbolically Solving Geometric Constraints Dan R. Olsen Jr. Kirk Allan Computer Science Department Brigham Young University Provo, UT 84602 olsen.chi@xerox.com or olsen@bunsen.byu.edu The Geometric Interactive Technique Solver (GITS) is described. New interactive techniques are created by drawing them and then placing constraints on the drawing’s geometry. The semantic interface is defined by parameters of the techniques which are also related to the geometry by constraints. A designer can define interactive methods for a technique, for which GITS will generate symbolic constraint solutions. Code is generated from the constraint solutions to provide an implementation of the interactive technique. The constraint solving and code generation algorithms are discussed.",0.0,"Creating Interactive Techniques by Symbolically Solving Geometric Constraints Dan R. Olsen Jr. Kirk Allan Computer Science Department Brigham Young University Provo, UT 84602 olsen.chi@xerox.com or olsen@bunsen.byu.edu The Geometric Interactive Technique Solver (GITS) is described. New interactive techniques are created by drawing them and then placing constraints on the drawing’s geometry. The semantic interface is defined by parameters of the techniques which are also related to the geometry by constraints. A designer can define interactive methods for a technique, for which GITS will generate symbolic constraint solutions. Code is generated from the constraint solutions to provide an implementation of the interactive technique. The constraint solving and code generation algorithms are discussed.","['Computer Science Department, Brigham Young University, Provo, UT,#TAB#', 'Computer Science Department, Brigham Young University, Provo, UT,#TAB#']","['2148302434', '2227367133']",2007262359.0,"{'offset': 0, 'data': [{'authorId': '1733794', 'url': 'https://www.semanticscholar.org/author/1733794', 'name': 'D. Olsen', 'affiliations': [], 'homepage': None, 'paperCount': 107, 'citationCount': 4379}, {'authorId': '2068757022', 'url': 'https://www.semanticscholar.org/author/2068757022', 'name': 'Kirk Allan', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 38}]}",38.0,"{'MAG': '2007262359', 'DBLP': 'conf/uist/OlsenA90', 'DOI': '10.1145/97924.97936', 'CorpusId': 14814671}",['Computer Science'],1.0,False,{'pages': '102-107'},8/1/1990,['JournalArticle'],9.0,Creating interactive techniques by symbolically solving geometric constraints,https://www.semanticscholar.org/paper/10e05a17c1ebbc50a1b0c754a0f7b8bdcb3b763e,UIST,1990
2008150314,"This TechNote reports on our initial results of realizing a computer augmented wall called the HoloWall. Using an infrared camera located behind the wall, this system allows a user to interact with this computerized wall using ngers, hands, their body, or even a physical object such as a document folder.",1.0,"This TechNote reports on our initial results of realizing a computer augmented wall called the HoloWall. Using an infrared camera located behind the wall, this system allows a user to interact with this computerized wall using ngers, hands, their body, or even a physical object such as a document folder.","['Sony Computer Science Laboratory Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 141, Japan', 'Department of Computer Science, Keio University, 3-14-1 Hiyoshi, Khoku-ku, Yokohama, Kanagawa 223 Japan']","['1994375393', '3206617274']",2008150314.0,"{'offset': 0, 'data': [{'authorId': '2096057048', 'url': 'https://www.semanticscholar.org/author/2096057048', 'name': 'Nobuyuki Matsushita', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 531}, {'authorId': '1685962', 'url': 'https://www.semanticscholar.org/author/1685962', 'name': 'J. Rekimoto', 'affiliations': [], 'homepage': None, 'paperCount': 312, 'citationCount': 11594}]}",243.0,"{'DBLP': 'conf/uist/MatsushitaR97', 'MAG': '2008150314', 'DOI': '10.1145/263407.263549', 'CorpusId': 18188221}",['Computer Science'],9.0,False,{'pages': '209-210'},10/1/1997,['JournalArticle'],8.0,"HoloWall: designing a finger, hand, body, and object sensitive wall",https://www.semanticscholar.org/paper/d2bd71c4e16710f8b2d5ddd7e1f6b2c3bb81bcb9,UIST,1997
2009357400,"We introduce the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical boom. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We present a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Design issues are discussed and results of an informal user study on the device and annotation software are presented. The results show that the Boom Chameleon annotation facilities have the potential to be an effective, easy to learn and operate 3D design review system.",1.0,"We introduce the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical boom. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We present a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Design issues are discussed and results of an informal user study on the device and annotation software are presented. The results show that the Boom Chameleon annotation facilities have the potential to be an effective, easy to learn and operate 3D design review system.","['Alias&verbar;wavefront, and University of Toronto Toronto, Ontario Canada#TAB#', 'Alias&verbar;wavefront, and University of Toronto Toronto, Ontario Canada#TAB#', 'University of Toronto and Alias&verbar;wavefront, Toronto, Ontario Canada', 'Alias&verbar;wavefront, and University of Toronto Toronto, Ontario Canada#TAB#', 'Alias&verbar;wavefront, and University of Toronto Toronto, Ontario Canada#TAB#']","['1899877228', '2025626156', '2145646624', '2149432401', '2893200430']",2009357400.0,<Response [200]>,167.0,"{'DBLP': 'conf/uist/TsangFKKB02', 'MAG': '2009357400', 'DOI': '10.1145/571985.572001', 'CorpusId': 251539}",['Computer Science'],7.0,True,{'pages': '111-120'},10/27/2002,"['JournalArticle', 'Review']",84.0,"Boom chameleon: simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display",https://www.semanticscholar.org/paper/cf70ed96e9f7ea7dfbbe3658c2a1c0699b9f1672,UIST,2002
2009840077,"A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques that exploit the affordances of the wand, resulting in an effective interface for large scale interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of postures and gestures to track state and enable command input. We also describe the use of multiple wands, and posit designs for more complex wands in the future.",1.0,"A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques that exploit the affordances of the wand, resulting in an effective interface for large scale interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of postures and gestures to track state and enable command input. We also describe the use of multiple wands, and posit designs for more complex wands in the future.","['Dept. of Computer Sci. University of Toronto', 'Dept. of Computer Sci. University of Toronto']","['2130130894', '2151535930']",2009840077.0,"{'offset': 0, 'data': [{'authorId': '2149213356', 'url': 'https://www.semanticscholar.org/author/2149213356', 'name': 'Xiang Cao', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 909}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",96.0,"{'DBLP': 'journals/tog/CaoB04', 'MAG': '2166540415', 'DOI': '10.1145/964696.964716', 'CorpusId': 6187890}",['Computer Science'],6.0,False,"{'name': 'ACM Trans. Graph.', 'pages': '729', 'volume': '23'}",11/2/2003,['JournalArticle'],35.0,VisionWand: interaction techniques for large displays using a passive wand tracked in 3D,https://www.semanticscholar.org/paper/5eb1a71d1c0bb90f516ad947b3c28475b74593f5,UIST,2003
2010227105,Scrolling is an essential part of our everyday computing experience. Contemporary scrolling techniques rely on the explicit initiation of scrolling by the user. The act of scrolling is tightly coupled with the user?s ability to absorb information via the visual channel. The use of eye gaze information is therefore a natural choice for enhancing scrolling techniques. We present several gaze-enhanced scrolling techniques for manual and automatic scrolling which use gaze information as a primary input or as an augmented input. We also introduce the use off-screen gaze-actuated buttons for document navigation and control.,1.0,Scrolling is an essential part of our everyday computing experience. Contemporary scrolling techniques rely on the explicit initiation of scrolling by the user. The act of scrolling is tightly coupled with the user?s ability to absorb information via the visual channel. The use of eye gaze information is therefore a natural choice for enhancing scrolling techniques. We present several gaze-enhanced scrolling techniques for manual and automatic scrolling which use gaze information as a primary input or as an augmented input. We also introduce the use off-screen gaze-actuated buttons for document navigation and control.,"['Stanford University, Stanford, CA, ', 'Stanford University, Stanford, CA, ']","['2091637845', '2195514383']",2010227105.0,"{'offset': 0, 'data': [{'authorId': '2109936985', 'url': 'https://www.semanticscholar.org/author/2109936985', 'name': 'Manu Kumar', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 834}, {'authorId': '1699245', 'url': 'https://www.semanticscholar.org/author/1699245', 'name': 'T. Winograd', 'affiliations': [], 'homepage': None, 'paperCount': 217, 'citationCount': 40096}]}",57.0,"{'MAG': '2010227105', 'DBLP': 'conf/uist/KumarW07', 'DOI': '10.1145/1294211.1294249', 'CorpusId': 17381387}",['Computer Science'],3.0,True,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference']",15.0,Gaze-enhanced scrolling techniques,https://www.semanticscholar.org/paper/5e893f4b4f0eeb24e4c83f43047da42da76a7ec0,UIST,2007
2010478175,"This paper discusses the internationalization of the X Window System developed by the MIT X consortium. The main purpose is to enable X Window System Release 4 (X11R4) and earlier versions to support Asian languages, primarily Japanese, Chinese, and Korean. Unlike English and other European-based languages, Asian languages involve idiographic character manipulation. X Window System XIlR4 and earlier versions can output such ideograms when the corresponding fonts are provided, but they have no corresponding input feature. Asian language input thus involves more than one keystroke to input a single ideogram, e.g., Japanese-language input uses romaji-kana-kanji conversion. This paper proposes an ideogram input architecture on the X Window System and discusses the interfaces between conversion systems Window oriented and X application programs. Like the X System, our input-conversion feature is to a distributed network environment.",1.0,"This paper discusses the internationalization of the X Window System developed by the MIT X consortium. The main purpose is to enable X Window System Release 4 (X11R4) and earlier versions to support Asian languages, primarily Japanese, Chinese, and Korean. Unlike English and other European-based languages, Asian languages involve idiographic character manipulation. X Window System XIlR4 and earlier versions can output such ideograms when the corresponding fonts are provided, but they have no corresponding input feature. Asian language input thus involves more than one keystroke to input a single ideogram, e.g., Japanese-language input uses romaji-kana-kanji conversion. This paper proposes an ideogram input architecture on the X Window System and discusses the interfaces between conversion systems Window oriented and X application programs. Like the X System, our input-conversion feature is to a distributed network environment.","['Open System Development Group, Development Department II, Fujitsu Limited, 1841-1, Tsuruma Aza 19, Machida, Tokyo, Japan#TAB#', 'Computer System R&D Laboratory, Omron Corporation, Shimokaiinji, Nagaokakyo, Kyoto, Japan', 'NTT Communications and Information Processing Laboratories, Nippon Telegraph and Telephone Corporation, 1-2356, Take, Yokosuka, Kanagawa, Japan', 'Computer System R&D Laboratory, Omron Corporation, Shimokaiinji, Nagaokakyo, Kyoto, Japan', 'NTT Communications and Information Processing Laboratories, Nippon Telegraph and Telephone Corporation, 1-2356, Take, Yokosuka, Kanagawa, Japan']","['2136108437', '2208191551', '2222418193', '2226714555', '2656110253']",2010478175.0,"{'offset': 0, 'data': [{'authorId': '2075882327', 'url': 'https://www.semanticscholar.org/author/2075882327', 'name': 'M. Morisaki', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 50}, {'authorId': '2070115169', 'url': 'https://www.semanticscholar.org/author/2070115169', 'name': 'Etsuo Kawada', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 7}, {'authorId': '2085177202', 'url': 'https://www.semanticscholar.org/author/2085177202', 'name': 'Hiroshi Kuribayashi', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 7}, {'authorId': '148078992', 'url': 'https://www.semanticscholar.org/author/148078992', 'name': 'Seiji Kuwari', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 4}, {'authorId': '1906401', 'url': 'https://www.semanticscholar.org/author/1906401', 'name': 'M. Narita', 'affiliations': [], 'homepage': None, 'paperCount': 60, 'citationCount': 256}]}",4.0,"{'MAG': '2010478175', 'DBLP': 'conf/uist/MorisakiKKKN91', 'DOI': '10.1145/120782.120802', 'CorpusId': 15268150}",['Computer Science'],0.0,False,{'pages': '185-194'},11/11/1991,['JournalArticle'],8.0,XJp system: an internationalized language interface for the X Window system,https://www.semanticscholar.org/paper/f7a984766cbc3eca22f16149a84182fc73bac2be,UIST,1991
2012368457,"We have implemented a browser companion called PadPrints that dynamically builds a graphical history-map of visited web pages. PadPrints relies on Pad++, a zooming user interface (ZUI) development substrate, to display the history-map. PadPrints functions in conjunction with a traditional web browser but without requiring any browser modifications.",1.0,"We have implemented a browser companion called PadPrints that dynamically builds a graphical history-map of visited web pages. PadPrints relies on Pad++, a zooming user interface (ZUI) development substrate, to display the history-map. PadPrints functions in conjunction with a traditional web browser but without requiring any browser modifications.","['Computer Science Department., Human-Computer, Interaction Lab, University of Maryland#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Cognitive Science Dept., University of California, San Diego#TAB#']","['2046653803', '2118963697', '2305937705', '2467223492', '400323076']",2012368457.0,"{'offset': 0, 'data': [{'authorId': '38807270', 'url': 'https://www.semanticscholar.org/author/38807270', 'name': 'R. Hightower', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 503}, {'authorId': '2102074821', 'url': 'https://www.semanticscholar.org/author/2102074821', 'name': 'Laura T. Ring', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 157}, {'authorId': '145461274', 'url': 'https://www.semanticscholar.org/author/145461274', 'name': 'J. Helfman', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 1200}, {'authorId': '1799187', 'url': 'https://www.semanticscholar.org/author/1799187', 'name': 'B. Bederson', 'affiliations': [], 'homepage': None, 'paperCount': 350, 'citationCount': 19703}, {'authorId': '1698170', 'url': 'https://www.semanticscholar.org/author/1698170', 'name': 'J. Hollan', 'affiliations': [], 'homepage': None, 'paperCount': 155, 'citationCount': 10261}]}",30.0,"{'DBLP': 'conf/uist/HightowerRHBH98', 'MAG': '2012368457', 'DOI': '10.1145/288392.288582', 'CorpusId': 6712343}",['Computer Science'],0.0,False,{'pages': '121-122'},11/1/1998,['JournalArticle'],5.0,PadPrints: graphical multiscale Web histories,https://www.semanticscholar.org/paper/9979445a327e02894dd313afb7a6109908c208e0,UIST,1998
2013714982,"Randy Pausch & Ronald D, Williams University of Virginia Thornton Hall Charlottesville, VA 22903 (pausch@Virginia.edu) 804-982-2211 Physical controls for most devices are either “one size fits all” or require custom hardware for each user. Cost often prohibits custom design, and each user must adapt to the standard device interface, typically with a loss of precision and efficiency. When user abilities vary widely, such as in the disabled community, devices often become unusable. Our goal is to create a system that will track user gestures and interpret them as control signals for devices. Existing gesture recognition research converts continuous body motion into discrete symbols. Our approach is to map continuous motions into a set of analog device control signals. Our system will allow us to quickly tailor a device interface to each user’s best physical range of motion. Our first application domain is a speech synthesizer for disabled users. We expect two major areas of applicability for non-disabled users: in telemanipulator interfaces, and as a design tool for creating biomechanically efficient interfaces. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Introduction The Augmentative Communications Group at the University of Virginia consists of researchers from the Computer Science and Electrical Engineering departments, and from the Medical and Education schools. Our current effort is to create a speech synthesizer for individuals with cerebral palsy, a disability affecting approximately 700,000 Americans [Il. A significant portion of the cerebral palsy population is communicative but non-verbal although the desire to communicate is present, speech is prohibited by damage to the part of the brain that controls the vocal tract. Most of these individuals do not have enough coordination for handwriting or typing. Al though primitive electronic communication aids exist, most are variations on picture boards, where the user points or looks at a twodimensional array of pictures to convey a thought such as “hungry” or “tired.” The Augmentative Communications Group has developed a speech synthesizer based on two-dimensional analog input. The creation of speech involves the coordination of a large number of muscles in the vocal tract. The synthesizer approximates this by receiving the position of the base and tip of the tongue as input signals and then synthesizes the sound produced by that position of the tongue. We have implemented a prototype @ 1990 ACM 089791410-4/90/0010/0123 $1.50 123 which synthesizes monotone speech from two analogue signals. The original research strategy was to attempt to design and build custom input devices for each user of the system. Biomedical engineers constructed various onedimensional potentiometers to be used in pairs to provide the analog inputs needed to drive the synthesizer. Building custom hardware interfaces is expensive, and cerebral palsy victims often have reduced strength, making control of any physical device cumbersome. As users fatigue, their efficiency with a particular device decreases and several different devices may be needed to accommodate various stages of fatigue. Our new approach is to create an individual gesture interface for each user. Our software maps body motions, reported by magnetic trackers, into continuous control signals for the speech synthesizer. The only physical effort by the user is to move a part of his body. This software tailoring allows us to create interfaces based on each user’s individual abilities, and makes it possible for those interfaces to adapt as the user fatigues. The idea of user tailoring, or customization, has long been understood as a crucial element in the design of traditional computer interfaces. Text editors allow users to rebind keyboards so that commonly used operations are easier to reach [ZI. Mice often allow for alterations in the ratio of device motion to cursor motion, to accommodate variations in user coordination. Some hardware interfaces allow minor customization! such as tilt-lock steering wheels in automobiles. Customization is necessary when there is a high degree of user idiosyncrasy, relative to the dexterity required for the task. While the need for individual tailoring is most apparent in the disabled community, able-bodied users exhibit high idiosyncrasy with respect to tasks where extreme dexterity is required, such as telemanipulation in microsurgery. We expect our techniques to be useful for able-bodied users when they must use interfaces that require high dexterity to complete tasks. Existing gesture research is dominated by a desire to understand or interpret gestures, and is commonly referred to as gesture recognition. Our approach is to map continuous data from one or more sensors to a set of continuous device control signals, rather than transforming gestures into symbols. Our primary goal is to create custom gesture mappings for device control. Our secondary goal is to make our mappings dynamically adjust for fatigue and changes in user ability. A Joystick-Driven Speech Synthesizer Because our synthesizer is such an unusual device, we first describe how it is able to synthesize speech from two analog inputs. Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature [3-51. The problem addressed here differs from previous work because we limit the number of articulator control parameters to those that can be provided by a human user in real-time. Articulator driven. synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech. A brief review of human physical speech production will be helpful in understanding the articulator driven synthesis approach. The physical process of speech production can be divided into three parts, First, air is forced through the vocal cords to produce either a voiced or unvoiced glottal excitation. Next, air flow is modified by a series of structures that constitute the vocal tract. Finally, the modified flow is radiated through the lips and nostrils. [61. The articulators used to produce speech are shown in Figure 1.",0.0,"Randy Pausch & Ronald D, Williams University of Virginia Thornton Hall Charlottesville, VA 22903 (pausch@Virginia.edu) 804-982-2211 Physical controls for most devices are either “one size fits all” or require custom hardware for each user. Cost often prohibits custom design, and each user must adapt to the standard device interface, typically with a loss of precision and efficiency. When user abilities vary widely, such as in the disabled community, devices often become unusable. Our goal is to create a system that will track user gestures and interpret them as control signals for devices. Existing gesture recognition research converts continuous body motion into discrete symbols. Our approach is to map continuous motions into a set of analog device control signals. Our system will allow us to quickly tailor a device interface to each user’s best physical range of motion. Our first application domain is a speech synthesizer for disabled users. We expect two major areas of applicability for non-disabled users: in telemanipulator interfaces, and as a design tool for creating biomechanically efficient interfaces. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Introduction The Augmentative Communications Group at the University of Virginia consists of researchers from the Computer Science and Electrical Engineering departments, and from the Medical and Education schools. Our current effort is to create a speech synthesizer for individuals with cerebral palsy, a disability affecting approximately 700,000 Americans [Il. A significant portion of the cerebral palsy population is communicative but non-verbal although the desire to communicate is present, speech is prohibited by damage to the part of the brain that controls the vocal tract. Most of these individuals do not have enough coordination for handwriting or typing. Al though primitive electronic communication aids exist, most are variations on picture boards, where the user points or looks at a twodimensional array of pictures to convey a thought such as “hungry” or “tired.” The Augmentative Communications Group has developed a speech synthesizer based on two-dimensional analog input. The creation of speech involves the coordination of a large number of muscles in the vocal tract. The synthesizer approximates this by receiving the position of the base and tip of the tongue as input signals and then synthesizes the sound produced by that position of the tongue. We have implemented a prototype @ 1990 ACM 089791410-4/90/0010/0123 $1.50 123 which synthesizes monotone speech from two analogue signals. The original research strategy was to attempt to design and build custom input devices for each user of the system. Biomedical engineers constructed various onedimensional potentiometers to be used in pairs to provide the analog inputs needed to drive the synthesizer. Building custom hardware interfaces is expensive, and cerebral palsy victims often have reduced strength, making control of any physical device cumbersome. As users fatigue, their efficiency with a particular device decreases and several different devices may be needed to accommodate various stages of fatigue. Our new approach is to create an individual gesture interface for each user. Our software maps body motions, reported by magnetic trackers, into continuous control signals for the speech synthesizer. The only physical effort by the user is to move a part of his body. This software tailoring allows us to create interfaces based on each user’s individual abilities, and makes it possible for those interfaces to adapt as the user fatigues. The idea of user tailoring, or customization, has long been understood as a crucial element in the design of traditional computer interfaces. Text editors allow users to rebind keyboards so that commonly used operations are easier to reach [ZI. Mice often allow for alterations in the ratio of device motion to cursor motion, to accommodate variations in user coordination. Some hardware interfaces allow minor customization! such as tilt-lock steering wheels in automobiles. Customization is necessary when there is a high degree of user idiosyncrasy, relative to the dexterity required for the task. While the need for individual tailoring is most apparent in the disabled community, able-bodied users exhibit high idiosyncrasy with respect to tasks where extreme dexterity is required, such as telemanipulation in microsurgery. We expect our techniques to be useful for able-bodied users when they must use interfaces that require high dexterity to complete tasks. Existing gesture research is dominated by a desire to understand or interpret gestures, and is commonly referred to as gesture recognition. Our approach is to map continuous data from one or more sensors to a set of continuous device control signals, rather than transforming gestures into symbols. Our primary goal is to create custom gesture mappings for device control. Our secondary goal is to make our mappings dynamically adjust for fatigue and changes in user ability. A Joystick-Driven Speech Synthesizer Because our synthesizer is such an unusual device, we first describe how it is able to synthesize speech from two analog inputs. Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature [3-51. The problem addressed here differs from previous work because we limit the number of articulator control parameters to those that can be provided by a human user in real-time. Articulator driven. synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech. A brief review of human physical speech production will be helpful in understanding the articulator driven synthesis approach. The physical process of speech production can be divided into three parts, First, air is forced through the vocal cords to produce either a voiced or unvoiced glottal excitation. Next, air flow is modified by a series of structures that constitute the vocal tract. Finally, the modified flow is radiated through the lips and nostrils. [61. The articulators used to produce speech are shown in Figure 1.","['University of Virginia, Thornton Hall, Charlottesville, VA#TAB#', 'University of Virginia, Thornton Hall, Charlottesville, VA#TAB#']","['2132052383', '2139439197']",2013714982.0,<Response [200]>,27.0,"{'DBLP': 'conf/uist/PauschW90', 'MAG': '2013714982', 'DOI': '10.1145/97924.97939', 'CorpusId': 18777067}",['Computer Science'],0.0,False,{'pages': '123-134'},8/1/1990,"['JournalArticle', 'Review']",42.0,Tailor: creating custom user interfaces based on gesture,https://www.semanticscholar.org/paper/adcca1cb7b104a36fcbe7dd2475f9b29f9111ece,UIST,1990
2013738217,"One major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology, such as a spoken language system. In recent years, the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures. In the present studies, over 2,600 utterances processed by a multimodal pen/voice system were collected during both mobile and stationary use. A new data collection infrastructure was developed, including instrumentation worn by the user while roaming, a researcher field station, and a multimodal data logger and analysis tool tailored for mobile research. Although speech recognition as a stand-alone failed more often during mobile system use, the results confirmed that a more stable multimodal architecture decreased this error rate by 1935%. Furthermore, these findings were replicated across different types of microphone technology. In large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture, with higher levels occurring in the noisy mobile environment. Implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner.",0.0,"One major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology, such as a spoken language system. In recent years, the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures. In the present studies, over 2,600 utterances processed by a multimodal pen/voice system were collected during both mobile and stationary use. A new data collection infrastructure was developed, including instrumentation worn by the user while roaming, a researcher field station, and a multimodal data logger and analysis tool tailored for mobile research. Although speech recognition as a stand-alone failed more often during mobile system use, the results confirmed that a more stable multimodal architecture decreased this error rate by 1935%. Furthermore, these findings were replicated across different types of microphone technology. In large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture, with higher levels occurring in the noisy mobile environment. Implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner.","['Department of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology, 20000 N.W. Walker Road, Beaverton, Oregon#TAB#']",['2669286322'],2013738217.0,"{'offset': 0, 'data': [{'authorId': '2807460', 'url': 'https://www.semanticscholar.org/author/2807460', 'name': 'S. Oviatt', 'affiliations': [], 'homepage': None, 'paperCount': 172, 'citationCount': 10365}]}",108.0,"{'DBLP': 'conf/uist/Oviatt00', 'MAG': '2013738217', 'DOI': '10.1145/354401.354408', 'CorpusId': 1050018}",['Computer Science'],4.0,False,{'pages': '21-30'},11/1/2000,['JournalArticle'],25.0,Multimodal system processing in mobile environments,https://www.semanticscholar.org/paper/3fd7101f82e83a94de5b2bc4e24736bc29a5a305,UIST,2000
2013742333,"IBIS (Intent-Based Illustration System) generates illustrations automatically, guided by communicative goals. Communicative goals specify that particular properties of objects, such as their color, size, or location are to be conveyed in the illustration. IBIS is intended to be part of an interactive multimedia explanation generation system. It has access to a knowledge base that contains a collection of objects, including information about their geometric properties, material, and location. As the goals are interpreted by a rule-based control component, the system generates a precise definition of the final illustration. If IBIS determines that a set of goals cannot be satisfied in a single picture, then it attempts to create a composite illustration that has multiple viewports. For example, a composite illustration may contain a nested inset illustration showing an object in greater detail than is possible in the parent picture. Each component illustration is defined by its placement, size, viewing specification, lighting specification, and list of objects to be displayed and their graphical style.",0.0,"IBIS (Intent-Based Illustration System) generates illustrations automatically, guided by communicative goals. Communicative goals specify that particular properties of objects, such as their color, size, or location are to be conveyed in the illustration. IBIS is intended to be part of an interactive multimedia explanation generation system. It has access to a knowledge base that contains a collection of objects, including information about their geometric properties, material, and location. As the goals are interpreted by a rule-based control component, the system generates a precise definition of the final illustration. If IBIS determines that a set of goals cannot be satisfied in a single picture, then it attempts to create a composite illustration that has multiple viewports. For example, a composite illustration may contain a nested inset illustration showing an object in greater detail than is possible in the parent picture. Each component illustration is defined by its placement, size, viewing specification, lighting specification, and list of objects to be displayed and their graphical style.","['Department of Computer Science Columbia University New York, New York', 'Department of Computer Science Columbia University New York, New York']","['2010893152', '252695556']",2013742333.0,"{'offset': 0, 'data': [{'authorId': '2034307', 'url': 'https://www.semanticscholar.org/author/2034307', 'name': 'D. Seligmann', 'affiliations': [], 'homepage': None, 'paperCount': 71, 'citationCount': 2543}, {'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}]}",26.0,"{'MAG': '2013742333', 'DBLP': 'conf/uist/SeligmannF89', 'DOI': '10.1145/73660.73661', 'CorpusId': 17685315}",['Computer Science'],0.0,False,{'pages': '1-9'},11/13/1989,['JournalArticle'],14.0,Specifying composite illustrations with communicative goals,https://www.semanticscholar.org/paper/d972ba590cd8ed600860c7cac308f065701fdcdb,UIST,1989
2016149317,"This paper describes a Computer Aided Design system for sketching free-form polygonal surfaces such as terrains and other natural objects. The user manipulates two 3D position and orientation trackers with three buttons, one for each hand. Each hand has a distinct role to play, with the dominant hand being responsible for picking and manipulation, and the less-dominant hand being responsible for context setting of various kinds. The less-dominant hand holds the workpiece, sets which refinement level that can be picked by the dominant hand, and generally acts as a counterpoint to the dominant hand. In this paper, the architecture of the system is outlined, and a simple surface is shown.",1.0,"This paper describes a Computer Aided Design system for sketching free-form polygonal surfaces such as terrains and other natural objects. The user manipulates two 3D position and orientation trackers with three buttons, one for each hand. Each hand has a distinct role to play, with the dominant hand being responsible for picking and manipulation, and the less-dominant hand being responsible for context setting of various kinds. The less-dominant hand holds the workpiece, sets which refinement level that can be picked by the dominant hand, and generally acts as a counterpoint to the dominant hand. In this paper, the architecture of the system is outlined, and a simple surface is shown.","['Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2H1, CANADA', 'Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2H1, CANADA']","['2146598526', '2570261396']",2016149317.0,"{'offset': 0, 'data': [{'authorId': '10696033', 'url': 'https://www.semanticscholar.org/author/10696033', 'name': 'Christopher D. Shaw', 'affiliations': [], 'homepage': None, 'paperCount': 26, 'citationCount': 1127}, {'authorId': '50153789', 'url': 'https://www.semanticscholar.org/author/50153789', 'name': 'Mark W. Green', 'affiliations': [], 'homepage': None, 'paperCount': 96, 'citationCount': 3544}]}",79.0,"{'MAG': '2016149317', 'DBLP': 'conf/uist/ShawG94', 'DOI': '10.1145/192426.197517', 'CorpusId': 2726539}",['Computer Science'],6.0,True,{'pages': '205-212'},11/2/1994,['JournalArticle'],27.0,Two-handed polygonal surface design,https://www.semanticscholar.org/paper/96d00669c3eb7b15f2933a32629cbcc6b5541855,UIST,1994
2016395521,"The first requirement of a ""spatial mouse"" is the ability to identify the object that it is aiming at. Among many possible technologies that can be employed for this purpose, possibly the best solution would be object recognition by machine vision. The problem, however, is that object recognition algorithms are not yet reliable enough or light enough for hand-held devices. This paper demonstrates that a simple object recognition algorithm can become a practical solution when augmented by interactivity. The user draw a circle around a target using a spatial mouse, and the mouse captures a series of camera frames. The frames can be easily stitched together to give a target image separated from the background, with which we need only additional steps of feature extraction and object classification. We present here results from two experiments with a few household objects.",0.0,"The first requirement of a ""spatial mouse"" is the ability to identify the object that it is aiming at. Among many possible technologies that can be employed for this purpose, possibly the best solution would be object recognition by machine vision. The problem, however, is that object recognition algorithms are not yet reliable enough or light enough for hand-held devices. This paper demonstrates that a simple object recognition algorithm can become a practical solution when augmented by interactivity. The user draw a circle around a target using a spatial mouse, and the mouse captures a series of camera frames. The frames can be easily stitched together to give a target image separated from the background, with which we need only additional steps of feature extraction and object classification. We present here results from two experiments with a few household objects.","['Information and Communications University, Yuseong-gu, Daejeon, Korea#TAB#', 'Information and Communications University, Gangnam-gu, Seoul, Korea']","['2168719295', '2335788814']",2016395521.0,"{'offset': 0, 'data': [{'authorId': '1903919', 'url': 'https://www.semanticscholar.org/author/1903919', 'name': 'Byungkon Sohn', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 17}, {'authorId': '1717371', 'url': 'https://www.semanticscholar.org/author/1717371', 'name': 'Geehyuk Lee', 'affiliations': [], 'homepage': None, 'paperCount': 160, 'citationCount': 1625}]}",1.0,"{'MAG': '2016395521', 'DBLP': 'conf/uist/SohnL05', 'DOI': '10.1145/1095034.1095051', 'CorpusId': 13099602}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['JournalArticle', 'Book', 'Conference']",7.0,Circle & identify: interactivity-augmented object recognition for handheld devices,https://www.semanticscholar.org/paper/6d890d2f34a762c6bd6501a5be072f740c3143c2,UIST,2005
2017525862,"We describe a tangible interface for building virtual structures using physical building blocks. We demonstrate two applications of our system. In one version, the blocks are used to construct geometric models of objects and structures for a popular game, Quake II™. In another version, buildings created with our blocks are rendered in different styles, using intelligent decoration of the building model.",1.0,"We describe a tangible interface for building virtual structures using physical building blocks. We demonstrate two applications of our system. In one version, the blocks are used to construct geometric models of objects and structures for a popular game, Quake II™. In another version, buildings created with our blocks are rendered in different styles, using intelligent decoration of the building model.","['MERL - A Mitsubishi Electric Research Laboratory, Cambridge, MA', 'Frankel and Associates, Inc., Lexington, MA and MERL-A Mitsubishi Electric Research Laboratory, Cambridge, MA#TAB#', 'University of Virginia Department of Computer Science, Charlottesville, VA', 'MERL - A Mitsubishi Electric Research Laboratory, Cambridge, MA', 'MERL - A Mitsubishi Electric Research Laboratory, Cambridge, MA', 'MERL - A Mitsubishi Electric Research Laboratory, Cambridge, MA', 'MERL - A Mitsubishi Electric Research Laboratory, Cambridge, MA']","['2041147275', '2116646964', '2132214060', '2167581412', '2205021947', '2226563455', '2642847987']",2017525862.0,"{'offset': 0, 'data': [{'authorId': '2116531776', 'url': 'https://www.semanticscholar.org/author/2116531776', 'name': 'David Anderson', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 371}, {'authorId': '2166957', 'url': 'https://www.semanticscholar.org/author/2166957', 'name': 'James L. Frankel', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 252}, {'authorId': '1822613', 'url': 'https://www.semanticscholar.org/author/1822613', 'name': 'J. Marks', 'affiliations': [], 'homepage': None, 'paperCount': 89, 'citationCount': 3565}, {'authorId': '144478480', 'url': 'https://www.semanticscholar.org/author/144478480', 'name': 'D. Leigh', 'affiliations': [], 'homepage': None, 'paperCount': 41, 'citationCount': 2928}, {'authorId': '49503526', 'url': 'https://www.semanticscholar.org/author/49503526', 'name': 'E. Sullivan', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 230}, {'authorId': '3198578', 'url': 'https://www.semanticscholar.org/author/3198578', 'name': 'J. Yedidia', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 6747}, {'authorId': '8905733', 'url': 'https://www.semanticscholar.org/author/8905733', 'name': 'K. Ryall', 'affiliations': [], 'homepage': None, 'paperCount': 63, 'citationCount': 3358}]}",52.0,"{'DBLP': 'conf/uist/AndersonFMLSYR99', 'MAG': '2017525862', 'DOI': '10.1145/320719.322587', 'CorpusId': 9110948}",['Computer Science'],3.0,False,{'pages': '71-72'},11/7/1999,['JournalArticle'],9.0,Building virtual structures with physical blocks,https://www.semanticscholar.org/paper/027802d9dff924e80880248ece8303815fcced1f,UIST,1999
2028085418,"Position control devices enable precise selection, but significant clutching degrades performance. Clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. Elastic rate control eliminates clutching completely, but can make precise selection difficult. We show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. Unlike previous work, our RubberEdge technique eliminates trajectory and velocity discontinuities. We derive predictive models for position control with clutching and hybrid control, and present a prototype RubberEdge position-rate control device including initial user feedback.",1.0,"Position control devices enable precise selection, but significant clutching degrades performance. Clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. Elastic rate control eliminates clutching completely, but can make precise selection difficult. We show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. Unlike previous work, our RubberEdge technique eliminates trajectory and velocity discontinuities. We derive predictive models for position control with clutching and hybrid control, and present a prototype RubberEdge position-rate control device including initial user feedback.","['LIFL & INRIA Futurs \\ University of Lille, Lille, France', 'University of Toronto, Toronto, Canada', 'LIFL & INRIA Futurs \\ University of Lille, Lille, France', 'LIFL & INRIA Futurs \\ University of Lille, Lille, France']","['2127275148', '2154794983', '2256161107', '80311574']",2028085418.0,"{'offset': 0, 'data': [{'authorId': '3051289', 'url': 'https://www.semanticscholar.org/author/3051289', 'name': 'Géry Casiez', 'affiliations': [], 'homepage': None, 'paperCount': 119, 'citationCount': 2460}, {'authorId': '145731603', 'url': 'https://www.semanticscholar.org/author/145731603', 'name': 'Daniel Vogel', 'affiliations': [], 'homepage': None, 'paperCount': 111, 'citationCount': 3712}, {'authorId': '2057914027', 'url': 'https://www.semanticscholar.org/author/2057914027', 'name': 'Qing Pan', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 70}, {'authorId': '1774410', 'url': 'https://www.semanticscholar.org/author/1774410', 'name': 'C. Chaillou', 'affiliations': [], 'homepage': None, 'paperCount': 85, 'citationCount': 930}]}",64.0,"{'DBLP': 'journals/corr/abs-0804-0556', 'MAG': '3106162525', 'ArXiv': '0804.0556', 'DOI': '10.1145/1294211.1294234', 'CorpusId': 6550875}",['Computer Science'],10.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['JournalArticle', 'Book', 'Conference']",28.0,RubberEdge: reducing clutching by combining position and rate control with elastic feedback,https://www.semanticscholar.org/paper/3b1c4a424780edbbca7aab6de89ee0c76b9ddf00,UIST,2007
2029615168,"We describe the design and prototype implementation of Scope, a system that generates graphical user interfaces for applications programmed in C++. The programmer chooses application data objects and functions that define the capabilities of the interface. At runtime, an interface design component, implemented as a set of production system rules, transforms this semantic specification into an interface built using a window system, an associated user interface toolkit, and the hardware input devices available on the system. The rules match application requirements against a semantic description of the toolkit, selecting virtual devices for input, output, and layout. Thus, Scope uses design rules to create interfaces from high-level programming semantics that are customized both for the application and the run-time environment.",1.0,"We describe the design and prototype implementation of Scope, a system that generates graphical user interfaces for applications programmed in C++. The programmer chooses application data objects and functions that define the capabilities of the interface. At runtime, an interface design component, implemented as a set of production system rules, transforms this semantic specification into an interface built using a window system, an associated user interface toolkit, and the hardware input devices available on the system. The rules match application requirements against a semantic description of the toolkit, selecting virtual devices for input, output, and layout. Thus, Scope uses design rules to create interfaces from high-level programming semantics that are customized both for the application and the run-time environment.","['Department of Computer Science, Columbia University New York, NY#TAB#', 'Department of Computer Science, Columbia University New York, NY#TAB#']","['1999148574', '252695556']",2029615168.0,"{'offset': 0, 'data': [{'authorId': '3141838', 'url': 'https://www.semanticscholar.org/author/3141838', 'name': 'Clifford Beshers', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 797}, {'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}]}",31.0,"{'DBLP': 'conf/uist/BeshersF89', 'MAG': '2029615168', 'DOI': '10.1145/73660.73670', 'CorpusId': 17630731}",['Computer Science'],1.0,False,{'pages': '76-85'},11/13/1989,['JournalArticle'],15.0,Scope: automated generation of graphical interfaces,https://www.semanticscholar.org/paper/b93bb0aa7ca65714261ae254e012b8f6360fa44a,UIST,1989
2030626427,"This paper describes ActiveText, a method for creating dynamic and interactive texts. ActiveText uses an object-based hierarchy to represent texts. This hierarchy makes it easy to work with the ASCII component and pixel component of the text at the same time. Static, dynamic and interactive properties of text can be easily intermixed and layered. The user can enter and edit text, adjust static and dynamic layout, apply dynamic and interactive behaviors, and adjust their parameters with a common set of tools and a common interface. Support for continuous editing allows the user to sketch dynamically. A prototype application called It's Alive! has been implemented to explore the ActiveText functionality. The documents produced by It's Alive! can be of use in a wide-range of areas, including chat-spaces, email, web-sites, fiction and poetry writing, and low-end film & video titling.",1.0,"This paper describes ActiveText, a method for creating dynamic and interactive texts. ActiveText uses an object-based hierarchy to represent texts. This hierarchy makes it easy to work with the ASCII component and pixel component of the text at the same time. Static, dynamic and interactive properties of text can be easily intermixed and layered. The user can enter and edit text, adjust static and dynamic layout, apply dynamic and interactive behaviors, and adjust their parameters with a common set of tools and a common interface. Support for continuous editing allows the user to sketch dynamically. A prototype application called It's Alive! has been implemented to explore the ActiveText functionality. The documents produced by It's Alive! can be of use in a wide-range of areas, including chat-spaces, email, web-sites, fiction and poetry writing, and low-end film & video titling.","['Interval Research Corporation, 1801 Page Mill Road, Building C, Palo Alto, CA', 'Interval Research Corporation, 1801 Page Mill Road, Building C, Palo Alto, CA']","['2103625110', '2223302605']",2030626427.0,"{'offset': 0, 'data': [{'authorId': '2109240966', 'url': 'https://www.semanticscholar.org/author/2109240966', 'name': 'Jason Lewis', 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 92}, {'authorId': '27986044', 'url': 'https://www.semanticscholar.org/author/27986044', 'name': 'Alex Weyers', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 22}]}",22.0,"{'DBLP': 'conf/uist/LewisW99', 'MAG': '2030626427', 'DOI': '10.1145/320719.322594', 'CorpusId': 12211556}",['Computer Science'],0.0,False,{'pages': '131-140'},11/7/1999,['JournalArticle'],32.0,ActiveText: a method for creating dynamic and interactive texts,https://www.semanticscholar.org/paper/1f124d09403317764f927ff197f87ab8ff916ffe,UIST,1999
2032273099,"Conventional windowing environments provide separate classes of objects for user interface components, or “widgets,” and graphical objects. Widgets negotiate layout and can be resized as rectangles, while graphics may be shared, transformed, transparent, and overlaid. This presents a major obstacle to applications like user interface builders and compound document editors where the manipulated objects need to behave both like graphics and widgets.Fresco[1] blends graphics and widgets into a single class of objects. We have an implementation of Fresco and an editor called Fdraw that allows graphical objects to be composed like widgets, and widgets to be transformed and shared like graphics. Performance measurements of Fdraw show that sharing reduces memory usage without slowing down redisplay.",1.0,"Conventional windowing environments provide separate classes of objects for user interface components, or “widgets,” and graphical objects. Widgets negotiate layout and can be resized as rectangles, while graphics may be shared, transformed, transparent, and overlaid. This presents a major obstacle to applications like user interface builders and compound document editors where the manipulated objects need to behave both like graphics and widgets.Fresco[1] blends graphics and widgets into a single class of objects. We have an implementation of Fresco and an editor called Fdraw that allows graphical objects to be composed like widgets, and widgets to be transformed and shared like graphics. Performance measurements of Fdraw show that sharing reduces memory usage without slowing down redisplay.","['Silicon Graphics', 'Fujitsu Ltd.']","['2153301466', '2497708644']",2032273099.0,"{'offset': 0, 'data': [{'authorId': '26592735', 'url': 'https://www.semanticscholar.org/author/26592735', 'name': 'Steven H. Tang', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 92}, {'authorId': '1801068', 'url': 'https://www.semanticscholar.org/author/1801068', 'name': 'M. Linton', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 1872}]}",13.0,"{'MAG': '2032273099', 'DBLP': 'conf/uist/TangL94', 'DOI': '10.1145/192426.192493', 'CorpusId': 6604734}",['Computer Science'],0.0,False,{'pages': '167-173'},11/2/1994,['JournalArticle'],17.0,Blending structured graphics and layout,https://www.semanticscholar.org/paper/98b74c77c70fed710e9e2f02878939a5da347316,UIST,1994
2043845455,"Video widgets are user-interface components rendered with video information. The implementation and several usage examples of a family of video widgets, called video actors,, are presented. Video actors rely on two capabilities of digital video: non-linear access, and the layering of video information. Non-linear access allows video frames to be displayed in mbitrary order without loss of continuity, layering allows two or more video streams to be spatially composed. Both capabilities are now becoming available to user-interface designers.",1.0,"Video widgets are user-interface components rendered with video information. The implementation and several usage examples of a family of video widgets, called video actors,, are presented. Video actors rely on two capabilities of digital video: non-linear access, and the layering of video information. Non-linear access allows video frames to be displayed in mbitrary order without loss of continuity, layering allows two or more video streams to be spatially composed. Both capabilities are now becoming available to user-interface designers.","['Centre Universitaire d’Informatique, Université de Genève, 24 rue Général Dufour, CH-1211 Genève 4, Switzerland', 'Centre Universitaire d’Informatique, Université de Genève, 24 rue Général Dufour, CH-1211 Genève 4, Switzerland', 'Centre Universitaire d’Informatique, Université de Genève, 24 rue Général Dufour, CH-1211 Genève 4, Switzerland', 'Centre Universitaire d’Informatique, Université de Genève, 24 rue Général Dufour, CH-1211 Genève 4, Switzerland']","['1973176221', '2100443602', '2275677278', '2308817654']",2043845455.0,"{'offset': 0, 'data': [{'authorId': '144282805', 'url': 'https://www.semanticscholar.org/author/144282805', 'name': 'S. Gibbs', 'affiliations': [], 'homepage': None, 'paperCount': 90, 'citationCount': 6909}, {'authorId': '1737188', 'url': 'https://www.semanticscholar.org/author/1737188', 'name': 'C. Breiteneder', 'affiliations': [], 'homepage': None, 'paperCount': 92, 'citationCount': 1382}, {'authorId': '1749600', 'url': 'https://www.semanticscholar.org/author/1749600', 'name': 'Vicki de Mey', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 166}, {'authorId': '2428574', 'url': 'https://www.semanticscholar.org/author/2428574', 'name': 'M. Papathomas', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 680}]}",35.0,"{'DBLP': 'conf/uist/GibbsBMP93', 'MAG': '2043845455', 'DOI': '10.1145/168642.168660', 'CorpusId': 8103166}",['Computer Science'],1.0,False,{'pages': '179-185'},12/1/1993,['JournalArticle'],16.0,Video widgets and video actors,https://www.semanticscholar.org/paper/3c0fcf5846c224c8a64b9fb95e58ad206155ca48,UIST,1993
2044706143,"GELO is a package that supports the interactive graphical display of software systems. Its features include built-in panning and zooming, abstraction of objects too small to see, pick correlation, windowing, and scroll bars. GELO creates a hierarchy of graphical objects that correspond to the components of the structure being displayed. Five flavors of graphical objects are supported, including those for simple structures, tiled layouts, and graph-based layouts. This framework is powerful enough to handle a wide variety of graphical visualizations, and it is general enough that new object flavors can be smoothly integrated in the future.GELO is easy to learn and to use, and is presently employed in two software development environments. Among its current applications are a variety of visual languages, an interactive display of call graphs, an interactive display of data structures, and a graphical representation of module dependencies.",0.0,"GELO is a package that supports the interactive graphical display of software systems. Its features include built-in panning and zooming, abstraction of objects too small to see, pick correlation, windowing, and scroll bars. GELO creates a hierarchy of graphical objects that correspond to the components of the structure being displayed. Five flavors of graphical objects are supported, including those for simple structures, tiled layouts, and graph-based layouts. This framework is powerful enough to handle a wide variety of graphical visualizations, and it is general enough that new object flavors can be smoothly integrated in the future.GELO is easy to learn and to use, and is presently employed in two software development environments. Among its current applications are a variety of visual languages, an interactive display of call graphs, an interactive display of data structures, and a graphical representation of module dependencies.","['Department of Computer Science, Brown University, Box 1910, Providence, RI', 'Department of Computer Science, Brown University, Box 1910, Providence, RI', 'Department of Computer Science, Brown University, Box 1910, Providence, RI']","['1964844633', '2101683298', '693769535']",2044706143.0,"{'offset': 0, 'data': [{'authorId': '1684347', 'url': 'https://www.semanticscholar.org/author/1684347', 'name': 'S. Reiss', 'affiliations': [], 'homepage': None, 'paperCount': 211, 'citationCount': 6838}, {'authorId': '1886739', 'url': 'https://www.semanticscholar.org/author/1886739', 'name': 'S. Meyers', 'affiliations': [], 'homepage': None, 'paperCount': 60, 'citationCount': 969}, {'authorId': '2267875', 'url': 'https://www.semanticscholar.org/author/2267875', 'name': 'Carolyn K. Duby', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 138}]}",32.0,"{'DBLP': 'conf/uist/ReissMD89', 'MAG': '2044706143', 'DOI': '10.1145/73660.73679', 'CorpusId': 14999177}",['Computer Science'],2.0,False,{'pages': '149-157'},11/13/1989,['JournalArticle'],21.0,Using GELO to visualize software systems,https://www.semanticscholar.org/paper/8b0542d0e931463bb44375ac8c1f27d08f959af9,UIST,1989
2050099917,"We present a novel display metaphor which extends traditional tabletop projections in collaborative environments by introducing freeform, environment-aware display representations and a matching set of interaction schemes. For that purpose, we map personalized widgets or ordinary computer applications that have been designed for a conventional, rectangular layout into space-efficient bubbles whose warping is performed with a potential-based physics approach. With a set of interaction operators based on laser pointer tracking, these freeform displays can be transformed and elastically deformed using focus and context visualization techniques. We also provide operations for intuitive instantiation of bubbles, cloning, cut & pasting, deletion and grouping in an interactive way, and we allow for user-drawn annotations and text entry using a projected keyboard. Additionally, an optional environment-aware adaptivity of the displays is achieved by imperceptible, realtime scanning of the projection geometry. Subsequently, collision-responses of the bubbles with non-optimal surface parts are computed in a rigid body simulation. The extraction of the projection surface properties runs concurrently with the main application of the system. Our approach is entirely based on off the-shelf, low-cost hardware including DLP-projectors and FireWire cameras.",1.0,"We present a novel display metaphor which extends traditional tabletop projections in collaborative environments by introducing freeform, environment-aware display representations and a matching set of interaction schemes. For that purpose, we map personalized widgets or ordinary computer applications that have been designed for a conventional, rectangular layout into space-efficient bubbles whose warping is performed with a potential-based physics approach. With a set of interaction operators based on laser pointer tracking, these freeform displays can be transformed and elastically deformed using focus and context visualization techniques. We also provide operations for intuitive instantiation of bubbles, cloning, cut & pasting, deletion and grouping in an interactive way, and we allow for user-drawn annotations and text entry using a projected keyboard. Additionally, an optional environment-aware adaptivity of the displays is achieved by imperceptible, realtime scanning of the projection geometry. Subsequently, collision-responses of the bubbles with non-optimal surface parts are computed in a rigid body simulation. The extraction of the projection surface properties runs concurrently with the main application of the system. Our approach is entirely based on off the-shelf, low-cost hardware including DLP-projectors and FireWire cameras.","['Computer Graphics Laboratory, ETH Zurich#TAB#', 'Computer Graphics Laboratory, ETH Zurich#TAB#']","['1626267555', '2289975239']",2050099917.0,"{'offset': 0, 'data': [{'authorId': '2225147', 'url': 'https://www.semanticscholar.org/author/2225147', 'name': 'D. Cotting', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 503}, {'authorId': '144877478', 'url': 'https://www.semanticscholar.org/author/144877478', 'name': 'M. Gross', 'affiliations': [], 'homepage': None, 'paperCount': 500, 'citationCount': 29573}]}",55.0,"{'DBLP': 'conf/uist/CottingG06', 'MAG': '2050099917', 'DOI': '10.1145/1166253.1166291', 'CorpusId': 15300602}",['Computer Science'],7.0,False,{'pages': '245-254'},10/15/2006,"['JournalArticle', 'Conference']",60.0,Interactive environment-aware display bubbles,https://www.semanticscholar.org/paper/ccfe59d3f8310b76aace66b4d9765e10b3021e97,UIST,2006
2052194215,"Today’s electronic desktop is quite separate from the physical desk of the user. Electronic documents lack many useful properties of paper, and paper lacks useful properties of electronic documents. Instead of making the electronic desktop more like the physical desk, this work attempts the opposite: to give the physical desk electronic properties and merge the two desktops into one. This paper describes a desk with a computer-controlled camera and projector above it. The camera sees where the user is pointing, rmd it reads portions of documents that are placed on the desk. The projector displays feedback and electronic objects onto the desk surface. This DigitalDesk adds electronic features to physical paper, and it adds physical features to electronic documents. The system allows the user to interact with paper and electronic objects by touching them with a bare finger (digit). Instead of “direct” manipulation with a mouse, this is tangible manipulation with a finger. The DigitalDesk Calculator is a prototype example of a simple application that can benefit from the interaction techniques enabled by this desktop. The paper begins by discussing the motivation behind this work, then describes the Digi talDesk, tangible manipulation, and the calculator prototype. It then discusses implementation details and ends with ideas for the future of tangible manipulation.",1.0,"Today’s electronic desktop is quite separate from the physical desk of the user. Electronic documents lack many useful properties of paper, and paper lacks useful properties of electronic documents. Instead of making the electronic desktop more like the physical desk, this work attempts the opposite: to give the physical desk electronic properties and merge the two desktops into one. This paper describes a desk with a computer-controlled camera and projector above it. The camera sees where the user is pointing, rmd it reads portions of documents that are placed on the desk. The projector displays feedback and electronic objects onto the desk surface. This DigitalDesk adds electronic features to physical paper, and it adds physical features to electronic documents. The system allows the user to interact with paper and electronic objects by touching them with a bare finger (digit). Instead of “direct” manipulation with a mouse, this is tangible manipulation with a finger. The DigitalDesk Calculator is a prototype example of a simple application that can benefit from the interaction techniques enabled by this desktop. The paper begins by discussing the motivation behind this work, then describes the Digi talDesk, tangible manipulation, and the calculator prototype. It then discusses implementation details and ends with ideas for the future of tangible manipulation.","['University of Cambridge Computer Laboratory and Rank Xerox EuroPARC, 61 Regent Street, Cambridge CB2 1AB United Kingdom']",['2047120366'],2052194215.0,"{'offset': 0, 'data': [{'authorId': '2751729', 'url': 'https://www.semanticscholar.org/author/2751729', 'name': 'P. Wellner', 'affiliations': [], 'homepage': None, 'paperCount': 34, 'citationCount': 4219}]}",324.0,"{'DBLP': 'conf/uist/Wellner91', 'MAG': '2052194215', 'DOI': '10.1145/120782.120785', 'CorpusId': 7836763}",['Computer Science'],20.0,False,{'pages': '27-33'},11/11/1991,['JournalArticle'],10.0,The DigitalDesk calculator: tangible manipulation on a desk top display,https://www.semanticscholar.org/paper/a5c43fdd1819cce2aa826e3d976e820503a793f4,UIST,1991
2053303771,"NP-hard combinatorial optimization problems are common in real life. Due to their intractability, local search algorithms are often used to solve such problems. Since these algorithms are heuristic-based, it is hard to understand how to improve or tune them. We propose an interactive visualization tool, VIZ, meant for understanding the behavior of local search. VIZ uses animation of abstract search trajectories with other visualizations which are also animated in a VCR-like fashion to graphically playback the algorithm behavior. It combines generic visualizations applicable on arbitrary algorithms with algorithm and problem specific visualizations. We use a variety of techniques such as alpha blending to reduce visual clutter and to smooth animation, highlights and shading, automatically generated index points for playback, and visual comparison of two algorithms. The use of multiple viewpoints can be an effective way of understanding search behavior and highlight algorithm behavior which might otherwise be hidden.",0.0,"NP-hard combinatorial optimization problems are common in real life. Due to their intractability, local search algorithms are often used to solve such problems. Since these algorithms are heuristic-based, it is hard to understand how to improve or tune them. We propose an interactive visualization tool, VIZ, meant for understanding the behavior of local search. VIZ uses animation of abstract search trajectories with other visualizations which are also animated in a VCR-like fashion to graphically playback the algorithm behavior. It combines generic visualizations applicable on arbitrary algorithms with algorithm and problem specific visualizations. We use a variety of techniques such as alpha blending to reduce visual clutter and to smooth animation, highlights and shading, automatically generated index points for playback, and visual comparison of two algorithms. The use of multiple viewpoints can be an effective way of understanding search behavior and highlight algorithm behavior which might otherwise be hidden.","['National University of Singapore, ', 'Singapore Management University', 'National University of Singapore, ']","['2171866233', '2218696388', '2249085170']",2053303771.0,"{'offset': 0, 'data': [{'authorId': '144294817', 'url': 'https://www.semanticscholar.org/author/144294817', 'name': 'Steven Halim', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 191}, {'authorId': '1713932', 'url': 'https://www.semanticscholar.org/author/1713932', 'name': 'R. Yap', 'affiliations': [], 'homepage': None, 'paperCount': 184, 'citationCount': 3230}, {'authorId': '1725253', 'url': 'https://www.semanticscholar.org/author/1725253', 'name': 'H. Lau', 'affiliations': [], 'homepage': None, 'paperCount': 283, 'citationCount': 3524}]}",22.0,"{'DBLP': 'conf/uist/HalimYL06', 'MAG': '2053303771', 'DOI': '10.1145/1166253.1166264', 'CorpusId': 2610610}",['Computer Science'],0.0,False,{'pages': '57-66'},10/15/2006,"['JournalArticle', 'Conference']",24.0,Viz: a visual analysis suite for explaining local search behavior,https://www.semanticscholar.org/paper/55b2b209ab4b4baf4dcabc1dd3c0fe4b60b86f90,UIST,2006
2054099078,"Overview visualizations for small-screen web browsers were designed to provide users with visual context and to allow them to rapidly zoom in on tiles of relevant content. Given that content in the overview is reduced, however, users are often unable to tell which tiles hold the relevant material, which can force them to adopt a time-consuming hunt-and-peck strategy. Collapse-to-zoom addresses this issue by offering an alternative exploration strategy. In addition to allowing users to zoom into relevant areas, collapse-to-zoom allows users to collapse areas deemed irrelevant, such as columns containing menus, archive material, or advertising. Collapsing content causes all remaining content to expand in size causing it to reveal more detail, which increases the user's chance of identifying relevant content. Collapse-to-zoom navigation is based on a hybrid between a marquee selection tool and a marking menu, called marquee menu. It offers four commands for collapsing content areas at different granularities and to switch to a full-size reading view of what is left of the page.",1.0,"Overview visualizations for small-screen web browsers were designed to provide users with visual context and to allow them to rapidly zoom in on tiles of relevant content. Given that content in the overview is reduced, however, users are often unable to tell which tiles hold the relevant material, which can force them to adopt a time-consuming hunt-and-peck strategy. Collapse-to-zoom addresses this issue by offering an alternative exploration strategy. In addition to allowing users to zoom into relevant areas, collapse-to-zoom allows users to collapse areas deemed irrelevant, such as columns containing menus, archive material, or advertising. Collapsing content causes all remaining content to expand in size causing it to reveal more detail, which increases the user's chance of identifying relevant content. Collapse-to-zoom navigation is based on a hybrid between a marquee selection tool and a marking menu, called marquee menu. It offers four commands for collapsing content areas at different granularities and to switch to a full-size reading view of what is left of the page.","['[Microsoft research, Redmond, WA]', 'Microsoft Research Asia, Beijing, China.', 'Tsinghua University, Beijing China#TAB#', 'Microsoft Research Asia, Beijing, China.']","['2009751849', '3190884006', '3213035388', '3214595345']",2054099078.0,"{'offset': 0, 'data': [{'authorId': '1729393', 'url': 'https://www.semanticscholar.org/author/1729393', 'name': 'Patrick Baudisch', 'affiliations': [], 'homepage': None, 'paperCount': 220, 'citationCount': 12226}, {'authorId': '144076239', 'url': 'https://www.semanticscholar.org/author/144076239', 'name': 'Xing Xie', 'affiliations': [], 'homepage': None, 'paperCount': 343, 'citationCount': 28821}, {'authorId': '2108882425', 'url': 'https://www.semanticscholar.org/author/2108882425', 'name': 'Chong Wang', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 307}, {'authorId': '1712167', 'url': 'https://www.semanticscholar.org/author/1712167', 'name': 'Wei-Ying Ma', 'affiliations': [], 'homepage': None, 'paperCount': 384, 'citationCount': 36672}]}",124.0,"{'DBLP': 'conf/uist/BaudischX0M04', 'MAG': '2054099078', 'DOI': '10.1145/1029632.1029647', 'CorpusId': 3980537}",['Computer Science'],8.0,False,{'pages': '91-94'},10/24/2004,"['JournalArticle', 'Review']",20.0,Collapse-to-zoom: viewing web pages on small screen devices by interactively removing irrelevant content,https://www.semanticscholar.org/paper/3a8730e5bd7ee30fa616861f16ba3e25bb9604fb,UIST,2004
2055465369,"We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called LEAN that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called twist-lens, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen's pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.",1.0,"We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called LEAN that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called twist-lens, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen's pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.","['Dept. of Computer Sci. University of Toronto', 'Dept. of Computer Sci. University of Toronto']","['2130130894', '2137512077']",2055465369.0,"{'offset': 0, 'data': [{'authorId': '2057749310', 'url': 'https://www.semanticscholar.org/author/2057749310', 'name': 'Gonzalo A. Ramos', 'affiliations': [], 'homepage': None, 'paperCount': 42, 'citationCount': 2667}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",158.0,"{'DBLP': 'conf/uist/RamosB03', 'MAG': '2055465369', 'DOI': '10.1145/964696.964708', 'CorpusId': 2556563}",['Computer Science'],16.0,True,{'pages': '105-114'},11/2/2003,['JournalArticle'],25.0,Fluid interaction techniques for the control and annotation of digital video,https://www.semanticscholar.org/paper/2781e132de0a3ef2a25ad84d83523230b141b14c,UIST,2003
2056266358,"Over the past few years a lot of different information visualization techniques have been proposed. Being a relatively new and large field, the spectrum of emerging techniques has not clearly been identified. Another major consequence of the youthfulness of the field is that very few evaluation have been conducted so far. The aim of the panel will be to address these two points. First, panelist will characterize the spectrum of information visualization technology depending on tasks, users or data. Panelists will further discuss future trends in visualization technology by determining which are the most important features or challenges that information visualization systems should address. Second, the discussion will focus on how these systems m to be evaluated through controlled experiments, system evaluation, long-time studies, verbal protocols, theoretical evaluations, or else? INTRODUCTION Most information visualization systems that have been proposed over the past few years have been designed for very specific tasks, users or data structures. Characterizing the spectrum of information visualization technology often ends up in a long catalog of diffenmts ystem. Nevertheless, the field is now mature enough to extract from all these systems some of the main characteristics they share. The aim of the panel will be to determine the most important features of these systems and to discuss the future trends and challenges that they will have to face. Permission to make digital/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies a~e not made or distributed for profit or commercial advantage, the copyngbt notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission andlor fee. UIST ’96 Seattle Washington USA @ 1996 ACM 0-89791-798-7/96/11 ..$3.50 Catherine Pk2isant A.V. Williams Building, University of Maryland College Park, MD 20742, U.S.A. Tel: +1 (301) 405-2768, Email: plaisant@cs.umd.edu Matthew Chalmers UBILAB",0.0,"Over the past few years a lot of different information visualization techniques have been proposed. Being a relatively new and large field, the spectrum of emerging techniques has not clearly been identified. Another major consequence of the youthfulness of the field is that very few evaluation have been conducted so far. The aim of the panel will be to address these two points. First, panelist will characterize the spectrum of information visualization technology depending on tasks, users or data. Panelists will further discuss future trends in visualization technology by determining which are the most important features or challenges that information visualization systems should address. Second, the discussion will focus on how these systems m to be evaluated through controlled experiments, system evaluation, long-time studies, verbal protocols, theoretical evaluations, or else? INTRODUCTION Most information visualization systems that have been proposed over the past few years have been designed for very specific tasks, users or data structures. Characterizing the spectrum of information visualization technology often ends up in a long catalog of diffenmts ystem. Nevertheless, the field is now mature enough to extract from all these systems some of the main characteristics they share. The aim of the panel will be to determine the most important features of these systems and to discuss the future trends and challenges that they will have to face. Permission to make digital/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies a~e not made or distributed for profit or commercial advantage, the copyngbt notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission andlor fee. UIST ’96 Seattle Washington USA @ 1996 ACM 0-89791-798-7/96/11 ..$3.50 Catherine Pk2isant A.V. Williams Building, University of Maryland College Park, MD 20742, U.S.A. Tel: +1 (301) 405-2768, Email: plaisant@cs.umd.edu Matthew Chalmers UBILAB","['UBILAB', 'A.V. Williams Building, University of Maryland, College Park, MD', 'Xerox, Parc', 'LRI - Bât 490- Université Paris-Sud, 91 405 ORSAY Cedex - France#TAB#', 'IVEE - Chalmers University', 'PITTSBURGH UNIV.']","['2141153954', '216611935', '2171930786', '2430258909', '2974677205', '3103156495']",2056266358.0,"{'offset': 0, 'data': [{'authorId': '1813793', 'url': 'https://www.semanticscholar.org/author/1813793', 'name': 'Mountaz Hascoët', 'affiliations': [], 'homepage': None, 'paperCount': 39, 'citationCount': 451}, {'authorId': '145421667', 'url': 'https://www.semanticscholar.org/author/145421667', 'name': 'C. Ahlberg', 'affiliations': [], 'homepage': None, 'paperCount': 26, 'citationCount': 2548}, {'authorId': '3014360', 'url': 'https://www.semanticscholar.org/author/3014360', 'name': 'R. Korfhage', 'affiliations': [], 'homepage': None, 'paperCount': 115, 'citationCount': 1360}, {'authorId': '1764846', 'url': 'https://www.semanticscholar.org/author/1764846', 'name': 'C. Plaisant', 'affiliations': [], 'homepage': None, 'paperCount': 305, 'citationCount': 20919}, {'authorId': '144175654', 'url': 'https://www.semanticscholar.org/author/144175654', 'name': 'M. Chalmers', 'affiliations': [], 'homepage': None, 'paperCount': 150, 'citationCount': 6324}, {'authorId': '143857644', 'url': 'https://www.semanticscholar.org/author/143857644', 'name': 'R. Rao', 'affiliations': [], 'homepage': None, 'paperCount': 35, 'citationCount': 4892}]}",8.0,"{'DBLP': 'conf/uist/Hascoet-ZiziAKPCR96', 'MAG': '2056266358', 'DOI': '10.1145/237091.237101', 'CorpusId': 14652594}",['Computer Science'],0.0,False,{'pages': '75-77'},11/1/1996,['JournalArticle'],0.0,Where is information visualization technology going?,https://www.semanticscholar.org/paper/26ad346bee90a26669adfab867534fdc9d3d7083,UIST,1996
2057256643,"In this paper, we discuss a re-configurable pipeline architecture that is ideally suited for applications in which a user is interactively managing a stream of data. Currently, document service buses allow stand-alone document services (translation, printing, etc.) to be combined for batch processing. Our architecture allows services to be composed and re-configured on the fly in order to support interactive applications. To motivate the need for such an architecture we address the problem of finding and organizing images on the World Wide Web. The resulting tool, PicturePiper, provides a mechanism for allowing users access to images on the web related to a topic of interest.",1.0,"In this paper, we discuss a re-configurable pipeline architecture that is ideally suited for applications in which a user is interactively managing a stream of data. Currently, document service buses allow stand-alone document services (translation, printing, etc.) to be combined for batch processing. Our architecture allows services to be composed and re-configured on the fly in order to support interactive applications. To motivate the need for such an architecture we address the problem of finding and organizing images on the World Wide Web. The resulting tool, PicturePiper, provides a mechanism for allowing users access to images on the web related to a topic of interest.","['Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA#TAB#', 'Carnegie Mellon University (5000 Forbes Avenue, Pittsburgh, PA)', 'Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA#TAB#']","['1991760033', '2071123256', '2222047569']",2057256643.0,"{'offset': 0, 'data': [{'authorId': '26640936', 'url': 'https://www.semanticscholar.org/author/26640936', 'name': 'Adam M. Fass', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 261}, {'authorId': '1734223', 'url': 'https://www.semanticscholar.org/author/1734223', 'name': 'E. Bier', 'affiliations': [], 'homepage': None, 'paperCount': 64, 'citationCount': 3739}, {'authorId': '2630700', 'url': 'https://www.semanticscholar.org/author/2630700', 'name': 'Eytan Adar', 'affiliations': [], 'homepage': None, 'paperCount': 143, 'citationCount': 12129}]}",10.0,"{'DBLP': 'conf/uist/FassBA00', 'MAG': '2057256643', 'DOI': '10.1145/354401.354411', 'CorpusId': 2107036}",['Computer Science'],1.0,False,{'pages': '51-62'},11/1/2000,['JournalArticle'],24.0,PicturePiper: using a re-configurable pipeline to find images on the Web,https://www.semanticscholar.org/paper/835b8d5300598a29c6ee0a66b0720c5e395e689d,UIST,2000
2057533192,"We have developed an interface for editing and simulating Coloured Petri Nets based on toolglasses, marking menus and bi-manual interaction, in order to understand how novel interaction techniques could be supported by a new generation of user interface toolkits. The architecture of CPN2000 is based on three components: the Document Structure stores all the persistent data in the system; the Display Structure represents the contents of the screen and implements rendering and hit detection algorithms; and the Input Structure uses ""instruments"" to manage interaction. The rendering engine is based on OpenGL and a number of techniques have been developed to take advantage of 3D accelerated graphics for a 2D application. Performance data show that high frame rates have been achieved with off-theshelf hardware even with a non-optimized redisplay. This work paves the way towards a post-WIMP UI toolkit.",1.0,"We have developed an interface for editing and simulating Coloured Petri Nets based on toolglasses, marking menus and bi-manual interaction, in order to understand how novel interaction techniques could be supported by a new generation of user interface toolkits. The architecture of CPN2000 is based on three components: the Document Structure stores all the persistent data in the system; the Display Structure represents the contents of the screen and implements rendering and hit detection algorithms; and the Input Structure uses ""instruments"" to manage interaction. The rendering engine is based on OpenGL and a number of techniques have been developed to take advantage of 3D accelerated graphics for a 2D application. Performance data show that high frame rates have been achieved with off-theshelf hardware even with a non-optimized redisplay. This work paves the way towards a post-WIMP UI toolkit.","['Department of Computer Science, University of Aarhus, IT-Parken, Aabogade 34, 8200 Aarhus N, Denmark', 'LRI, Båt 490, Université Paris-Sud, 91405 Orsay Cedex, France and Department of Computer Science, University of Aarhus, IT-Parken, Aabogade 34, 8200 Aarhus N, Denmark#TAB#']","['2253974111', '2674390412']",2057533192.0,"{'offset': 0, 'data': [{'authorId': '1401678555', 'url': 'https://www.semanticscholar.org/author/1401678555', 'name': 'M. Beaudouin-Lafon', 'affiliations': [], 'homepage': None, 'paperCount': 177, 'citationCount': 7255}, {'authorId': '2553425', 'url': 'https://www.semanticscholar.org/author/2553425', 'name': 'Henry Michael Lassen', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 512}]}",66.0,"{'MAG': '2057533192', 'DBLP': 'conf/uist/Beaudouin-LafonL00', 'DOI': '10.1145/354401.354761', 'CorpusId': 14972218}",['Computer Science'],0.0,False,{'pages': '181-190'},11/1/2000,['JournalArticle'],34.0,"The architecture and implementation of CPN2000, a post-WIMP graphical application",https://www.semanticscholar.org/paper/dc8e6eaeb70402175c3030a19e4b3095112168b9,UIST,2000
2060353874,"The increasing mass of information confronting a business or an individual have created a demand for information management applications. Time-based information, in particular, is an important part of many information access tasks. This paper explores how to use 3D graphics and interactive animation to design and implement visualizers that improve access to large masses of time-based information. Two new visualizers have been developed for the Information Visualizer: 1) the Spiral Calendar was designed for rapid access to an individual's daily schedule, and 2) the Time Lattice was designed for analyzing the time relationships among the schedules of groups of people. The Spiral Calendar embodies a new 3D graphics technique for integrating detail and context by placing objects in a 3D spiral. It demonstrates that advanced graphics techniques can enhance routine office information tasks. The Time Lattice is formed by aligning a collection of 2D calendars. 2D translucent shadows provide views and interactive access to the resulting complex 3D object. The paper focuses on how these visualizations were developed. The Spiral Calendar, in particular, has gone through an entire cycle of development, including design, implementation, evaluation, revision and reuse. Our experience should prove useful to others developing user interfaces based on advanced graphics.",1.0,"The increasing mass of information confronting a business or an individual have created a demand for information management applications. Time-based information, in particular, is an important part of many information access tasks. This paper explores how to use 3D graphics and interactive animation to design and implement visualizers that improve access to large masses of time-based information. Two new visualizers have been developed for the Information Visualizer: 1) the Spiral Calendar was designed for rapid access to an individual's daily schedule, and 2) the Time Lattice was designed for analyzing the time relationships among the schedules of groups of people. The Spiral Calendar embodies a new 3D graphics technique for integrating detail and context by placing objects in a 3D spiral. It demonstrates that advanced graphics techniques can enhance routine office information tasks. The Time Lattice is formed by aligning a collection of 2D calendars. 2D translucent shadows provide views and interactive access to the resulting complex 3D object. The paper focuses on how these visualizations were developed. The Spiral Calendar, in particular, has gone through an entire cycle of development, including design, implementation, evaluation, revision and reuse. Our experience should prove useful to others developing user interfaces based on advanced graphics.","['Xerox Palo Alto Research Center, 3333 Coyote Hill Rd', 'Xerox Palo Alto Research Center, 3333 Coyote Hill Rd', ' Palo Alto, CA']","['2044770413', '2163972128', '2172094290']",2060353874.0,"{'offset': 0, 'data': [{'authorId': '1777393', 'url': 'https://www.semanticscholar.org/author/1777393', 'name': 'J. Mackinlay', 'affiliations': [], 'homepage': None, 'paperCount': 174, 'citationCount': 17359}, {'authorId': '1699184', 'url': 'https://www.semanticscholar.org/author/1699184', 'name': 'G. Robertson', 'affiliations': [], 'homepage': None, 'paperCount': 126, 'citationCount': 14059}, {'authorId': '1710751', 'url': 'https://www.semanticscholar.org/author/1710751', 'name': 'R. DeLine', 'affiliations': [], 'homepage': None, 'paperCount': 101, 'citationCount': 8682}]}",101.0,"{'DBLP': 'conf/uist/MackinlayRD94', 'MAG': '2060353874', 'DOI': '10.1145/192426.192470', 'CorpusId': 18223160}",['Computer Science'],1.0,False,{'pages': '109-118'},11/2/1994,['JournalArticle'],22.0,Developing calendar visualizers for the information visualizer,https://www.semanticscholar.org/paper/88410bca4a85ed12c791b2751812606c94a20bd5,UIST,1994
2060502770,"Thumbnail images provide users of image retrieval and browsing systems with a method for quickly scanning large numbers of images. Recognizing the objects in an image is important in many retrieval tasks, but thumbnails generated by shrinking the original image often render objects illegible. We study the ability of computer vision systems to detect key components of images so that automated cropping, prior to shrinking, can render objects more recognizable. We evaluate automatic cropping techniques 1) based on a general method that detects salient portions of images, and 2) based on automatic face detection. Our user study shows that these methods result in small thumbnails that are substantially more recognizable and easier to find in the context of visual search.",1.0,"Thumbnail images provide users of image retrieval and browsing systems with a method for quickly scanning large numbers of images. Recognizing the objects in an image is important in many retrieval tasks, but thumbnails generated by shrinking the original image often render objects illegible. We study the ability of computer vision systems to detect key components of images so that automated cropping, prior to shrinking, can render objects more recognizable. We evaluate automatic cropping techniques 1) based on a general method that detects salient portions of images, and 2) based on automatic face detection. Our user study shows that these methods result in small thumbnails that are substantially more recognizable and easier to find in the context of visual search.","['Human-Computer Interaction Laboratory, University of Maryland, College Park, MD#TAB#', 'Department of Computer Science , University of Maryland, College Park, MD,#TAB#', 'Department of Computer Science , University of Maryland, College Park, MD,#TAB#', 'Human-Computer Interaction Laboratory, University of Maryland, College Park, MD#TAB#']","['2046653803', '2138352179', '2276238216', '2497313926']",2060502770.0,"{'offset': 0, 'data': [{'authorId': '3994427', 'url': 'https://www.semanticscholar.org/author/3994427', 'name': 'B. Suh', 'affiliations': [], 'homepage': None, 'paperCount': 86, 'citationCount': 7173}, {'authorId': '1805398', 'url': 'https://www.semanticscholar.org/author/1805398', 'name': 'Haibin Ling', 'affiliations': [], 'homepage': None, 'paperCount': 363, 'citationCount': 20271}, {'authorId': '1799187', 'url': 'https://www.semanticscholar.org/author/1799187', 'name': 'B. Bederson', 'affiliations': [], 'homepage': None, 'paperCount': 350, 'citationCount': 19703}, {'authorId': '34734622', 'url': 'https://www.semanticscholar.org/author/34734622', 'name': 'D. Jacobs', 'affiliations': [], 'homepage': None, 'paperCount': 183, 'citationCount': 14494}]}",447.0,"{'MAG': '2060502770', 'DBLP': 'conf/uist/SuhLBJ03', 'DOI': '10.1145/964696.964707', 'CorpusId': 684692}",['Computer Science'],20.0,True,{'pages': '95-104'},11/2/2003,['JournalArticle'],24.0,Automatic thumbnail cropping and its effectiveness,https://www.semanticscholar.org/paper/df94ebff0290bfcb4bed05e2ab1693d42fdec97d,UIST,2003
2063812368,"All of these systems rely on an explicit model of history, which can be scanned to support search or “navigation” over a timeline, and all allow their timelines to be “traversed” to move the application’s state to other points in its history. However, as powerful as these applications are, their timeline representations are for the most part exceedingly simple. They typically support only linear, not branching timelines (GINA and Timewarp are exceptions, however); the “nodes” in a timeline must represent atomic operations with side effects that are well understood at the time the application is created; and, typically, the timeline of the entire application must be navigated or traversed as a whole—it is impossible to have a portion of the timeline exist in a “bubble” that can be manipulated separately.",1.0,"All of these systems rely on an explicit model of history, which can be scanned to support search or “navigation” over a timeline, and all allow their timelines to be “traversed” to move the application’s state to other points in its history. However, as powerful as these applications are, their timeline representations are for the most part exceedingly simple. They typically support only linear, not branching timelines (GINA and Timewarp are exceptions, however); the “nodes” in a timeline must represent atomic operations with side effects that are well understood at the time the application is created; and, typically, the timeline of the entire application must be navigated or traversed as a whole—it is impossible to have a portion of the timeline exist in a “bubble” that can be manipulated separately.","['College of Computing, Georgia Tech., Atlanta GA#TAB#', 'Yahoo, Inc., 3420 Central Expwy, Santa Clara, CA', 'Brown University, CS Dept., Box 1910, Providence, RI', 'Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA#TAB#']","['18171182', '2091518520', '2152110089', '2166870691']",2063812368.0,"{'offset': 0, 'data': [{'authorId': '144473638', 'url': 'https://www.semanticscholar.org/author/144473638', 'name': 'W. K. Edwards', 'affiliations': [], 'homepage': None, 'paperCount': 124, 'citationCount': 8239}, {'authorId': '1717356', 'url': 'https://www.semanticscholar.org/author/1717356', 'name': 'T. Igarashi', 'affiliations': [], 'homepage': None, 'paperCount': 465, 'citationCount': 11915}, {'authorId': '144230355', 'url': 'https://www.semanticscholar.org/author/144230355', 'name': 'A. LaMarca', 'affiliations': [], 'homepage': None, 'paperCount': 111, 'citationCount': 11559}, {'authorId': '1752751', 'url': 'https://www.semanticscholar.org/author/1752751', 'name': 'Elizabeth D. Mynatt', 'affiliations': [], 'homepage': None, 'paperCount': 262, 'citationCount': 13114}]}",69.0,"{'DBLP': 'conf/uist/EdwardsILM00', 'MAG': '2063812368', 'DOI': '10.1145/354401.354409', 'CorpusId': 256425}",['Computer Science'],4.0,False,{'pages': '31-40'},11/1/2000,['JournalArticle'],18.0,A temporal model for multi-level undo and redo,https://www.semanticscholar.org/paper/38a9e1b30d0a6a697cada7f596f02ddd6e62d003,UIST,2000
2090048052,"The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.",1.0,"The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.","[' Univ. of Washington, Seattle, WA, USA', 'University of Rochester, Rochester, NY USA', 'University of Maryland, College Park,MD,USA.', 'Massachusettes Institute of Technology, Cambridge, USA#TAB#', 'University of Rochester, Rochester, NY USA', 'University of Central Florida, Orlando, FL, USA; ', 'Massachusettes Institute of Technology,, CAMBRIDGE, MA, USA#TAB#', 'Massachusettes Institute of Technology, Cambridge, USA#TAB#', 'University of Rochester, Rochester, NY USA', 'University of Maryland, College Park,MD,USA.', 'University of Rochester, Rochester, NY USA']","['1237292554', '2103437488', '2104162850', '2104582966', '2112106364', '2118081206', '2156768364', '2232102094', '2232493242', '2339819898', '2485489804']",2090048052.0,"{'offset': 0, 'data': [{'authorId': '1744846', 'url': 'https://www.semanticscholar.org/author/1744846', 'name': 'Jeffrey P. Bigham', 'affiliations': [], 'homepage': None, 'paperCount': 282, 'citationCount': 9073}, {'authorId': '1712587', 'url': 'https://www.semanticscholar.org/author/1712587', 'name': 'C. Jayant', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 1614}, {'authorId': '1737220', 'url': 'https://www.semanticscholar.org/author/1737220', 'name': 'H. Ji', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 976}, {'authorId': '48155668', 'url': 'https://www.semanticscholar.org/author/48155668', 'name': 'Greg Little', 'affiliations': [], 'homepage': None, 'paperCount': 37, 'citationCount': 3712}, {'authorId': '144360239', 'url': 'https://www.semanticscholar.org/author/144360239', 'name': 'Andrew Miller', 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 1281}, {'authorId': '152160465', 'url': 'https://www.semanticscholar.org/author/152160465', 'name': 'Rob Miller', 'affiliations': [], 'homepage': None, 'paperCount': 120, 'citationCount': 9034}, {'authorId': '152160465', 'url': 'https://www.semanticscholar.org/author/152160465', 'name': 'Rob Miller', 'affiliations': [], 'homepage': None, 'paperCount': 120, 'citationCount': 9034}, {'authorId': '1715819', 'url': 'https://www.semanticscholar.org/author/1715819', 'name': 'Aubrey Tatarowicz', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 870}, {'authorId': '37929982', 'url': 'https://www.semanticscholar.org/author/37929982', 'name': 'B. White', 'affiliations': [], 'homepage': None, 'paperCount': 20, 'citationCount': 1288}, {'authorId': '144289340', 'url': 'https://www.semanticscholar.org/author/144289340', 'name': 'Samuel White', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 1473}, {'authorId': '2059814276', 'url': 'https://www.semanticscholar.org/author/2059814276', 'name': 'Tom Yeh', 'affiliations': [], 'homepage': None, 'paperCount': 34, 'citationCount': 2248}]}",535.0,"{'DBLP': 'conf/uist/BighamJJLMMMTWWY10', 'MAG': '2090048052', 'DOI': '10.1145/1866029.1866080', 'CorpusId': 52804681}",['Computer Science'],29.0,False,{'name': 'Proceedings of the 23nd annual ACM symposium on User interface software and technology'},10/3/2010,"['Book', 'JournalArticle', 'Conference']",40.0,VizWiz: nearly real-time answers to visual questions,https://www.semanticscholar.org/paper/8a960ea40fe7b32a1ee702a84f64ec1de5c3e7fe,UIST,2010
2095020710,"This paper explores the use of visual operators for solids modeling. We focus on designing interfaces for free-form operators such as blends, sweeps, and deformations, because these operators have a large number of interacting parameters whose effects are often determined by an underlying parameterization. In this type of interactive modeling good solutions to the design problem have aesthetic as well as engineering components. Traditionally, interaction with the parameters of these operators has been through text editors, curve editors, or trial-and-error with a slider bar. Parametric values have been estimated from data, but not interactively. These parametersare usually oneor two-dimensional,but the operators themselves are intrinsically three-dimensional in that they are used to model surfaces visualized in 3D. The traditional textual style of interaction is tedious and interposes a level of abstraction between the parameters and the resulting surface. A 3D visual interface has the potential to reduce or eliminate these problems by combining parameters and representing them with a higherlevel visual tool. The visual tools we present not only speed up the process of determining good parameter values but also provide visual interactions that are either independent of the particular parameterizations or make explicit the effect of the parameterizations. Additionally, these tools can be manipulated in the same 3D space as the surfaces producedby the operators, supporting quick, interactive exploration of the large design space of these free-form operators. This paper discusses the difficulties in creating a coherent user interface for interactive modeling. To this end we present four principles for designing visual operators, using several free-form visual operators as concrete examples. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling, Curve, surface, solid, and object representations, Splines; Additional",1.0,"This paper explores the use of visual operators for solids modeling. We focus on designing interfaces for free-form operators such as blends, sweeps, and deformations, because these operators have a large number of interacting parameters whose effects are often determined by an underlying parameterization. In this type of interactive modeling good solutions to the design problem have aesthetic as well as engineering components. Traditionally, interaction with the parameters of these operators has been through text editors, curve editors, or trial-and-error with a slider bar. Parametric values have been estimated from data, but not interactively. These parametersare usually oneor two-dimensional,but the operators themselves are intrinsically three-dimensional in that they are used to model surfaces visualized in 3D. The traditional textual style of interaction is tedious and interposes a level of abstraction between the parameters and the resulting surface. A 3D visual interface has the potential to reduce or eliminate these problems by combining parameters and representing them with a higherlevel visual tool. The visual tools we present not only speed up the process of determining good parameter values but also provide visual interactions that are either independent of the particular parameterizations or make explicit the effect of the parameterizations. Additionally, these tools can be manipulated in the same 3D space as the surfaces producedby the operators, supporting quick, interactive exploration of the large design space of these free-form operators. This paper discusses the difficulties in creating a coherent user interface for interactive modeling. To this end we present four principles for designing visual operators, using several free-form visual operators as concrete examples. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling, Curve, surface, solid, and object representations, Splines; Additional","['Department of Computer Science, Box 1910 Brown University, Providence, RI', 'Dept of Computer Science, University of Utah, Salt Lake City, UT']","['2129475567', '2233525272']",2095020710.0,"{'offset': 0, 'data': [{'authorId': '33901780', 'url': 'https://www.semanticscholar.org/author/33901780', 'name': 'C. Grimm', 'affiliations': [], 'homepage': None, 'paperCount': 198, 'citationCount': 2754}, {'authorId': '144034396', 'url': 'https://www.semanticscholar.org/author/144034396', 'name': 'D. Pugmire', 'affiliations': [], 'homepage': None, 'paperCount': 103, 'citationCount': 1230}]}",22.0,"{'DBLP': 'conf/uist/GrimmP95', 'MAG': '2095020710', 'DOI': '10.1145/215585.215652', 'CorpusId': 13621504}",['Computer Science'],0.0,True,{'pages': '51-60'},12/1/1995,['JournalArticle'],40.0,Visual interfaces for solids modeling,https://www.semanticscholar.org/paper/d88539f2f273ea2920f36a9b566c3829a3137399,UIST,1995
2099856682,"We present Ripples, a system which enables visualizations around each contact point on a touch display and, through these visualizations, provides feedback to the user about successes and errors of their touch interactions. Our visualization system is engineered to be overlaid on top of existing applications without requiring the applications to be modified in any way, and functions independently of the application's responses to user input. Ripples reduces the fundamental problem of ambiguity of feedback when an action results in an unexpected behaviour. This ambiguity can be caused by a wide variety of sources. We describe the ambiguity problem, and identify those sources. We then define a set of visual states and transitions needed to resolve this ambiguity, of use to anyone designing touch applications or systems. We then present the Ripples implementation of visualizations for those states, and the results of a user study demonstrating user preference for the system, and demonstrating its utility in reducing errors.",1.0,"We present Ripples, a system which enables visualizations around each contact point on a touch display and, through these visualizations, provides feedback to the user about successes and errors of their touch interactions. Our visualization system is engineered to be overlaid on top of existing applications without requiring the applications to be modified in any way, and functions independently of the application's responses to user input. Ripples reduces the fundamental problem of ambiguity of feedback when an action results in an unexpected behaviour. This ambiguity can be caused by a wide variety of sources. We describe the ambiguity problem, and identify those sources. We then define a set of visual states and transitions needed to resolve this ambiguity, of use to anyone designing touch applications or systems. We then present the Ripples implementation of visualizations for those states, and the results of a user study demonstrating user preference for the system, and demonstrating its utility in reducing errors.","['Microsoft Research, redmond, WA, USA#TAB#', 'Microsoft Surface, Redmond, WA, USA#TAB#', 'Microsoft Surface, Redmond, WA, USA#TAB#', '[Microsoft Corp., Redmond, WA, USA]', 'Microsoft Surface, Redmond, WA, USA#TAB#', '[Microsoft Corp., Redmond, WA, USA]', '[Microsoft Corp., Redmond, WA, USA]']","['1886754024', '1992290920', '2065696548', '2217788749', '2373765700', '2571441711', '2666836200']",2099856682.0,"{'offset': 0, 'data': [{'authorId': '1961958', 'url': 'https://www.semanticscholar.org/author/1961958', 'name': 'Daniel J. Wigdor', 'affiliations': [], 'homepage': None, 'paperCount': 189, 'citationCount': 7036}, {'authorId': '2111143309', 'url': 'https://www.semanticscholar.org/author/2111143309', 'name': 'Sarah Williams', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 189}, {'authorId': '2056248702', 'url': 'https://www.semanticscholar.org/author/2056248702', 'name': 'Michael Cronin', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 74}, {'authorId': '1410942323', 'url': 'https://www.semanticscholar.org/author/1410942323', 'name': 'R. Levy', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 76}, {'authorId': '2112604890', 'url': 'https://www.semanticscholar.org/author/2112604890', 'name': 'Katie White', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 74}, {'authorId': '2508975', 'url': 'https://www.semanticscholar.org/author/2508975', 'name': 'Maxim Mazeev', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 74}, {'authorId': '2704133', 'url': 'https://www.semanticscholar.org/author/2704133', 'name': 'Hrvoje Benko', 'affiliations': [], 'homepage': None, 'paperCount': 144, 'citationCount': 7711}]}",73.0,"{'DBLP': 'conf/uist/WigdorWCLWMB09', 'MAG': '2099856682', 'DOI': '10.1145/1622176.1622180', 'CorpusId': 5880925}",['Computer Science'],5.0,False,{'pages': '3-12'},10/4/2009,['JournalArticle'],35.0,Ripples: utilizing per-contact visualizations to improve user interaction with touch displays,https://www.semanticscholar.org/paper/ba9916f43057a8f385b2cb75e7326f5303a298aa,UIST,2009
2103934241,"The deep web contains an order of magnitude more information than the surface web, but that information is hidden behind the web forms of a large number of web sites. Metasearch engines can help users explore this information by aggregating results from multiple resources, but previously these could only be created and maintained by programmers. In this paper, we explore the automatic creation of metasearch mash-ups by mining the web interactions of multiple web users to find relations between query forms on different web sites. We also present an implemented system called TX2 that uses those connections to search multiple deep web resources simultaneously and integrate the results in context in a single results page. TX2 illustrates the promise of constructing mash-ups automatically and the potential of mining web interactions to explore deep web resources.",0.0,"The deep web contains an order of magnitude more information than the surface web, but that information is hidden behind the web forms of a large number of web sites. Metasearch engines can help users explore this information by aggregating results from multiple resources, but previously these could only be created and maintained by programmers. In this paper, we explore the automatic creation of metasearch mash-ups by mining the web interactions of multiple web users to find relations between query forms on different web sites. We also present an implemented system called TX2 that uses those connections to search multiple deep web resources simultaneously and integrate the results in context in a single results page. TX2 illustrates the promise of constructing mash-ups automatically and the potential of mining web interactions to explore deep web resources.","['University of Rochester, Rochester, NY USA', 'IBM Almaden Reserach Center, San Jose, CA, USA', ' Univ. of Washington, Seattle, WA, USA']","['2112106364', '2130903245', '2152190508']",2103934241.0,"{'offset': 0, 'data': [{'authorId': '1744846', 'url': 'https://www.semanticscholar.org/author/1744846', 'name': 'Jeffrey P. Bigham', 'affiliations': [], 'homepage': None, 'paperCount': 282, 'citationCount': 9073}, {'authorId': '48543374', 'url': 'https://www.semanticscholar.org/author/48543374', 'name': 'R. S. Kaminsky', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 274}, {'authorId': '2057155807', 'url': 'https://www.semanticscholar.org/author/2057155807', 'name': 'Jeffrey Nichols', 'affiliations': [], 'homepage': None, 'paperCount': 121, 'citationCount': 4174}]}",3.0,"{'DBLP': 'conf/uist/BighamKN09', 'MAG': '2103934241', 'DOI': '10.1145/1622176.1622215', 'CorpusId': 1120029}",['Computer Science'],0.0,False,{'pages': '203-212'},10/4/2009,['JournalArticle'],25.0,Mining web interactions to automatically create mash-ups,https://www.semanticscholar.org/paper/43fe42999f13de090d7e7859c96307f1e6eb4290,UIST,2009
2113702535,"This paper explores techniques for evaluating and improving immersion in Desktop Virtual Reality (VR). Three experiments are reported which extend findings on immersion in VR reported by Pausch et al. [9]. In the current experiments, a visual search paradigm was used to examine navigation in Desktop VR both with and without navigational aids. Pausch et al. found that non-head tracked users took significantly longer than predicted when the search target was absent, which was interpreted as indicative of a loss of sense of immersion. Our first experiment extended the Pausch et al. experiment to a desktop display. Our findings differ in that search times matched prediction when the target was absent, indicating that the Pausch et al. study does not transfer to Desktop VR. In the second and third experiments, our visual search task was performed while navigating a set of 3D hallways. We introduce a new navigation aid called Peripheral L.enses, intended to provide simulated peripheral vision. Informal studies suggested that Peripheral Lenses decrease search time, indicating an enhanced sense of immersion in Desktop VR. However, formal studies contradict that, demonstrating the importance of formal usability studies in the development of user interface software. We also gained evidence that visual attention findings transfer to Desktop VR.",1.0,"This paper explores techniques for evaluating and improving immersion in Desktop Virtual Reality (VR). Three experiments are reported which extend findings on immersion in VR reported by Pausch et al. [9]. In the current experiments, a visual search paradigm was used to examine navigation in Desktop VR both with and without navigational aids. Pausch et al. found that non-head tracked users took significantly longer than predicted when the search target was absent, which was interpreted as indicative of a loss of sense of immersion. Our first experiment extended the Pausch et al. experiment to a desktop display. Our findings differ in that search times matched prediction when the target was absent, indicating that the Pausch et al. study does not transfer to Desktop VR. In the second and third experiments, our visual search task was performed while navigating a set of 3D hallways. We introduce a new navigation aid called Peripheral L.enses, intended to provide simulated peripheral vision. Informal studies suggested that Peripheral Lenses decrease search time, indicating an enhanced sense of immersion in Desktop VR. However, formal studies contradict that, demonstrating the importance of formal usability studies in the development of user interface software. We also gained evidence that visual attention findings transfer to Desktop VR.","['Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Microsoft Research, One Microsoft Way Redmond, WA#TAB#']","['2054449169', '2071984077', '2172094290']",2113702535.0,"{'offset': 0, 'data': [{'authorId': '1699184', 'url': 'https://www.semanticscholar.org/author/1699184', 'name': 'G. Robertson', 'affiliations': [], 'homepage': None, 'paperCount': 126, 'citationCount': 14059}, {'authorId': '1817251', 'url': 'https://www.semanticscholar.org/author/1817251', 'name': 'M. Czerwinski', 'affiliations': [], 'homepage': None, 'paperCount': 250, 'citationCount': 14788}, {'authorId': '3104944', 'url': 'https://www.semanticscholar.org/author/3104944', 'name': 'Maarten van Dantzich', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 1631}]}",128.0,"{'DBLP': 'conf/uist/RobertsonCD97', 'MAG': '2113702535', 'DOI': '10.1145/263407.263409', 'CorpusId': 2699341}",['Computer Science'],3.0,False,{'pages': '11-19'},10/1/1997,['JournalArticle'],25.0,Immersion in desktop virtual reality,https://www.semanticscholar.org/paper/6db0d33f6de453b5b44a938fe78007f6b4dd3e71,UIST,1997
2113771664,"High precision parameter manipulation tasks typically require adjustment of the scale of manipulation in addition to the parameter itself. This paper introduces the notion of Zoom Sliding, or Zliding, for fluid integrated manipulation of scale (zooming) via pressure input while parameter manipulation within that scale is achieved via x-y cursor movement (sliding). We also present the Zlider (Figure 1), a widget that instantiates the Zliding concept. We experimentally evaluate three different input techniques for use with the Zlider in conjunction with a stylus for x-y cursor positioning, in a high accuracy zoom and select task. Our results marginally favor the stylus with integrated isometric pressure sensing tip over bimanual techniques which separate zooming and sliding controls over the two hands. We discuss the implications of our results and present further designs that make use of Zliding.",1.0,"High precision parameter manipulation tasks typically require adjustment of the scale of manipulation in addition to the parameter itself. This paper introduces the notion of Zoom Sliding, or Zliding, for fluid integrated manipulation of scale (zooming) via pressure input while parameter manipulation within that scale is achieved via x-y cursor movement (sliding). We also present the Zlider (Figure 1), a widget that instantiates the Zliding concept. We experimentally evaluate three different input techniques for use with the Zlider in conjunction with a stylus for x-y cursor positioning, in a high accuracy zoom and select task. Our results marginally favor the stylus with integrated isometric pressure sensing tip over bimanual techniques which separate zooming and sliding controls over the two hands. We discuss the implications of our results and present further designs that make use of Zliding.","['University of Toronto', 'University of Toronto']","['2130130894', '2137512077']",2113771664.0,"{'offset': 0, 'data': [{'authorId': '2057749310', 'url': 'https://www.semanticscholar.org/author/2057749310', 'name': 'Gonzalo A. Ramos', 'affiliations': [], 'homepage': None, 'paperCount': 42, 'citationCount': 2667}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",114.0,"{'DBLP': 'conf/uist/RamosB05', 'MAG': '2113771664', 'DOI': '10.1145/1095034.1095059', 'CorpusId': 13064259}",['Computer Science'],17.0,False,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['JournalArticle', 'Book', 'Conference']",40.0,Zliding: fluid zooming and sliding for high precision parameter manipulation,https://www.semanticscholar.org/paper/4a2d856bf893f15c68c2f3664ae6790c722d3ed6,UIST,2005
2115630841,"We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.",1.0,"We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.","[', Georgia Institute of Technology, Atlanta, GA', ', Georgia Institute of Technology, Atlanta, GA']","['2127286128', '620732773']",2115630841.0,"{'offset': 0, 'data': [{'authorId': '1701358', 'url': 'https://www.semanticscholar.org/author/1701358', 'name': 'Shwetak N. Patel', 'affiliations': [], 'homepage': None, 'paperCount': 241, 'citationCount': 10115}, {'authorId': '9267108', 'url': 'https://www.semanticscholar.org/author/9267108', 'name': 'G. Abowd', 'affiliations': [], 'homepage': None, 'paperCount': 482, 'citationCount': 38493}]}",52.0,"{'DBLP': 'conf/uist/PatelA07', 'MAG': '2115630841', 'DOI': '10.1145/1294211.1294250', 'CorpusId': 195027}",['Computer Science'],3.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['JournalArticle', 'Book', 'Conference']",8.0,Blui: low-cost localized blowable user interfaces,https://www.semanticscholar.org/paper/3b7da2ac78c7c024b37060178cfaaf32e49ad017,UIST,2007
2124670211,"We introduce ARC-Pad (Absolute+Relative Cursor pad), a novel technique for interacting with large displays using a mobile phone's touchscreen. In ARC-Pad we combine ab-solute and relative cursor positioning. Tapping with ARC-Pad causes the cursor to jump to the corresponding location on the screen, providing rapid movement across large distances. For fine position control, users can also clutch using relative mode. Unlike prior hybrid cursor positioning techniques, ARC-Pad does not require an explicit switch between relative and absolute modes. We compared ARC-Pad with the relative positioning commonly found on touchpads. Users were given a target acquisition task on a large display, and results showed that they were faster with ARC-Pad, without sacrificing accuracy. Users welcomed the benefits associated with ARC-Pad.",1.0,"We introduce ARC-Pad (Absolute+Relative Cursor pad), a novel technique for interacting with large displays using a mobile phone's touchscreen. In ARC-Pad we combine ab-solute and relative cursor positioning. Tapping with ARC-Pad causes the cursor to jump to the corresponding location on the screen, providing rapid movement across large distances. For fine position control, users can also clutch using relative mode. Unlike prior hybrid cursor positioning techniques, ARC-Pad does not require an explicit switch between relative and absolute modes. We compared ARC-Pad with the relative positioning commonly found on touchpads. Users were given a target acquisition task on a large display, and results showed that they were faster with ARC-Pad, without sacrificing accuracy. Users welcomed the benefits associated with ARC-Pad.","['Univ. of Manitoba, Winnipeg, Man., Canada', 'Univ. of Manitoba, Winnipeg, Man., Canada']","['1569396467', '2223111602']",2124670211.0,"{'offset': 0, 'data': [{'authorId': '40323726', 'url': 'https://www.semanticscholar.org/author/40323726', 'name': 'David C. McCallum', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 243}, {'authorId': '1773923', 'url': 'https://www.semanticscholar.org/author/1773923', 'name': 'Pourang Irani', 'affiliations': [], 'homepage': None, 'paperCount': 231, 'citationCount': 5076}]}",93.0,"{'DBLP': 'conf/uist/McCallumI09', 'MAG': '2124670211', 'DOI': '10.1145/1622176.1622205', 'CorpusId': 6167094}",['Computer Science'],9.0,False,{'pages': '153-156'},10/4/2009,['JournalArticle'],15.0,ARC-Pad: absolute+relative cursor positioning for large displays with a mobile touchscreen,https://www.semanticscholar.org/paper/4133acd85f449f4d929374777325d27c9bdab375,UIST,2009
2126434504,"We describe input devices and two-handed interaction techniques to support map navigation tasks. We discuss several design variations and user testing of two-handed navigation techniques, including puck and stylus input on a Wacom tablet, as well as a novel design incorporating a touchpad (for the nonpreferred hand) and a mouse (for the preferred hand). To support the latter technique, we introduce a new input device, the TouchMouse, which is a standard mouse augmented with a pair of one-bit touch sensors, one for the palm and one for the index finger. Finally, we propose several enhancements to Buxton’s three-state model of graphical input and extend this model to encompass two-handed input transactions as well.",1.0,"We describe input devices and two-handed interaction techniques to support map navigation tasks. We discuss several design variations and user testing of two-handed navigation techniques, including puck and stylus input on a Wacom tablet, as well as a novel design incorporating a touchpad (for the nonpreferred hand) and a mouse (for the preferred hand). To support the latter technique, we introduce a new input device, the TouchMouse, which is a standard mouse augmented with a pair of one-bit touch sensors, one for the palm and one for the index finger. Finally, we propose several enhancements to Buxton’s three-state model of graphical input and extend this model to encompass two-handed input transactions as well.","['Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Microsoft Research, One Microsoft Way Redmond, WA#TAB#']","['1560725665', '2071984077', '2154671057']",2126434504.0,"{'offset': 0, 'data': [{'authorId': '1738072', 'url': 'https://www.semanticscholar.org/author/1738072', 'name': 'K. Hinckley', 'affiliations': [], 'homepage': None, 'paperCount': 163, 'citationCount': 9081}, {'authorId': '1817251', 'url': 'https://www.semanticscholar.org/author/1817251', 'name': 'M. Czerwinski', 'affiliations': [], 'homepage': None, 'paperCount': 250, 'citationCount': 14788}, {'authorId': '39453376', 'url': 'https://www.semanticscholar.org/author/39453376', 'name': 'M. Sinclair', 'affiliations': [], 'homepage': None, 'paperCount': 61, 'citationCount': 2643}]}",121.0,"{'DBLP': 'conf/uist/HinckleyCS98', 'MAG': '2126434504', 'DOI': '10.1145/288392.288572', 'CorpusId': 207235326}",['Computer Science'],6.0,False,{'pages': '49-58'},11/1/1998,['JournalArticle'],29.0,Interaction and modeling techniques for desktop two-handed input,https://www.semanticscholar.org/paper/8eab679e7e148ca18029a95c8cabde0fb543ab4c,UIST,1998
2136419864,"Members of geographically distributed work groups often complain of a feeling of isolation and of not knowing “who is around”. Argohalls attempt to solve this problem by integrating video icons, clustered into groups representing physical hallways, into the Argo telecollaboration system. Argo users can “hang out” in hallways in order to keep track of the co-workers on their projects, and they can roam other hallways to “run into” whoever happens to be there.",1.0,"Members of geographically distributed work groups often complain of a feeling of isolation and of not knowing “who is around”. Argohalls attempt to solve this problem by integrating video icons, clustered into groups representing physical hallways, into the Argo telecollaboration system. Argo users can “hang out” in hallways in order to keep track of the co-workers on their projects, and they can roam other hallways to “run into” whoever happens to be there.","['SunSoft, 2550 Garcia Ave, Mountain View, CA', 'DEC Systems Research Center, 130 Lytton Avenue, Palo Alto, CA', 'DEC Systems Research Center, 130 Lytton Avenue, Palo Alto, CA']","['1989920923', '2040079410', '2973593973']",2136419864.0,"{'offset': 0, 'data': [{'authorId': '2481955', 'url': 'https://www.semanticscholar.org/author/2481955', 'name': 'Hania Gajewska', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 96}, {'authorId': '2083378', 'url': 'https://www.semanticscholar.org/author/2083378', 'name': 'M. Manasse', 'affiliations': [], 'homepage': None, 'paperCount': 59, 'citationCount': 7778}, {'authorId': '2737515', 'url': 'https://www.semanticscholar.org/author/2737515', 'name': 'D. Redell', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 1249}]}",12.0,"{'MAG': '2136419864', 'DBLP': 'conf/uist/GajewskaMR95', 'DOI': '10.1145/215585.219090', 'CorpusId': 37459808}",['Computer Science'],0.0,False,{'pages': '157-158'},12/1/1995,['JournalArticle'],7.0,Argohalls: adding support for group awareness to the Argo telecollaboration system,https://www.semanticscholar.org/paper/a91eb1f9275992e9ea90aac476e2e402c6521756,UIST,1995
2139459444,"In this paper we present novel input devices that combine the standard capabilities of a computer mouse with multi-touch sensing. Our goal is to enrich traditional pointer-based desktop interactions with touch and gestures. To chart the design space, we present five different multi-touch mouse implementations. Each explores a different touch sensing strategy, which leads to differing form-factors and hence interactive possibilities. In addition to the detailed description of hardware and software implementations of our prototypes, we discuss the relative strengths, limitations and affordances of these novel input devices as informed by the results of a preliminary user study.",1.0,"In this paper we present novel input devices that combine the standard capabilities of a computer mouse with multi-touch sensing. Our goal is to enrich traditional pointer-based desktop interactions with touch and gestures. To chart the design space, we present five different multi-touch mouse implementations. Each explores a different touch sensing strategy, which leads to differing form-factors and hence interactive possibilities. In addition to the detailed description of hardware and software implementations of our prototypes, we discuss the relative strengths, limitations and affordances of these novel input devices as informed by the results of a preliminary user study.","['[Microsoft Corp., Redmond, WA, USA]', 'Microsoft Research, redmond, WA, USA#TAB#', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', '[Microsoft Corp., Redmond, WA, USA]', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research Cambridge, Cambrdige, United Kingdom', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', '[Microsoft Corp., Redmond, WA, USA]', '[Microsoft Corp., Redmond, WA, USA]']","['1794776656', '1886754024', '2065024650', '2098553916', '2124876992', '2148014207', '2151535930', '2165003359', '2168200088', '2617234186', '431806573']",2139459444.0,"{'offset': 0, 'data': [{'authorId': '144989882', 'url': 'https://www.semanticscholar.org/author/144989882', 'name': 'N. Villar', 'affiliations': [], 'homepage': None, 'paperCount': 84, 'citationCount': 3148}, {'authorId': '79406746', 'url': 'https://www.semanticscholar.org/author/79406746', 'name': 'S. Izadi', 'affiliations': [], 'homepage': None, 'paperCount': 212, 'citationCount': 20595}, {'authorId': '40356481', 'url': 'https://www.semanticscholar.org/author/40356481', 'name': 'Dan Rosenfeld', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 326}, {'authorId': '2704133', 'url': 'https://www.semanticscholar.org/author/2704133', 'name': 'Hrvoje Benko', 'affiliations': [], 'homepage': None, 'paperCount': 144, 'citationCount': 7711}, {'authorId': '1800990', 'url': 'https://www.semanticscholar.org/author/1800990', 'name': 'J. Helmes', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 594}, {'authorId': '2711385', 'url': 'https://www.semanticscholar.org/author/2711385', 'name': 'Jonathan Westhues', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 629}, {'authorId': '144356949', 'url': 'https://www.semanticscholar.org/author/144356949', 'name': 'Steve Hodges', 'affiliations': [], 'homepage': None, 'paperCount': 139, 'citationCount': 12297}, {'authorId': '20592981', 'url': 'https://www.semanticscholar.org/author/20592981', 'name': 'E. Ofek', 'affiliations': [], 'homepage': None, 'paperCount': 130, 'citationCount': 6658}, {'authorId': '120225597', 'url': 'https://www.semanticscholar.org/author/120225597', 'name': 'Alex Butler', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 2353}, {'authorId': '144937256', 'url': 'https://www.semanticscholar.org/author/144937256', 'name': 'Xiang Cao', 'affiliations': [], 'homepage': None, 'paperCount': 46, 'citationCount': 1806}, {'authorId': '48951491', 'url': 'https://www.semanticscholar.org/author/48951491', 'name': 'Billy Chen', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 1289}]}",91.0,"{'DBLP': 'conf/uist/VillarIRBHWHOBCC09', 'MAG': '2139459444', 'DOI': '10.1145/1622176.1622184', 'CorpusId': 14367567}",['Computer Science'],2.0,False,{'pages': '33-42'},10/4/2009,['JournalArticle'],34.0,Mouse 2.0: multi-touch meets the mouse,https://www.semanticscholar.org/paper/fe9d0954465aa5e9c9a56fbab1fca784a224463e,UIST,2009
2139906308,"The information a small mobile device can show via its display has been always limited by its size. In large information spaces, relevant information, such as important locations on a map can get clipped when a user starts zooming and panning. Dynamic ambient lighting allows mobile devices to visualize off-screen objects by illuminating the background without compromising valuable display space. The lighted spots can be used to show the direction and distance of such objects by varying the spot's position and intensity. Dynamic ambient lighting also provides a new way of displaying the state of a mobile device. Illumination is provided by a prototype rear of device shell which contains LEDs and requires the device to be placed on a surface, such as a table or desk.",0.0,"The information a small mobile device can show via its display has been always limited by its size. In large information spaces, relevant information, such as important locations on a map can get clipped when a user starts zooming and panning. Dynamic ambient lighting allows mobile devices to visualize off-screen objects by illuminating the background without compromising valuable display space. The lighted spots can be used to show the direction and distance of such objects by varying the spot's position and intensity. Dynamic ambient lighting also provides a new way of displaying the state of a mobile device. Illumination is provided by a prototype rear of device shell which contains LEDs and requires the device to be placed on a surface, such as a table or desk.","['Ludwig-Maximilians-Universität München , München , Germany', 'Technische Universitát Berlin, Berlin, Germany', 'Ludwig-Maximilians-Universität München , München , Germany']","['2083094305', '2226586409', '2276246055']",2139906308.0,"{'offset': 0, 'data': [{'authorId': '2066431186', 'url': 'https://www.semanticscholar.org/author/2066431186', 'name': 'Qian Qin', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 19}, {'authorId': '1876551', 'url': 'https://www.semanticscholar.org/author/1876551', 'name': 'M. Rohs', 'affiliations': [], 'homepage': None, 'paperCount': 171, 'citationCount': 5632}, {'authorId': '3103005', 'url': 'https://www.semanticscholar.org/author/3103005', 'name': 'Sven G. Kratz', 'affiliations': [], 'homepage': None, 'paperCount': 60, 'citationCount': 1773}]}",19.0,"{'DBLP': 'conf/uist/QinRK11', 'MAG': '2139906308', 'DOI': '10.1145/2046396.2046418', 'CorpusId': 23549269}",['Computer Science'],0.0,False,{'pages': '51-52'},10/16/2011,['JournalArticle'],6.0,Dynamic ambient lighting for mobile devices,https://www.semanticscholar.org/paper/15a091e135b17a27c1c56661e537a8d4a4edbb5d,UIST,2011
2142747340,"YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.",1.0,"YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.","['Autodesk Research,Toronto,Canada', 'Autodesk Research,Toronto,Canada', 'Autodesk Research,Toronto,Canada', 'University of Alberta, Edmonton, Canada']","['1899877228', '2090457899', '2115951828', '2304707697']",2142747340.0,"{'offset': 0, 'data': [{'authorId': '2408187', 'url': 'https://www.semanticscholar.org/author/2408187', 'name': 'F. Anderson', 'affiliations': [], 'homepage': None, 'paperCount': 53, 'citationCount': 1382}, {'authorId': '2666589', 'url': 'https://www.semanticscholar.org/author/2666589', 'name': 'Tovi Grossman', 'affiliations': [], 'homepage': None, 'paperCount': 187, 'citationCount': 7900}, {'authorId': '49668403', 'url': 'https://www.semanticscholar.org/author/49668403', 'name': 'Justin Matejka', 'affiliations': [], 'homepage': None, 'paperCount': 46, 'citationCount': 1867}, {'authorId': '1703735', 'url': 'https://www.semanticscholar.org/author/1703735', 'name': 'G. Fitzmaurice', 'affiliations': [], 'homepage': None, 'paperCount': 182, 'citationCount': 9540}]}",236.0,"{'DBLP': 'conf/uist/AndersonGMF13', 'MAG': '2142747340', 'DOI': '10.1145/2501988.2502045', 'CorpusId': 2640813}",['Computer Science'],25.0,False,{'name': 'Proceedings of the 26th annual ACM symposium on User interface software and technology'},10/8/2013,"['Book', 'JournalArticle', 'Conference']",49.0,YouMove: enhancing movement training with an augmented reality mirror,https://www.semanticscholar.org/paper/b8aade70c8a310c232a2968b54d50f5d170e08df,UIST,2013
2148819007,"We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.",1.0,"We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.","['[Microsoft research, Redmond, WA]']",['2105571773'],2148819007.0,"{'offset': 0, 'data': [{'authorId': '145771244', 'url': 'https://www.semanticscholar.org/author/145771244', 'name': 'Andrew D. Wilson', 'affiliations': [], 'homepage': None, 'paperCount': 123, 'citationCount': 12219}]}",419.0,"{'DBLP': 'conf/uist/Wilson05', 'MAG': '2148819007', 'DOI': '10.1145/1095034.1095047', 'CorpusId': 1008695}",['Computer Science'],16.0,False,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['JournalArticle', 'Book', 'Conference']",50.0,PlayAnywhere: a compact interactive tabletop projection-vision system,https://www.semanticscholar.org/paper/749683345813b14018ddea33157d2a2efb0f6565,UIST,2005
2150549828,"This paper presents PneUI, an enabling technology to build shape-changing interfaces through pneumatically-actuated soft composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. This is enabled by the composites' multi-layer structures with different mechanical or electrical properties. The shape changing states are computationally controllable through pneumatics and pre-defined structure. We explore the design space of PneUI through four applications: height changing tangible phicons, a shape changing mobile, a transformable tablet case and a shape shifting lamp.",1.0,"This paper presents PneUI, an enabling technology to build shape-changing interfaces through pneumatically-actuated soft composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. This is enabled by the composites' multi-layer structures with different mechanical or electrical properties. The shape changing states are computationally controllable through pneumatics and pre-defined structure. We explore the design space of PneUI through four applications: height changing tangible phicons, a shape changing mobile, a transformable tablet case and a shape shifting lamp.","['Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA']","['1664658768', '2069682576', '2101434678', '2123988653', '2127473554', '2127501813']",2150549828.0,"{'offset': 0, 'data': [{'authorId': '2912448', 'url': 'https://www.semanticscholar.org/author/2912448', 'name': 'Lining Yao', 'affiliations': [], 'homepage': None, 'paperCount': 85, 'citationCount': 1741}, {'authorId': '2392339', 'url': 'https://www.semanticscholar.org/author/2392339', 'name': 'Ryuma Niiyama', 'affiliations': [], 'homepage': None, 'paperCount': 111, 'citationCount': 1600}, {'authorId': '39577419', 'url': 'https://www.semanticscholar.org/author/39577419', 'name': 'Jifei Ou', 'affiliations': [], 'homepage': None, 'paperCount': 27, 'citationCount': 1107}, {'authorId': '2770912', 'url': 'https://www.semanticscholar.org/author/2770912', 'name': 'S. Follmer', 'affiliations': [], 'homepage': None, 'paperCount': 122, 'citationCount': 3908}, {'authorId': '3332772', 'url': 'https://www.semanticscholar.org/author/3332772', 'name': 'Clark Della Silva', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 344}, {'authorId': '145208441', 'url': 'https://www.semanticscholar.org/author/145208441', 'name': 'H. Ishii', 'affiliations': [], 'homepage': None, 'paperCount': 337, 'citationCount': 24821}]}",279.0,"{'DBLP': 'conf/uist/YaoNOFSI13', 'MAG': '2150549828', 'DOI': '10.1145/2501988.2502037', 'CorpusId': 1753273}",['Computer Science'],9.0,False,{'name': 'Proceedings of the 26th annual ACM symposium on User interface software and technology'},10/8/2013,"['JournalArticle', 'Book', 'Conference']",43.0,PneUI: pneumatically actuated soft composite materials for shape changing interfaces,https://www.semanticscholar.org/paper/5edc648630e4e855b5e640318a993e5e7a679594,UIST,2013
2151217988,"Modern computer displays tend to be in fixed size, rigid, and rectilinear rendering them insensitive to the visual area demands of an application or the desires of the user. Foldable displays offer the ability to reshape and resize the interactive surface at our convenience and even permit us to carry a very large display surface in a small volume. In this paper, we implement four interactive foldable display designs using image projection with low-cost tracking and explore display behaviors using orientation sensitivity.",1.0,"Modern computer displays tend to be in fixed size, rigid, and rectilinear rendering them insensitive to the visual area demands of an application or the desires of the user. Foldable displays offer the ability to reshape and resize the interactive surface at our convenience and even permit us to carry a very large display surface in a small volume. In this paper, we implement four interactive foldable display designs using image projection with low-cost tracking and explore display behaviors using orientation sensitivity.","[' Carnegie Mellon University Pittsburgh PA USA', ' Carnegie Mellon University Pittsburgh PA USA', 'Smart Technologies, Calgary, AB, Canada']","['2135985964', '2171298838', '2225522945']",2151217988.0,"{'offset': 0, 'data': [{'authorId': '34741611', 'url': 'https://www.semanticscholar.org/author/34741611', 'name': 'J. C. Lee', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 2444}, {'authorId': '1749296', 'url': 'https://www.semanticscholar.org/author/1749296', 'name': 'S. Hudson', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 13645}, {'authorId': '2371546', 'url': 'https://www.semanticscholar.org/author/2371546', 'name': 'Edward Tse', 'affiliations': [], 'homepage': None, 'paperCount': 42, 'citationCount': 994}]}",92.0,"{'DBLP': 'conf/uist/LeeHT08', 'MAG': '2151217988', 'DOI': '10.1145/1449715.1449763', 'CorpusId': 15425905}",['Computer Science'],1.0,False,{'pages': '287-290'},10/19/2008,['JournalArticle'],7.0,Foldable interactive displays,https://www.semanticscholar.org/paper/80ffd5e89473d1096e6f848f10b8929b54a4d1b5,UIST,2008
2154174922,"In this paper we propose a new model for a class of rapid serial visual presentation (RSVP) interfaces [16] in the context of consumer video devices. The basic spatial layout ""explodes"" a sequence of image frames into a 3D trail in order to provide more context for a spatial/temporal presentation. As the user plays forward or back, the trail advances or recedes while the image in the foreground focus position is replaced. The design is able to incorporate a variety of methods for analyzing or highlighting images in the trail. Our hypotheses are that users can navigate more quickly and precisely to points of interest when compared to conventional consumer-based browsing, channel flipping, or fast-forwarding techniques. We report on an experiment testing our hypotheses in which we found that subjects were more accurate but not faster in browsing to a target of interest in recorded television content with a TV remote.",1.0,"In this paper we propose a new model for a class of rapid serial visual presentation (RSVP) interfaces [16] in the context of consumer video devices. The basic spatial layout ""explodes"" a sequence of image frames into a 3D trail in order to provide more context for a spatial/temporal presentation. As the user plays forward or back, the trail advances or recedes while the image in the foreground focus position is replaced. The design is able to incorporate a variety of methods for analyzing or highlighting images in the trail. Our hypotheses are that users can navigate more quickly and precisely to points of interest when compared to conventional consumer-based browsing, channel flipping, or fast-forwarding techniques. We report on an experiment testing our hypotheses in which we found that subjects were more accurate but not faster in browsing to a target of interest in recorded television content with a TV remote.","['Mitsubishi Electric Research Laboratories, Inc., 201 Broadway, Cambridge, MA#TAB#', 'Mitsubishi Electric Corporation, Industrial Design Center, 5-1-1 Ofuna, Kamakura, Kanagawa 247-8501 JAPAN', 'Mitsubishi Electric Research Laboratories, Inc., 201 Broadway, Cambridge, MA#TAB#', 'Mitsubishi Electric Corporation, Industrial Design Center, 5-1-1 Ofuna, Kamakura, Kanagawa 247-8501 JAPAN', 'Mitsubishi Electric Research Laboratories, Inc., 201 Broadway, Cambridge, MA#TAB#', 'Mitsubishi Electric Research Laboratories, Inc., 201 Broadway, Cambridge, MA#TAB#']","['2025996706', '2228683290', '2607714654', '2785337044', '2950596765', '399295147']",2154174922.0,"{'offset': 0, 'data': [{'authorId': '3242147', 'url': 'https://www.semanticscholar.org/author/3242147', 'name': 'K. Wittenburg', 'affiliations': [], 'homepage': None, 'paperCount': 72, 'citationCount': 1480}, {'authorId': '1694854', 'url': 'https://www.semanticscholar.org/author/1694854', 'name': 'C. Forlines', 'affiliations': [], 'homepage': None, 'paperCount': 106, 'citationCount': 5818}, {'authorId': '144548081', 'url': 'https://www.semanticscholar.org/author/144548081', 'name': 'T. Lanning', 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 310}, {'authorId': '2802986', 'url': 'https://www.semanticscholar.org/author/2802986', 'name': 'Alan Esenther', 'affiliations': [], 'homepage': None, 'paperCount': 30, 'citationCount': 749}, {'authorId': '2057597021', 'url': 'https://www.semanticscholar.org/author/2057597021', 'name': 'Shigeo Harada', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 48}, {'authorId': '2074239871', 'url': 'https://www.semanticscholar.org/author/2074239871', 'name': 'T. Miyachi', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 49}]}",48.0,"{'DBLP': 'conf/uist/WittenburgFLEHM03', 'MAG': '2154174922', 'DOI': '10.1145/964696.964709', 'CorpusId': 2348571}",['Computer Science'],5.0,True,{'pages': '115-124'},11/2/2003,['JournalArticle'],23.0,Rapid serial visual presentation techniques for consumer digital video devices,https://www.semanticscholar.org/paper/8f38359663e9e737b71ce0c6b8e9c51badae2e4c,UIST,2003
2161401111,"Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.",1.0,"Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.","['Microsoft Research, redmond, WA, USA#TAB#', 'Microsoft Research, redmond, WA, USA#TAB#', 'Microsoft Research, Redmond, WA, USA & University of Toronto, Toronto, ON, Canada']","['1886754024', '2105571773', '2130130894']",2161401111.0,"{'offset': 0, 'data': [{'authorId': '2704133', 'url': 'https://www.semanticscholar.org/author/2704133', 'name': 'Hrvoje Benko', 'affiliations': [], 'homepage': None, 'paperCount': 144, 'citationCount': 7711}, {'authorId': '145771244', 'url': 'https://www.semanticscholar.org/author/145771244', 'name': 'Andrew D. Wilson', 'affiliations': [], 'homepage': None, 'paperCount': 123, 'citationCount': 12219}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",194.0,"{'MAG': '2161401111', 'DBLP': 'conf/uist/BenkoWB08', 'DOI': '10.1145/1449715.1449729', 'CorpusId': 9444826}",['Computer Science'],14.0,False,{'pages': '77-86'},10/19/2008,['JournalArticle'],36.0,Sphere: multi-touch interactions on a spherical display,https://www.semanticscholar.org/paper/48ca535feaebd597a59eb15d07336bef54f46f95,UIST,2008
2168442304,"User interfaces are often based on static presentations, a model ill suited for conveying change. Consequently, events on the screen frequently startle and confuse users. Cartoon animation, in contrast, is exceedingly successful at engaging its audience; even the most bizarre events are easily comprehended. The Self user interface has served as a testbed for the application of cartoon animation techniques as a means of making the interface easier to understand and more pleasant to use. Attention to timing and transient detail allows Self objects to move solidly. Use of cartoon-style motion blur allows Self objects to move quickly and still maintain their comprehensibility. Self objects arrive and depart smoothly, without sudden materializations and disappearances, and they rise to the front of overlapping objects smoothly through the use of dissolve. Anticipating motion with a small contrary motion and pacing the middle of transitions faster than the endpoints results in smoother and clearer movements. Despite the differences between user interfaces and cartoons --cartoons are frivolous, passive entertainment and user interfaces are serious, interactive tools -- cartoon animation has much to lend to user interfaces to realize both affective and cognitive benefits. *This work was originally supported by Sun Microsystems Laboratories, an NSF Graduate Fellowship, National Science Foundation Presidential Young Investigator Grant #CCR-8657631, IBM Powell Foundation, Apple Computer, Inc., Cray Laboratories, Tandem, NCR Corporation, Texas Instruments, Inc., and Digital Equipment Corporation.",1.0,"User interfaces are often based on static presentations, a model ill suited for conveying change. Consequently, events on the screen frequently startle and confuse users. Cartoon animation, in contrast, is exceedingly successful at engaging its audience; even the most bizarre events are easily comprehended. The Self user interface has served as a testbed for the application of cartoon animation techniques as a means of making the interface easier to understand and more pleasant to use. Attention to timing and transient detail allows Self objects to move solidly. Use of cartoon-style motion blur allows Self objects to move quickly and still maintain their comprehensibility. Self objects arrive and depart smoothly, without sudden materializations and disappearances, and they rise to the front of overlapping objects smoothly through the use of dissolve. Anticipating motion with a small contrary motion and pacing the middle of transitions faster than the endpoints results in smoother and clearer movements. Despite the differences between user interfaces and cartoons --cartoons are frivolous, passive entertainment and user interfaces are serious, interactive tools -- cartoon animation has much to lend to user interfaces to realize both affective and cognitive benefits. *This work was originally supported by Sun Microsystems Laboratories, an NSF Graduate Fellowship, National Science Foundation Presidential Young Investigator Grant #CCR-8657631, IBM Powell Foundation, Apple Computer, Inc., Cray Laboratories, Tandem, NCR Corporation, Texas Instruments, Inc., and Digital Equipment Corporation.","['Sun Microsystems Laboratories, Inc., 2550 Garcia Avenue, Mountain View, CA', 'Computer Systems Laboratory, Stanford University, Stanford, CA#TAB#']","['2041937263', '2627193806']",2168442304.0,"{'offset': 0, 'data': [{'authorId': '145313643', 'url': 'https://www.semanticscholar.org/author/145313643', 'name': 'B. Chang', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 1196}, {'authorId': '1722615', 'url': 'https://www.semanticscholar.org/author/1722615', 'name': 'D. Ungar', 'affiliations': [], 'homepage': None, 'paperCount': 85, 'citationCount': 5771}]}",229.0,"{'MAG': '2168442304', 'DBLP': 'conf/uist/ChangU93', 'DOI': '10.1145/168642.168647', 'CorpusId': 1112711}",['Computer Science'],15.0,False,{'pages': '45-55'},12/1/1993,['JournalArticle'],47.0,Animation: from cartoons to the user interface,https://www.semanticscholar.org/paper/a952c0c2f231607c5bf66263d5884dc81863215e,UIST,1993
2171581899,"We explore the use of abstracted screenshots as part of a new help interface. Graphstract, an implementation of a graphical help system, extends the ideas of textually oriented Minimal Manuals to the use of screenshots, allowing multiple small graphical elements to be shown in a limited space. This allows a user to get an overview of a complex sequential task as a whole. The ideas have been developed by three iterations of prototyping and evaluation. A user study shows that Graphstract helps users perform tasks faster on some but not all tasks. Due to their graphical nature, it is possible to construct Graphstracts automatically from pre-recorded interactions. A second study shows that automated capture and replay is a low-cost method for authoring Graphstracts, and the resultant help is as understandable as manually constructed help.",1.0,"We explore the use of abstracted screenshots as part of a new help interface. Graphstract, an implementation of a graphical help system, extends the ideas of textually oriented Minimal Manuals to the use of screenshots, allowing multiple small graphical elements to be shown in a limited space. This allows a user to get an overview of a complex sequential task as a whole. The ideas have been developed by three iterations of prototyping and evaluation. A user study shows that Graphstract helps users perform tasks faster on some but not all tasks. Due to their graphical nature, it is possible to construct Graphstracts automatically from pre-recorded interactions. A second study shows that automated capture and replay is a low-cost method for authoring Graphstracts, and the resultant help is as understandable as manually constructed help.","['[Yahoo!, Sunnyvale, CA]', 'University of Illinois Urbana Champaign, Champaign, IL']","['2228850527', '281862109']",2171581899.0,"{'offset': 0, 'data': [{'authorId': '32065383', 'url': 'https://www.semanticscholar.org/author/32065383', 'name': 'Jeff Huang', 'affiliations': [], 'homepage': None, 'paperCount': 91, 'citationCount': 3040}, {'authorId': '1772877', 'url': 'https://www.semanticscholar.org/author/1772877', 'name': 'M. Twidale', 'affiliations': [], 'homepage': None, 'paperCount': 229, 'citationCount': 4663}]}",28.0,"{'DBLP': 'conf/uist/HuangT07', 'MAG': '2171581899', 'DOI': '10.1145/1294211.1294248', 'CorpusId': 10172005}",['Computer Science'],2.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference', 'Review']",25.0,Graphstract: minimal graphical help for computers,https://www.semanticscholar.org/paper/e5fac551f72c54086ae3c8b59bbddbee6d513e1b,UIST,2007
2246256135,"The Camera Culture Group at the MIT Media Lab aims to create a new class of imaging platforms. This talk will discuss three tracks of research: femto photography, retinal imaging, and 3D displays. Femto Photography consists of femtosecond laser illumination, picosecond-accurate detectors and mathematical reconstruction techniques allowing researchers to visualize propagation of light. Direct recording of reflected or scattered light at such a frame rate with sufficient brightness is nearly impossible. Using an indirect 'stroboscopic' method that records millions of repeated measurements by careful scanning in time and viewpoints we can rearrange the data to create a 'movie' of a nanosecond long event. Femto photography and a new generation of nano-photography (using ToF cameras) allow powerful inference with computer vision in presence of scattering. EyeNetra is a mobile phone attachment that allows users to test their own eyesight. The device reveals corrective measures thus bringing vision to billions of people who would not have had access otherwise. Another project, eyeMITRA, is a mobile retinal imaging solution that brings retinal exams to the realm of routine care, by lowering the cost of the imaging device to a 10th of its current cost and integrating the device with image analysis software and predictive analytics. This provides early detection of Diabetic Retinopathy that can change the arc of growth of the world's largest cause of blindness. Finally the talk will describe novel lightfield cameras and lightfield displays that require a compressive optical architecture to deal with high bandwidth requirements of 4D signals",0.0,"The Camera Culture Group at the MIT Media Lab aims to create a new class of imaging platforms. This talk will discuss three tracks of research: femto photography, retinal imaging, and 3D displays. Femto Photography consists of femtosecond laser illumination, picosecond-accurate detectors and mathematical reconstruction techniques allowing researchers to visualize propagation of light. Direct recording of reflected or scattered light at such a frame rate with sufficient brightness is nearly impossible. Using an indirect 'stroboscopic' method that records millions of repeated measurements by careful scanning in time and viewpoints we can rearrange the data to create a 'movie' of a nanosecond long event. Femto photography and a new generation of nano-photography (using ToF cameras) allow powerful inference with computer vision in presence of scattering. EyeNetra is a mobile phone attachment that allows users to test their own eyesight. The device reveals corrective measures thus bringing vision to billions of people who would not have had access otherwise. Another project, eyeMITRA, is a mobile retinal imaging solution that brings retinal exams to the realm of routine care, by lowering the cost of the imaging device to a 10th of its current cost and integrating the device with image analysis software and predictive analytics. This provides early detection of Diabetic Retinopathy that can change the arc of growth of the world's largest cause of blindness. Finally the talk will describe novel lightfield cameras and lightfield displays that require a compressive optical architecture to deal with high bandwidth requirements of 4D signals","['[MIT Media Lab, Cambridge, MA, USA]']",['2140705446'],2246256135.0,"{'offset': 0, 'data': [{'authorId': '145711633', 'url': 'https://www.semanticscholar.org/author/145711633', 'name': 'R. Raskar', 'affiliations': [], 'homepage': None, 'paperCount': 562, 'citationCount': 24374}]}",0.0,"{'DBLP': 'conf/uist/Raskar15', 'MAG': '2246256135', 'DOI': '10.1145/2807442.2814654', 'CorpusId': 6237519}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['Book', 'JournalArticle', 'Conference']",0.0,Extreme Computational Photography,https://www.semanticscholar.org/paper/f2d57dd89c04534ca5034fe56cf51da3853ecd20,UIST,2015
2533819212,"Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.",1.0,"Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.","[' Carnegie Mellon University Pittsburgh PA USA', ' Carnegie Mellon University Pittsburgh PA USA', ' Carnegie Mellon University Pittsburgh PA USA']","['2123491528', '2147333048', '2305394960']",2533819212.0,"{'offset': 0, 'data': [{'authorId': '2145953811', 'url': 'https://www.semanticscholar.org/author/2145953811', 'name': 'Yang Zhang', 'affiliations': [], 'homepage': None, 'paperCount': 24, 'citationCount': 1146}, {'authorId': '144742880', 'url': 'https://www.semanticscholar.org/author/144742880', 'name': 'Robert Xiao', 'affiliations': [], 'homepage': None, 'paperCount': 43, 'citationCount': 1830}, {'authorId': '145078227', 'url': 'https://www.semanticscholar.org/author/145078227', 'name': 'Chris Harrison', 'affiliations': [], 'homepage': None, 'paperCount': 154, 'citationCount': 8137}]}",86.0,"{'DBLP': 'conf/uist/ZhangXH16', 'MAG': '2533819212', 'DOI': '10.1145/2984511.2984574', 'CorpusId': 8809423}",['Computer Science'],6.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",59.0,Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography,https://www.semanticscholar.org/paper/9a761d349b636f1a616f9b2699e2e0403359593b,UIST,2016
2535337559,"Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.",0.0,"Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.","['Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Universidade Federal Fluminense, Niteroi (Brazil)', 'Massachusetts Institute of Technology Lincoln Laboratory, Cambridge, MA, USA.']","['2042299866', '2084788496', '2096783351', '2107086826', '2536879771', '2538411357']",2535337559.0,"{'offset': 0, 'data': [{'authorId': '3024298', 'url': 'https://www.semanticscholar.org/author/3024298', 'name': 'Misha Sra', 'affiliations': [], 'homepage': None, 'paperCount': 62, 'citationCount': 562}, {'authorId': '37883312', 'url': 'https://www.semanticscholar.org/author/37883312', 'name': 'D. Jain', 'affiliations': [], 'homepage': None, 'paperCount': 95, 'citationCount': 640}, {'authorId': '2056901064', 'url': 'https://www.semanticscholar.org/author/2056901064', 'name': 'Arthur Pitzer Caetano', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 12}, {'authorId': '47729073', 'url': 'https://www.semanticscholar.org/author/47729073', 'name': 'A. Calvo', 'affiliations': [], 'homepage': None, 'paperCount': 24, 'citationCount': 373}, {'authorId': '33184250', 'url': 'https://www.semanticscholar.org/author/33184250', 'name': 'Erwin Hilton', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 12}, {'authorId': '1729321', 'url': 'https://www.semanticscholar.org/author/1729321', 'name': 'C. Schmandt', 'affiliations': [], 'homepage': None, 'paperCount': 132, 'citationCount': 5206}]}",12.0,"{'MAG': '2535337559', 'DBLP': 'conf/uist/SraJCCHS16', 'DOI': '10.1145/2984751.2984779', 'CorpusId': 6140975}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",3.0,Resolving Spatial Variation And Allowing Spectator Participation In Multiplayer VR,https://www.semanticscholar.org/paper/782ebc83e8820ebb33bc4948cfb01d528c1eec26,UIST,2016
2538096772,"Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.",0.0,"Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.","['Keio Univ., Yokohama, Japan#TAB#']",['2505732502'],2538096772.0,"{'offset': 0, 'data': [{'authorId': '3427883', 'url': 'https://www.semanticscholar.org/author/3427883', 'name': 'Y. S. Pai', 'affiliations': [], 'homepage': None, 'paperCount': 62, 'citationCount': 277}]}",4.0,"{'DBLP': 'conf/uist/Pai16', 'MAG': '2538096772', 'DOI': '10.1145/2984751.2984787', 'CorpusId': 17858362}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['Book', 'JournalArticle', 'Conference']",10.0,Physiological Signal-Driven Virtual Reality in Social Spaces,https://www.semanticscholar.org/paper/5c8062b6f795a76140f74107c04a7770a8f3616a,UIST,2016
2539400915,"We demonstrate a new approach for designing functional material definitions for multi-material fabrication using our system called Foundry. Foundry provides an interactive and visual process for hierarchically designing spatially-varying material properties (e.g., appearance, mechanical, optical). The resulting meta-materials exhibit structure at the micro and macro level and can surpass the qualities of traditional composites. The material definitions are created by composing a set of operators into an operator graph. Each operator performs a volume decomposition operation, remaps space, or constructs and assigns a material composition. The operators are implemented using a domain-specific language for multi-material fabrication; users can easily extend the library by writing their own operators. Foundry can be used to build operator graphs that describe complex, parameterized, resolution-independent, and reusable material definitions. We also describe how to stage the evaluation of the final material definition which in conjunction with progressive refinement, allows for interactive material evaluation even for complex designs. We show sophisticated and functional parts designed with our system.",0.0,"We demonstrate a new approach for designing functional material definitions for multi-material fabrication using our system called Foundry. Foundry provides an interactive and visual process for hierarchically designing spatially-varying material properties (e.g., appearance, mechanical, optical). The resulting meta-materials exhibit structure at the micro and macro level and can surpass the qualities of traditional composites. The material definitions are created by composing a set of operators into an operator graph. Each operator performs a volume decomposition operation, remaps space, or constructs and assigns a material composition. The operators are implemented using a domain-specific language for multi-material fabrication; users can easily extend the library by writing their own operators. Foundry can be used to build operator graphs that describe complex, parameterized, resolution-independent, and reusable material definitions. We also describe how to stage the evaluation of the final material definition which in conjunction with progressive refinement, allows for interactive material evaluation even for complex designs. We show sophisticated and functional parts designed with our system.","['Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA']","['122968414', '2231832276', '2471472118', '2776395743']",2539400915.0,"{'offset': 0, 'data': [{'authorId': '2891782', 'url': 'https://www.semanticscholar.org/author/2891782', 'name': 'Kiril Vidimce', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 894}, {'authorId': '2694281', 'url': 'https://www.semanticscholar.org/author/2694281', 'name': 'Alexandre Kaspar', 'affiliations': [], 'homepage': None, 'paperCount': 17, 'citationCount': 355}, {'authorId': '2115738616', 'url': 'https://www.semanticscholar.org/author/2115738616', 'name': 'Ye Wang', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 42}, {'authorId': '1752521', 'url': 'https://www.semanticscholar.org/author/1752521', 'name': 'W. Matusik', 'affiliations': [], 'homepage': None, 'paperCount': 295, 'citationCount': 14670}]}",42.0,"{'DBLP': 'conf/uist/VidimceKWM16', 'MAG': '2539400915', 'DOI': '10.1145/2984511.2984516', 'CorpusId': 207242786}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",54.0,Foundry: Hierarchical Material Design for Multi-Material Fabrication,https://www.semanticscholar.org/paper/ddbb050e10ede9ec9748691dd8dfb56a237b64ec,UIST,2016
2765259813,"Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use.",0.0,"Analog circuit design is a complex, error-prone task in which the processes of gathering observations, formulating reasonable hypotheses, and manually adjusting the circuit raise significant barriers to an iterative workflow. We present Scanalog, a tool built on programmable analog hardware that enables users to rapidly explore different circuit designs using direct manipulation, and receive immediate feedback on the resulting behaviors without manual assembly, calculation, or probing. Users can interactively tune modular signal transformations on hardware with real inputs, while observing real-time changes at all points in the circuit. They can create custom unit tests and assertions to detect potential issues. We describe three interactive applications demonstrating the expressive potential of Scanalog. In an informal evaluation, users successfully conditioned analog sensors and described Scanalog as both enjoyable and easy to use.","['Stanford Univ., Palo Alto, CA, USA', 'Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)']","['2069682576', '2536959262', '718039462']",2765259813.0,"{'offset': 0, 'data': [{'authorId': '3492590', 'url': 'https://www.semanticscholar.org/author/3492590', 'name': 'Evan Strasnick', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 182}, {'authorId': '1820412', 'url': 'https://www.semanticscholar.org/author/1820412', 'name': 'Maneesh Agrawala', 'affiliations': [], 'homepage': None, 'paperCount': 230, 'citationCount': 13867}, {'authorId': '2770912', 'url': 'https://www.semanticscholar.org/author/2770912', 'name': 'S. Follmer', 'affiliations': [], 'homepage': None, 'paperCount': 122, 'citationCount': 3908}]}",32.0,"{'DBLP': 'conf/uist/StrasnickAF17', 'MAG': '2765259813', 'DOI': '10.1145/3126594.3126618', 'CorpusId': 23660939}",['Computer Science'],2.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['JournalArticle', 'Book', 'Conference']",18.0,Scanalog: Interactive Design and Debugging of Analog Circuits with Programmable Hardware,https://www.semanticscholar.org/paper/f7b6e11ea4408a2b7bb8a56075f9a01fd9ae6378,UIST,2017
2766198205,"Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user's onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector's pointing performance was compared to today's major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.",0.0,"Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user's onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector's pointing performance was compared to today's major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.","['Korea Advanced Institute of Science and Technology Daejeon Republic of Korea.', 'Microsoft Res., Beijing, China', 'Korea Advanced Institute of Science and Technology Daejeon Republic of Korea.', 'Korea Advanced Institute of Science and Technology Daejeon Republic of Korea.']","['2130488905', '2884029373', '2920926560', '3037898020']",2766198205.0,"{'offset': 0, 'data': [{'authorId': '2144694535', 'url': 'https://www.semanticscholar.org/author/2144694535', 'name': 'Jong-In Lee', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 40}, {'authorId': '2072144', 'url': 'https://www.semanticscholar.org/author/2072144', 'name': 'Sunjun Kim', 'affiliations': [], 'homepage': None, 'paperCount': 39, 'citationCount': 477}, {'authorId': '2603520', 'url': 'https://www.semanticscholar.org/author/2603520', 'name': 'M. Fukumoto', 'affiliations': [], 'homepage': None, 'paperCount': 65, 'citationCount': 1382}, {'authorId': '40216478', 'url': 'https://www.semanticscholar.org/author/40216478', 'name': 'Byungjoo Lee', 'affiliations': [], 'homepage': None, 'paperCount': 39, 'citationCount': 330}]}",5.0,"{'DBLP': 'conf/uist/LeeKFL17', 'MAG': '2766198205', 'DOI': '10.1145/3126594.3126665', 'CorpusId': 13794493}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",75.0,"Reflector: Distance-Independent, Private Pointing on a Reflective Screen",https://www.semanticscholar.org/paper/184a3d032a2f011acf07dd5aa443151a0f8b19aa,UIST,2017
2766538591,"Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world. This unique feature bridges the gap between virtuality and reality and largely elevates the user experience. Unfortunately, the current solution only performs well if the virtual contents complement the real scene. It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design. We present a novel system, called emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality. Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene. We also demonstrate emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.",0.0,"Due to the development of 3D sensing and modeling techniques, the state-of-the-art mixed reality devices such as Microsoft Hololens have the ability of digitalizing the physical world. This unique feature bridges the gap between virtuality and reality and largely elevates the user experience. Unfortunately, the current solution only performs well if the virtual contents complement the real scene. It can easily cause visual artifacts when the reality needs to be modified due to the virtuality (e.g., remove real objects to offer more space for virtual objects), a common scenario in mixed reality applications such as room redecoration and environment design. We present a novel system, called emph{SceneCtrl}, that allows the user to interactively edit the real scene sensed by Hololens, such that the reality can be adapted to suit virtuality. Our proof-of-concept prototype employs scene reconstruction and understanding to enable efficient editing such as deleting, moving, and copying real objects in the scene. We also demonstrate emph{SceneCtrl} on a number of example scenarios in mixed reality, verifying the enhanced experience by resolving conflicts between virtuality and reality.","['The University of Hong Kong, Hong Kong, HONG KONG', 'University of Bath, Bath, Avon, United Kingdom#TAB#', 'The University of Hong Kong - Hong Kong, China', 'Xiamen University of Technology, Xiamen, Fujian, China#TAB#']","['2118099990', '2566166521', '2611187615', '2671588202']",2766538591.0,"{'offset': 0, 'data': [{'authorId': '10671516', 'url': 'https://www.semanticscholar.org/author/10671516', 'name': 'Y. Yue', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 60}, {'authorId': '6635795', 'url': 'https://www.semanticscholar.org/author/6635795', 'name': 'Yong-Liang Yang', 'affiliations': [], 'homepage': None, 'paperCount': 60, 'citationCount': 2141}, {'authorId': '1752870663', 'url': 'https://www.semanticscholar.org/author/1752870663', 'name': 'Gang Ren', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 60}, {'authorId': '49336608', 'url': 'https://www.semanticscholar.org/author/49336608', 'name': 'Wenping Wang', 'affiliations': [], 'homepage': None, 'paperCount': 263, 'citationCount': 7129}]}",21.0,"{'DBLP': 'conf/uist/YueYRW17', 'MAG': '2766538591', 'DOI': '10.1145/3126594.3126601', 'CorpusId': 12582060}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",40.0,SceneCtrl: Mixed Reality Enhancement via Efficient Scene Editing,https://www.semanticscholar.org/paper/be88206b9a4cd624606b3fff6e5fef971612df43,UIST,2017
2897263022,"This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",0.0,"This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.","['University of Colorado Boulder, Boulder, CO, USA', 'University of Colorado Boulder, Boulder, CO, USA', 'The University of Tokyo, Tokyo,  Japan;', 'University of Colorado Boulder, Boulder, CO, USA', 'The University of Tokyo, Tokyo,  Japan;', 'University of Colorado Boulder, Boulder, CO, USA', 'The University of Tokyo, Tokyo,  Japan;']","['1647524748', '2027929530', '2093298323', '2104162850', '2154162571', '2155651480', '2491773333']",2897263022.0,"{'offset': 0, 'data': [{'authorId': '1665179454', 'url': 'https://www.semanticscholar.org/author/1665179454', 'name': 'R. Suzuki', 'affiliations': [], 'homepage': None, 'paperCount': 40, 'citationCount': 372}, {'authorId': '1665022856', 'url': 'https://www.semanticscholar.org/author/1665022856', 'name': 'Junichi Yamaoka', 'affiliations': [], 'homepage': None, 'paperCount': 34, 'citationCount': 246}, {'authorId': '3136322', 'url': 'https://www.semanticscholar.org/author/3136322', 'name': 'Daniel Leithinger', 'affiliations': [], 'homepage': None, 'paperCount': 57, 'citationCount': 1968}, {'authorId': '1704158', 'url': 'https://www.semanticscholar.org/author/1704158', 'name': 'Tom Yeh', 'affiliations': [], 'homepage': None, 'paperCount': 61, 'citationCount': 1153}, {'authorId': '1700028', 'url': 'https://www.semanticscholar.org/author/1700028', 'name': 'M. Gross', 'affiliations': [], 'homepage': None, 'paperCount': 206, 'citationCount': 4637}, {'authorId': '1773545', 'url': 'https://www.semanticscholar.org/author/1773545', 'name': 'Y. Kawahara', 'affiliations': [], 'homepage': None, 'paperCount': 320, 'citationCount': 2811}, {'authorId': '1755682', 'url': 'https://www.semanticscholar.org/author/1755682', 'name': 'Y. Kakehi', 'affiliations': [], 'homepage': None, 'paperCount': 124, 'citationCount': 1218}]}",25.0,"{'DBLP': 'conf/uist/SuzukiYLYGKK18', 'MAG': '2897263022', 'DOI': '10.1145/3242587.3242659', 'CorpusId': 52981311}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology'},10/11/2018,"['Book', 'JournalArticle', 'Conference']",61.0,Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation,https://www.semanticscholar.org/paper/e0eb13abacd055cb3a577bef14b4077a322d93a8,UIST,2018
2897976513,"The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.",1.0,"The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.","['National Taiwan Univ., Taipei, Taiwan ROC#TAB#', 'National Chiao-Tung University, Hsinchu, Taiwan Roc', 'National Chiao Tung University, Taipei, Taiwan Roc', 'National Taiwan Univ., Taipei, Taiwan ROC#TAB#', '[National Taiwan Univ. of Sci. & Tech., Taipei, Taiwan (ROC)]', 'National Taiwan Univ., Taipei, Taiwan ROC#TAB#', 'National Taiwan Univ., Taipei, Taiwan ROC#TAB#']","['2139482179', '2146507359', '2151954868', '2766349606', '2896127149', '2897304637', '2897786766']",2897976513.0,"{'offset': 0, 'data': [{'authorId': '9935485', 'url': 'https://www.semanticscholar.org/author/9935485', 'name': 'Shan-Yuan Teng', 'affiliations': [], 'homepage': None, 'paperCount': 26, 'citationCount': 252}, {'authorId': '2054323555', 'url': 'https://www.semanticscholar.org/author/2054323555', 'name': 'Tzu-Sheng Kuo', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 131}, {'authorId': '3227715', 'url': 'https://www.semanticscholar.org/author/3227715', 'name': 'Chi Wang', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 115}, {'authorId': '48023673', 'url': 'https://www.semanticscholar.org/author/48023673', 'name': 'C. Chiang', 'affiliations': [], 'homepage': None, 'paperCount': 32, 'citationCount': 243}, {'authorId': '3310020', 'url': 'https://www.semanticscholar.org/author/3310020', 'name': 'Da-Yuan Huang', 'affiliations': [], 'homepage': None, 'paperCount': 51, 'citationCount': 601}, {'authorId': '3212475', 'url': 'https://www.semanticscholar.org/author/3212475', 'name': 'Liwei Chan', 'affiliations': [], 'homepage': None, 'paperCount': 50, 'citationCount': 636}, {'authorId': '1733344', 'url': 'https://www.semanticscholar.org/author/1733344', 'name': 'Bing-Yu Chen', 'affiliations': [], 'homepage': None, 'paperCount': 262, 'citationCount': 3639}]}",46.0,"{'MAG': '2897976513', 'DBLP': 'conf/uist/TengKWCHCC18', 'DOI': '10.1145/3242587.3242628', 'CorpusId': 52981726}",['Computer Science'],4.0,False,{'name': 'Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology'},10/11/2018,"['JournalArticle', 'Book', 'Conference']",48.0,PuPoP: Pop-up Prop on Palm for Virtual Reality,https://www.semanticscholar.org/paper/a0d85f81284a1855c5cf39e4ceffad05608bee09,UIST,2018
1994760467,"Due to of their intuitive usage especially for novice users, graphical user interfaces (GUI) are nowadays a widespread user frontend for almost any kind of application. It is wellknown that the advantages to sighted users hide strong drawbacks for the community of blind people. Their special needs are not very well catered for the common software design. The control over GUI applications with their overlapping windows and buttons are no analog to the way blind people “see” their environment as it is for sighted people. Thus, the competitiveness of these p?ople is drastically reduced. The basic goal of HyperBraille is to enable blind or visually impaired people to participate as fully competitive members in today’s information technology oriented office worlds. We did not aim to create another tool to access graphical user interfaces but rather decided to realize a textscreen-oriented application especially for blind people which integrates tools to retrieve, create and exchange printed as well as electronic documents. Thereby we used the hypertext and formatting features of the Hypertext Markup Language HTML. On the other hand we adapted the GUI concept of the pulldot.vn menus to be customized on a Braille display. As for the sighted user, pull-down menus allow the novice user to immediately operate any application like word-processors or WWW-browsers without knowing the various key bindings. The development of HyperBraille started three years ago with the construction of a World Wide Web client that allowed easy access to all the documents of the Web[l 1]. This article will describe the new features of HyperBraille that are mostly driven by user feedback and by the needs for individual configurations of potential users, Permission to make digital/trard copies of all or part of tbk material for personal or clasaroom use is granted without fee provided that the copies are not made or dkibuted for profit or commercial advantage, the copyright notice, the title of tie publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @1996 ACM 0-89791-798-7/96/11 ..$3.50",1.0,"Due to of their intuitive usage especially for novice users, graphical user interfaces (GUI) are nowadays a widespread user frontend for almost any kind of application. It is wellknown that the advantages to sighted users hide strong drawbacks for the community of blind people. Their special needs are not very well catered for the common software design. The control over GUI applications with their overlapping windows and buttons are no analog to the way blind people “see” their environment as it is for sighted people. Thus, the competitiveness of these p?ople is drastically reduced. The basic goal of HyperBraille is to enable blind or visually impaired people to participate as fully competitive members in today’s information technology oriented office worlds. We did not aim to create another tool to access graphical user interfaces but rather decided to realize a textscreen-oriented application especially for blind people which integrates tools to retrieve, create and exchange printed as well as electronic documents. Thereby we used the hypertext and formatting features of the Hypertext Markup Language HTML. On the other hand we adapted the GUI concept of the pulldot.vn menus to be customized on a Braille display. As for the sighted user, pull-down menus allow the novice user to immediately operate any application like word-processors or WWW-browsers without knowing the various key bindings. The development of HyperBraille started three years ago with the construction of a World Wide Web client that allowed easy access to all the documents of the Web[l 1]. This article will describe the new features of HyperBraille that are mostly driven by user feedback and by the needs for individual configurations of potential users, Permission to make digital/trard copies of all or part of tbk material for personal or clasaroom use is granted without fee provided that the copies are not made or dkibuted for profit or commercial advantage, the copyright notice, the title of tie publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @1996 ACM 0-89791-798-7/96/11 ..$3.50","['German Research Center for Artificial Intelligence - DFKI GmbH, P.O.Box 2080, 67608 Kaiserslautern, Germany']",['2022169665'],1994760467.0,"{'offset': 0, 'data': [{'authorId': '3320897', 'url': 'https://www.semanticscholar.org/author/3320897', 'name': 'T. Kieninger', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 674}]}",12.0,"{'DBLP': 'conf/uist/Kieninger96', 'MAG': '1994760467', 'DOI': '10.1145/237091.237100', 'CorpusId': 1866896}",['Computer Science'],0.0,False,{'pages': '67-73'},11/1/1996,['JournalArticle'],18.0,The “growing up” of HyperBraille—an office workspace for blind people,https://www.semanticscholar.org/paper/a5d831ae67e1801da4df1ea0f73c7bbb3fc03590,UIST,1996
2153041427,"An important aspect of making the Web accessible to blind users is ensuring that all important web page elements such as links, clickable buttons, and form fields have explicitly assigned labels. Properly labeled content is then correctly read out by screen readers, a dominant assistive technology used by blind users. In particular, improperly labeled form fields can critically impede online transactions such as shopping, paying bills, etc. with screen readers. Very often labels are not associated with form fields or are missing altogether, making form filling a challenge for blind users. Algorithms for associating a form element with one of several candidate labels in its vicinity must cope with the variability of the element's features including label's location relative to the element, distance to the element, etc. Probabilistic models provide a natural machinery to reason with such uncertainties. In this paper we present a Finite Mixture Model (FMM) formulation of the label association problem. The variability of feature values are captured in the FMM by a mixture of random variables that are drawn from parameterized distributions. Then, the most likely label to be paired with a form element is computed by maximizing the log-likelihood of the feature data using the Expectation-Maximization algorithm. We also adapt the FMM approach for two related problems: assigning labels (from an external Knowledge Base) to form elements that have no candidate labels in their vicinity and for quickly identifying clickable elements such as add-to-cart, checkout, etc., used in online transactions even when these elements do not have textual captions (e.g., image buttons w/o alternative text). We provide a quantitative evaluation of our techniques, as well as a user study with two blind subjects who used an aural web browser implementing our approach.",1.0,"An important aspect of making the Web accessible to blind users is ensuring that all important web page elements such as links, clickable buttons, and form fields have explicitly assigned labels. Properly labeled content is then correctly read out by screen readers, a dominant assistive technology used by blind users. In particular, improperly labeled form fields can critically impede online transactions such as shopping, paying bills, etc. with screen readers. Very often labels are not associated with form fields or are missing altogether, making form filling a challenge for blind users. Algorithms for associating a form element with one of several candidate labels in its vicinity must cope with the variability of the element's features including label's location relative to the element, distance to the element, etc. Probabilistic models provide a natural machinery to reason with such uncertainties. In this paper we present a Finite Mixture Model (FMM) formulation of the label association problem. The variability of feature values are captured in the FMM by a mixture of random variables that are drawn from parameterized distributions. Then, the most likely label to be paired with a form element is computed by maximizing the log-likelihood of the feature data using the Expectation-Maximization algorithm. We also adapt the FMM approach for two related problems: assigning labels (from an external Knowledge Base) to form elements that have no candidate labels in their vicinity and for quickly identifying clickable elements such as add-to-cart, checkout, etc., used in online transactions even when these elements do not have textual captions (e.g., image buttons w/o alternative text). We provide a quantitative evaluation of our techniques, as well as a user study with two blind subjects who used an aural web browser implementing our approach.","['Stony Brook University, Stony Brook , NY, USA', 'Stony Brook University, Stony Brook , NY, USA', 'Stony Brook University, Stony Brook , NY, USA']","['1990835124', '2121268056', '2164224863']",2153041427.0,"{'offset': 0, 'data': [{'authorId': '3020758', 'url': 'https://www.semanticscholar.org/author/3020758', 'name': 'M. Islam', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 117}, {'authorId': '2133004', 'url': 'https://www.semanticscholar.org/author/2133004', 'name': 'Y. Borodin', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 942}, {'authorId': '145761288', 'url': 'https://www.semanticscholar.org/author/145761288', 'name': 'I. Ramakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 253, 'citationCount': 4644}]}",7.0,"{'DBLP': 'conf/uist/IslamBR10', 'MAG': '2153041427', 'DOI': '10.1145/1866029.1866041', 'CorpusId': 3221404}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 23nd annual ACM symposium on User interface software and technology'},10/3/2010,"['Book', 'JournalArticle', 'Conference']",28.0,Mixture model based label association techniques for web accessibility,https://www.semanticscholar.org/paper/12540e76907f0613a60f0e87859417b83a467bfb,UIST,2010
2293405979,"We introduce a new genre of user interface applications that can migrate from one machine to another, taking their user interface and application contexts with them, and continue from where they left off. Such applications are not tied to one user or one machine, and can roam freely over the network, rendering service to a community of users, gathering human input and interacting with people. We envisage that this will support many new agent-based collaboration metaphors. The ability to migrate executing programs has applicability to mobile computing as well. Users can have their applications travel with them, as they move from one computing environment to another. We present an elegant programming model for creating migratory applications and describe an implementation. The biggest strength of our implementation is that the details of migration are completely hidden from the application programmer; arbitrary user interface applications can be migrated by a single ÒmigrationÓ command. We address system issues such as robustness, persistence and memory usage, and also human factors relating to application design, the interaction metaphor and safety.",1.0,"We introduce a new genre of user interface applications that can migrate from one machine to another, taking their user interface and application contexts with them, and continue from where they left off. Such applications are not tied to one user or one machine, and can roam freely over the network, rendering service to a community of users, gathering human input and interacting with people. We envisage that this will support many new agent-based collaboration metaphors. The ability to migrate executing programs has applicability to mobile computing as well. Users can have their applications travel with them, as they move from one computing environment to another. We present an elegant programming model for creating migratory applications and describe an implementation. The biggest strength of our implementation is that the details of migration are completely hidden from the application programmer; arbitrary user interface applications can be migrated by a single ÒmigrationÓ command. We address system issues such as robustness, persistence and memory usage, and also human factors relating to application design, the interaction metaphor and safety.","['Graphics, Visualization & Usability Center, College of Computing, Georgia Tech, Atlanta, GA#TAB#', 'Digital, Systems Research Center, 130, Lytton Avenue, Palo Alto, CA']","['2893140440', '331751290']",2293405979.0,"{'offset': 0, 'data': [{'authorId': '145845790', 'url': 'https://www.semanticscholar.org/author/145845790', 'name': 'K. Bharat', 'affiliations': [], 'homepage': None, 'paperCount': 38, 'citationCount': 3627}, {'authorId': '145457097', 'url': 'https://www.semanticscholar.org/author/145457097', 'name': 'L. Cardelli', 'affiliations': [], 'homepage': None, 'paperCount': 285, 'citationCount': 21107}]}",181.0,"{'DBLP': 'conf/mos/BharatC96', 'MAG': '2914350155', 'DOI': '10.1145/215585.215711', 'CorpusId': 1528287}",['Computer Science'],9.0,True,{'pages': '132-142'},12/1/1995,['JournalArticle'],54.0,Migratory applications,https://www.semanticscholar.org/paper/6a0be2b75111355b29329f076ac90b37b38cfbca,UIST,1995
2062249380,"There are a variety of potential uses for interactive spatial sound in human-computer interfaces, but hardware costs have made most of these applications impractical. Recently, however, single-chip digital signal processors have made real-time spatial audio an affordable possibility for many workstations. This paper describes an efficient spatialization technique and the associated computational requirements. Issues specific to the use of spatial audio in user interfaces are addressed. The paper also describes the design of a network server for spatial audio that can support a number of users at modest cost.",1.0,"There are a variety of potential uses for interactive spatial sound in human-computer interfaces, but hardware costs have made most of these applications impractical. Recently, however, single-chip digital signal processors have made real-time spatial audio an affordable possibility for many workstations. This paper describes an efficient spatialization technique and the associated computational requirements. Issues specific to the use of spatial audio in user interfaces are addressed. The paper also describes the design of a network server for spatial audio that can support a number of users at modest cost.",['#N#‡#N#Georgia Institute of Technology#N#'],['2150404212'],2062249380.0,"{'offset': 0, 'data': [{'authorId': '153290920', 'url': 'https://www.semanticscholar.org/author/153290920', 'name': 'D.A. Burgess', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 90}]}",55.0,"{'MAG': '2062249380', 'DBLP': 'conf/uist/Burgess92', 'DOI': '10.1145/142621.142628', 'CorpusId': 7413673}",['Computer Science'],4.0,True,{'pages': '53-59'},12/1/1992,['JournalArticle'],37.0,Techniques for low cost spatial audio,https://www.semanticscholar.org/paper/8329b352bd9ef39fecaa32be6c9d056e68c966bf,UIST,1992
2068483976,"The promise of Brain-Computer Interfaces (BCI) technology is to augment human capabilities by enabling people to interact with a computer through a conscious and spontaneous modulation of their brainwaves after a short training period. Indeed, by analyzing brain electrical activity online, several groups have designed brain-actuated systems that provide alternative channels for communication, entertainment and control. Thus, a person can write messages using a virtual keyboard on a computer screen and also browse the internet. Alternatively, subjects can operate simple computer games, or brain games, and interact with educational software. Researchers have also been able to train monkeys to move a computer cursor to desired targets and also to control a robot arm. Work with humans has shown that it is possible for them to move a cursor and even to drive a mobile robot between rooms in a house model. In this talk I will review the field of BCI, with a focus on non-invasive systems based on electroencephalogram (EEG) signals. I will also describe three brain-actuated applications we have developed: a virtual keyboard, a brain game, and a mobile robot (emulating a motorized wheelchair). Finally, we discuss current research directions we are pursuing in order to improve the performance and robustness of our BCI system, especially for real-time control of brain-actuated robots.",0.0,"The promise of Brain-Computer Interfaces (BCI) technology is to augment human capabilities by enabling people to interact with a computer through a conscious and spontaneous modulation of their brainwaves after a short training period. Indeed, by analyzing brain electrical activity online, several groups have designed brain-actuated systems that provide alternative channels for communication, entertainment and control. Thus, a person can write messages using a virtual keyboard on a computer screen and also browse the internet. Alternatively, subjects can operate simple computer games, or brain games, and interact with educational software. Researchers have also been able to train monkeys to move a computer cursor to desired targets and also to control a robot arm. Work with humans has shown that it is possible for them to move a cursor and even to drive a mobile robot between rooms in a house model. In this talk I will review the field of BCI, with a focus on non-invasive systems based on electroencephalogram (EEG) signals. I will also describe three brain-actuated applications we have developed: a virtual keyboard, a brain game, and a mobile robot (emulating a motorized wheelchair). Finally, we discuss current research directions we are pursuing in order to improve the performance and robustness of our BCI system, especially for real-time control of brain-actuated robots.","['Idiap Research Institute, , Martigny, Switzerland']",['2143138563'],2068483976.0,"{'offset': 0, 'data': [{'authorId': '1716694', 'url': 'https://www.semanticscholar.org/author/1716694', 'name': 'J. Millán', 'affiliations': [], 'homepage': None, 'paperCount': 377, 'citationCount': 14303}]}",6.0,"{'DBLP': 'conf/uist/Millan06', 'MAG': '2068483976', 'DOI': '10.1145/1166253.1166255', 'CorpusId': 6163696}",['Computer Science'],0.0,False,{'pages': '277-278'},10/15/2006,"['JournalArticle', 'Conference', 'Review']",1.0,Brain-computer interaction,https://www.semanticscholar.org/paper/fb1042fe51b8d2351d4ee8b30d736f9a9c0dbd7b,UIST,2006
2081606565,"In this paper, we describe a multimodal interface prototype system based on Dynamical Dialogue Model. This system not only integrates information of speech and gestures, but also controls the response timing in order to realize a smooth interaction between user and computer. Our approach consists of human-human dialogue analysis, and computational modeling of dialogue.",0.0,"In this paper, we describe a multimodal interface prototype system based on Dynamical Dialogue Model. This system not only integrates information of speech and gestures, but also controls the response timing in order to realize a smooth interaction between user and computer. Our approach consists of human-human dialogue analysis, and computational modeling of dialogue.","['Multimodal Functions Sharp Laboratory, Real World Computing Partnership in System Technology Development Center, SHARP Corporation, 1-9-2 Nakase, Mihama-ku, Chiba-shi, Chiba 261-8520, Japan', 'Multimodal Functions Sharp Laboratory, Real World Computing Partnership in System Technology Development Center, SHARP Corporation, 1-9-2 Nakase, Mihama-ku, Chiba-shi, Chiba 261-8520, Japan', 'Multimodal Functions Sharp Laboratory, Real World Computing Partnership in System Technology Development Center, SHARP Corporation, 1-9-2 Nakase, Mihama-ku, Chiba-shi, Chiba 261-8520, Japan', 'Multimodal Functions Sharp Laboratory, Real World Computing Partnership in System Technology Development Center, SHARP Corporation, 1-9-2 Nakase, Mihama-ku, Chiba-shi, Chiba 261-8520, Japan', 'Multimodal Functions Sharp Laboratory, Real World Computing Partnership in System Technology Development Center, SHARP Corporation, 1-9-2 Nakase, Mihama-ku, Chiba-shi, Chiba 261-8520, Japan']","['1996024940', '2629340686', '2632106881', '2974154106', '3142748084']",2081606565.0,"{'offset': 0, 'data': [{'authorId': '2000076', 'url': 'https://www.semanticscholar.org/author/2000076', 'name': 'Toshiro Mukai', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 44}, {'authorId': '2006078', 'url': 'https://www.semanticscholar.org/author/2006078', 'name': 'S. Seki', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 162}, {'authorId': '2056146607', 'url': 'https://www.semanticscholar.org/author/2056146607', 'name': 'M. Nakazawa', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 19}, {'authorId': '38605700', 'url': 'https://www.semanticscholar.org/author/38605700', 'name': 'K. Watanuki', 'affiliations': [], 'homepage': None, 'paperCount': 9, 'citationCount': 35}, {'authorId': '1400442872', 'url': 'https://www.semanticscholar.org/author/1400442872', 'name': 'H. Miyoshi', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 67}]}",1.0,"{'MAG': '1561394309', 'DBLP': 'conf/uist/MukaiSNWM99', 'DOI': '10.1145/320719.322586', 'CorpusId': 18849968}",['Computer Science'],0.0,False,{'pages': '69-70'},11/7/1999,['JournalArticle'],8.0,Multimodal agent interface based on dynamical dialogue model: MAICO: multimodal agent interface for communication,https://www.semanticscholar.org/paper/b754e9b66a57dd528f3eafa377990c01248919e1,UIST,1999
2125082607,"Communication is about people, not machines. But as firms and families alike spread out geographically, we rely increasingly on telecommunications tools to keep us “connected”. The challenge of such systems is to enable conversation between individuals without computational infrastructure getting in the way. This paper compares two speech-based communication systems, Phoneshell and Chatter, in how they deal with the keys to communication: proper names. Chatter, a conversational system using speech-recognition, improves upon the hierarchical nature of the touch-tone based Phoneshell by maintaining context and enabling use of anaphora. Proper names can present particular problems for speech recognizers, so an interface algorithm for reliable name specification by spelling is offered. Since individual letter recognition is non-robust, Chatter implicitly disambiguates strings of letters based on context. We hypothesize that the right interface can make faulty speech recognition as usable as TouchTones—even more so.",1.0,"Communication is about people, not machines. But as firms and families alike spread out geographically, we rely increasingly on telecommunications tools to keep us “connected”. The challenge of such systems is to enable conversation between individuals without computational infrastructure getting in the way. This paper compares two speech-based communication systems, Phoneshell and Chatter, in how they deal with the keys to communication: proper names. Chatter, a conversational system using speech-recognition, improves upon the hierarchical nature of the touch-tone based Phoneshell by maintaining context and enabling use of anaphora. Proper names can present particular problems for speech recognizers, so an interface algorithm for reliable name specification by spelling is offered. Since individual letter recognition is non-robust, Chatter implicitly disambiguates strings of letters based on context. We hypothesize that the right interface can make faulty speech recognition as usable as TouchTones—even more so.","['Speech Research Group, MIT Media Laboratory, 20 Ames St., Cambridge, MA#TAB#', 'Speech Research Group, MIT Media Laboratory, 20 Ames St., Cambridge, MA#TAB#']","['2042299866', '2742640827']",2125082607.0,"{'offset': 0, 'data': [{'authorId': '40464784', 'url': 'https://www.semanticscholar.org/author/40464784', 'name': 'Matthew Marx', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 421}, {'authorId': '1729321', 'url': 'https://www.semanticscholar.org/author/1729321', 'name': 'C. Schmandt', 'affiliations': [], 'homepage': None, 'paperCount': 132, 'citationCount': 5206}]}",32.0,"{'MAG': '2125082607', 'DBLP': 'conf/uist/MarxS94', 'DOI': '10.1145/192426.192439', 'CorpusId': 3208370}",['Computer Science'],3.0,False,{'pages': '29-37'},11/2/1994,['JournalArticle'],8.0,Putting people first: specifying proper names in speech interfaces,https://www.semanticscholar.org/paper/47e0ace805f3ba6bb53cac3f84108aeaec906b42,UIST,1994
1747887080,"OpenInterface Kernel is a lightweight open-source plat-form designed for supporting the effective prototyping of multimodal interactive systems. Iterative design of such applications requires the easy integration, replacement, interconnection or upgrade of components. OpenInterface provides a thin integration platform able to manage these key elements with little programming knowledge, and thus provide the research community a tool to fill the gap in the current support for multimodal applications implementation. The platform offers non-intrusive tools and techniques to assemble various modalities developed with different implementation technologies, while keeping a high level of performance of the integrated system.",0.0,"OpenInterface Kernel is a lightweight open-source plat-form designed for supporting the effective prototyping of multimodal interactive systems. Iterative design of such applications requires the easy integration, replacement, interconnection or upgrade of components. OpenInterface provides a thin integration platform able to manage these key elements with little programming knowledge, and thus provide the research community a tool to fill the gap in the current support for multimodal applications implementation. The platform offers non-intrusive tools and techniques to assemble various modalities developed with different implementation technologies, while keeping a high level of performance of the integrated system.","['', '', '']","['2121253530', '2284081945', '2310626402']",1747887080.0,"{'offset': 0, 'data': [{'authorId': '7340460', 'url': 'https://www.semanticscholar.org/author/7340460', 'name': 'Jean-Yves Lionel Lawson', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 328}, {'authorId': '1749130', 'url': 'https://www.semanticscholar.org/author/1749130', 'name': 'J. Vanderdonckt', 'affiliations': [], 'homepage': None, 'paperCount': 541, 'citationCount': 10024}, {'authorId': '1749943', 'url': 'https://www.semanticscholar.org/author/1749943', 'name': 'B. Macq', 'affiliations': [], 'homepage': None, 'paperCount': 552, 'citationCount': 11418}]}",9.0,"{'MAG': '1747887080', 'CorpusId': 60639075}",['Computer Science'],1.0,False,"{'name': '', 'volume': ''}",,,2.0,Rapid Prototyping of Multimodal Interactive Applications Based on Off-The-Shelf Heterogeneous Components,https://www.semanticscholar.org/paper/0d750c266cd33c26674a49933dc66b774855500e,UIST,2008
1963545996,"We present EverybodyLovesSketch, a gesture-based 3D curve sketching system for rapid ideation and visualization of 3D forms, aimed at a broad audience. We first analyze traditional perspective drawing in professional practice. We then design a system built upon the paradigm of ILoveSketch, a 3D curve drawing system for design professionals. The new system incorporates many interaction aspects of perspective drawing with judicious automation to enable novices with no perspective training to proficiently create 3D curve sketches. EverybodyLovesSketch supports a number of novel interactions: tick-based sketch plane selection, single view definition of arbitrary extrusion vectors, multiple extruded surface sketching, copy-and-project of 3D curves, freeform surface sketching, and an interactive perspective grid. Finally, we present a study involving 49 high school students (with no formal artistic training) who each learned and used the system over 11 days, which provides detailed insights into the popularity, power and usability of the various techniques, and shows our system to be easily learnt and effectively used, with broad appeal.",1.0,"We present EverybodyLovesSketch, a gesture-based 3D curve sketching system for rapid ideation and visualization of 3D forms, aimed at a broad audience. We first analyze traditional perspective drawing in professional practice. We then design a system built upon the paradigm of ILoveSketch, a 3D curve drawing system for design professionals. The new system incorporates many interaction aspects of perspective drawing with judicious automation to enable novices with no perspective training to proficiently create 3D curve sketches. EverybodyLovesSketch supports a number of novel interactions: tick-based sketch plane selection, single view definition of arbitrary extrusion vectors, multiple extruded surface sketching, copy-and-project of 3D curves, freeform surface sketching, and an interactive perspective grid. Finally, we present a study involving 49 high school students (with no formal artistic training) who each learned and used the system over 11 days, which provides detailed insights into the popularity, power and usability of the various techniques, and shows our system to be easily learnt and effectively used, with broad appeal.","['Univ. of Toronto, Toronto ON Canada', 'Univ. of Toronto, Toronto ON Canada', 'Univ. of Toronto, Toronto ON Canada']","['2112488153', '2130130894', '2151341577']",1963545996.0,"{'offset': 0, 'data': [{'authorId': '1715434', 'url': 'https://www.semanticscholar.org/author/1715434', 'name': 'Seok-Hyung Bae', 'affiliations': [], 'homepage': None, 'paperCount': 37, 'citationCount': 861}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}, {'authorId': '2109146482', 'url': 'https://www.semanticscholar.org/author/2109146482', 'name': 'K. Singh', 'affiliations': [], 'homepage': None, 'paperCount': 29, 'citationCount': 1048}]}",93.0,"{'MAG': '1963545996', 'DBLP': 'conf/uist/BaeBS09', 'DOI': '10.1145/1622176.1622189', 'CorpusId': 7399785}",['Computer Science'],3.0,False,{'pages': '59-68'},10/4/2009,['JournalArticle'],52.0,EverybodyLovesSketch: 3D sketching for a broader audience,https://www.semanticscholar.org/paper/b334403c02de53b3d6b5cb0879cd93487ef36ffc,UIST,2009
1963822838,"Serial periodic data exhibit both serial and periodic properties. For example, time continues forward serially, but weeks, months, and years are periods that recur. While there are extensive visualization techniques for exploring serial data, and a few for exploring periodic data, no existing technique simultaneously displays serial and periodic attributes of a data set. We introduce a spiral visualization technique, which displays data along a spiral to highlight serial attributes along the spiral axis and periodic ones along the radii. We show several applications of the spiral visualization to data exploration tasks, present our implementation, discuss the capacity for data analysis, and present findings of our informal study with users in data-rich scientific domains.",1.0,"Serial periodic data exhibit both serial and periodic properties. For example, time continues forward serially, but weeks, months, and years are periods that recur. While there are extensive visualization techniques for exploring serial data, and a few for exploring periodic data, no existing technique simultaneously displays serial and periodic attributes of a data set. We introduce a spiral visualization technique, which displays data along a spiral to highlight serial attributes along the spiral axis and periodic ones along the radii. We show several applications of the spiral visualization to data exploration tasks, present our implementation, discuss the capacity for data analysis, and present findings of our informal study with users in data-rich scientific domains.","['[Dept. of Computer Science & Engineering University of Minnesota Minneapolis, MN]', '[Dept. of Computer Science & Engineering University of Minnesota Minneapolis, MN]']","['2002483998', '42252880']",1963822838.0,"{'offset': 0, 'data': [{'authorId': '1977396', 'url': 'https://www.semanticscholar.org/author/1977396', 'name': 'J. Carlis', 'affiliations': [], 'homepage': None, 'paperCount': 127, 'citationCount': 7128}, {'authorId': '2478310', 'url': 'https://www.semanticscholar.org/author/2478310', 'name': 'J. Konstan', 'affiliations': [], 'homepage': None, 'paperCount': 271, 'citationCount': 50483}]}",244.0,"{'MAG': '1963822838', 'DBLP': 'conf/uist/CarlisK98', 'DOI': '10.1145/288392.288399', 'CorpusId': 11951930}",['Computer Science'],8.0,True,{'pages': '29-38'},11/1/1998,['JournalArticle'],28.0,Interactive visualization of serial periodic data,https://www.semanticscholar.org/paper/e53b74de851f5ce5657fd5303c606dc1c2ec42c9,UIST,1998
1963959836,"We describe the Informediatm News-on-Demand system. News-on-Demand is an innovative example of indexing and searching broadcast video and audio material by text content. The fully-automatic system monitors TV news and allows selective retrieval of news items based on spoken queries. The user then plays the appropriate video ""paragraph"". The system runs on a Pentium PC using MPEG-I video compression and the Sphinx-II continuous speech recognition system [6].",1.0,"We describe the Informediatm News-on-Demand system. News-on-Demand is an innovative example of indexing and searching broadcast video and audio material by text content. The fully-automatic system monitors TV news and allows selective retrieval of news items based on spoken queries. The user then plays the appropriate video ""paragraph"". The system runs on a Pentium PC using MPEG-I video compression and the Sphinx-II continuous speech recognition system [6].","['Carnegie Mellon University, School of Computer Science, 5000 Forbes Avenue, Pittsburgh, PA#TAB#', 'Carnegie Mellon University, School of Computer Science, 5000 Forbes Avenue, Pittsburgh, PA#TAB#', 'Carnegie Mellon University, School of Computer Science, 5000 Forbes Avenue, Pittsburgh, PA#TAB#']","['17920550', '2113269357', '2724598099']",1963959836.0,"{'offset': 0, 'data': [{'authorId': '7661726', 'url': 'https://www.semanticscholar.org/author/7661726', 'name': 'Alexander Hauptmann', 'affiliations': [], 'homepage': None, 'paperCount': 538, 'citationCount': 22665}, {'authorId': '2819135', 'url': 'https://www.semanticscholar.org/author/2819135', 'name': 'M. Witbrock', 'affiliations': [], 'homepage': None, 'paperCount': 95, 'citationCount': 3189}, {'authorId': '1783635', 'url': 'https://www.semanticscholar.org/author/1783635', 'name': 'Alexander I. Rudnicky', 'affiliations': [], 'homepage': None, 'paperCount': 255, 'citationCount': 7019}]}",23.0,"{'DBLP': 'conf/uist/HauptmannWR95', 'MAG': '1963959836', 'DOI': '10.1145/215585.215667', 'CorpusId': 207193409}",['Computer Science'],1.0,True,{'pages': '79-80'},12/1/1995,['JournalArticle'],5.0,Speech for multimedia information retrieval,https://www.semanticscholar.org/paper/6d0449eaf2723275d25b91788f60a74c0a83222b,UIST,1995
1968251787,"Intelligent keyboards aid fast text entry by correcting user's erroneous input, but there is a big problem that a user always has to watch and judge of their suggestion results. Contelli, a user-controllable intelligent keyboard, monitors the duration of each key-tapping, and analyzes the possibility of mis-typing only for short-tapped letters. A long-tapped letter is regarded as a precise input and excluded in the process of candidate generation from a lexicon. Using Contelli, a user may actively ""control"" the intelligent keyboards. S/he may type ordinary words quickly on watch-sized small touchscreens. Also, s/he may input a word as typed without switching off the automatic replacement or performing additional actions for the replaced result. In addition, long-tapping a part of a string reduces the number of replacement candidates, which contributes the more precise word replacement for highly erroneous input typed on small touchscreens.",0.0,"Intelligent keyboards aid fast text entry by correcting user's erroneous input, but there is a big problem that a user always has to watch and judge of their suggestion results. Contelli, a user-controllable intelligent keyboard, monitors the duration of each key-tapping, and analyzes the possibility of mis-typing only for short-tapped letters. A long-tapped letter is regarded as a precise input and excluded in the process of candidate generation from a lexicon. Using Contelli, a user may actively ""control"" the intelligent keyboards. S/he may type ordinary words quickly on watch-sized small touchscreens. Also, s/he may input a word as typed without switching off the automatic replacement or performing additional actions for the replaced result. In addition, long-tapping a part of a string reduces the number of replacement candidates, which contributes the more precise word replacement for highly erroneous input typed on small touchscreens.","['Samsung Electronics Co., Ltd., Suwon-si, Gyeonggi-do, South Korea#TAB#', 'Samsung Electronics Co., Ltd., Suwon-si, Gyeonggi-do, South Korea#TAB#', 'Samsung Electronics Co., Ltd., Suwon-si, Gyeonggi-do, South Korea#TAB#']","['2223049232', '2233836842', '2397232401']",1968251787.0,"{'offset': 0, 'data': [{'authorId': '153354801', 'url': 'https://www.semanticscholar.org/author/153354801', 'name': 'Taik-Heon Rhee', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 142}, {'authorId': '2217431', 'url': 'https://www.semanticscholar.org/author/2217431', 'name': 'Kwangmin Byeon', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 0}, {'authorId': '2111392861', 'url': 'https://www.semanticscholar.org/author/2111392861', 'name': 'Hochul Shin', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 23}]}",0.0,"{'DBLP': 'conf/uist/RheeBS14', 'MAG': '1968251787', 'DOI': '10.1145/2658779.2658788', 'CorpusId': 16286858}",['Computer Science'],0.0,False,{'pages': '85-86'},10/5/2014,['JournalArticle'],7.0,Contelli: a user-controllable intelligent keyboard for watch-sized small touchscreens,https://www.semanticscholar.org/paper/b52c8932bf492a825c042965ed150ce384f7ec9a,UIST,2014
1968589407,"We explore the use of tracked 2D object motion to enable novel approaches to interacting with video. These include moving annotations, video navigation by direct manipulation of objects, and creating an image composite from multiple video frames. Features in the video are automatically tracked and grouped in an off-line preprocess that enables later interactive manipulation. Examples of annotations include speech and thought balloons, video graffiti, path arrows, video hyperlinks, and schematic storyboards. We also demonstrate a direct-manipulation interface for random frame access using spatial constraints, and a drag-and-drop interface for assembling still images from videos. Taken together, our tools can be employed in a variety of applications including film and video editing, visual tagging, and authoring rich media such as hyperlinked video.",1.0,"We explore the use of tracked 2D object motion to enable novel approaches to interacting with video. These include moving annotations, video navigation by direct manipulation of objects, and creating an image composite from multiple video frames. Features in the video are automatically tracked and grouped in an off-line preprocess that enables later interactive manipulation. Examples of annotations include speech and thought balloons, video graffiti, path arrows, video hyperlinks, and schematic storyboards. We also demonstrate a direct-manipulation interface for random frame access using spatial constraints, and a drag-and-drop interface for assembling still images from videos. Taken together, our tools can be employed in a variety of applications including film and video editing, visual tagging, and authoring rich media such as hyperlinked video.","['Adobe Systems, Inc., and University of Washington, Seattle, WA, USA#TAB#', ' Univ. of Washington, Seattle, WA, USA', 'Adobe Systems, Inc., Seattle, WA, USA#TAB#', ' Univ. of Washington, Seattle, WA, USA', ' Univ. of Washington, Seattle, WA, USA']","['2054111979', '2134777055', '2152336186', '2227005256', '511526681']",1968589407.0,"{'offset': 0, 'data': [{'authorId': '1976171', 'url': 'https://www.semanticscholar.org/author/1976171', 'name': 'Dan B. Goldman', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 6459}, {'authorId': '2660079', 'url': 'https://www.semanticscholar.org/author/2660079', 'name': 'Chris Gonterman', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 132}, {'authorId': '143800609', 'url': 'https://www.semanticscholar.org/author/143800609', 'name': 'B. Curless', 'affiliations': [], 'homepage': None, 'paperCount': 135, 'citationCount': 24154}, {'authorId': '1745260', 'url': 'https://www.semanticscholar.org/author/1745260', 'name': 'D. Salesin', 'affiliations': [], 'homepage': None, 'paperCount': 146, 'citationCount': 19060}, {'authorId': '1679223', 'url': 'https://www.semanticscholar.org/author/1679223', 'name': 'S. Seitz', 'affiliations': [], 'homepage': None, 'paperCount': 194, 'citationCount': 28122}]}",132.0,"{'MAG': '1968589407', 'DBLP': 'conf/uist/GoldmanGCSS08', 'DOI': '10.1145/1449715.1449719', 'CorpusId': 855614}",['Computer Science'],9.0,False,{'pages': '3-12'},10/19/2008,['JournalArticle'],31.0,"Video object annotation, navigation, and composition",https://www.semanticscholar.org/paper/a9b62222da1b447115cd74f351f42d80846660a0,UIST,2008
1971218461,"We introduce a set of techniques for haptically manipulating digital media such as video, audio, voicemail and computer graphics, utilizing virtual mediating dynamic models based on intuitive physical metaphors. For example, a video sequence can be modeled by linking its motion to a heavy spinning virtual wheel: the user browses by grasping a physical force-feedback knob and engaging the virtual wheel through a simulated clutch to spin or brake it, while feeling the passage of individual frames. These systems were implemented on a collection of single axis actuated displays (knobs and sliders), equipped with orthogonal force sensing to enhance their expressive potential. We demonstrate how continuous interaction through a haptically actuated device rather than discrete button and key presses can produce simple yet powerful tools that leverage physical intuition.",1.0,"We introduce a set of techniques for haptically manipulating digital media such as video, audio, voicemail and computer graphics, utilizing virtual mediating dynamic models based on intuitive physical metaphors. For example, a video sequence can be modeled by linking its motion to a heavy spinning virtual wheel: the user browses by grasping a physical force-feedback knob and engaging the virtual wheel through a simulated clutch to spin or brake it, while feeling the passage of individual frames. These systems were implemented on a collection of single axis actuated displays (knobs and sliders), equipped with orthogonal force sensing to enhance their expressive potential. We demonstrate how continuous interaction through a haptically actuated device rather than discrete button and key presses can produce simple yet powerful tools that leverage physical intuition.","['', '', 'San Francisco, CA', '', 'Univ. of British Columbia, Vancouver, B.C. V6T 1Z4 Canada#TAB#', '']","['1419199090', '1977264600', '1983653882', '2030478388', '2105254560', '2575841758']",1971218461.0,"{'offset': 0, 'data': [{'authorId': '3163335', 'url': 'https://www.semanticscholar.org/author/3163335', 'name': 'Scott S. Snibbe', 'affiliations': [], 'homepage': None, 'paperCount': 19, 'citationCount': 1362}, {'authorId': '1796517', 'url': 'https://www.semanticscholar.org/author/1796517', 'name': 'K. MacLean', 'affiliations': [], 'homepage': None, 'paperCount': 170, 'citationCount': 5580}, {'authorId': '2060344349', 'url': 'https://www.semanticscholar.org/author/2060344349', 'name': 'Robert Shaw', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 194}, {'authorId': '152778898', 'url': 'https://www.semanticscholar.org/author/152778898', 'name': 'J. Roderick', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 145}, {'authorId': '3344074', 'url': 'https://www.semanticscholar.org/author/3344074', 'name': 'B. Verplank', 'affiliations': [], 'homepage': None, 'paperCount': 29, 'citationCount': 1855}, {'authorId': '2565834', 'url': 'https://www.semanticscholar.org/author/2565834', 'name': 'M. Scheeff', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 305}]}",97.0,"{'MAG': '1971218461', 'DBLP': 'conf/uist/SnibbeMSRVS01', 'DOI': '10.1145/502348.502387', 'CorpusId': 10872534}",['Computer Science'],1.0,False,{'pages': '199-208'},11/11/2001,['JournalArticle'],23.0,Haptic techniques for media control,https://www.semanticscholar.org/paper/51c48bb110b077d5f07230ebcf4b468df2a0f9a4,UIST,2001
1971314683,"This paper proposes a distributed shared memory, Window Real-Object (WROL which facilitates the construction of GUI applications with a set of cooperating parallel units running on multiple machines. To support unit cooperation, the WRO provides a shared data structure storing interaction objects displayed in a window among multiple machines. It also provides an event mechanism called absfrac[ everm to support control transfer among these units. Abstract events are generated when the shared data structure is updated, and invoke the units sharing the WRo. Both the function and mechanism of the WRO are tailored for GUI applications. The WRO provides spatia~ addressing to the data structure, which is implemented efficiently using an R-tree. It also adopts aflli-replication algorithm as a shared-memory coherenee scheme. Consequently, the WRO can be implemented with adequate performance on a usual workstation environment.",1.0,"This paper proposes a distributed shared memory, Window Real-Object (WROL which facilitates the construction of GUI applications with a set of cooperating parallel units running on multiple machines. To support unit cooperation, the WRO provides a shared data structure storing interaction objects displayed in a window among multiple machines. It also provides an event mechanism called absfrac[ everm to support control transfer among these units. Abstract events are generated when the shared data structure is updated, and invoke the units sharing the WRo. Both the function and mechanism of the WRO are tailored for GUI applications. The WRO provides spatia~ addressing to the data structure, which is implemented efficiently using an R-tree. It also adopts aflli-replication algorithm as a shared-memory coherenee scheme. Consequently, the WRO can be implemented with adequate performance on a usual workstation environment.","['Department of Information Science, Faculty of Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113 Japan', 'Department of Information Science, Faculty of Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113 Japan']","['1978122496', '45280299']",1971314683.0,"{'offset': 0, 'data': [{'authorId': '2032849', 'url': 'https://www.semanticscholar.org/author/2032849', 'name': 'N. Koshizuka', 'affiliations': [], 'homepage': None, 'paperCount': 136, 'citationCount': 1311}, {'authorId': '1706494', 'url': 'https://www.semanticscholar.org/author/1706494', 'name': 'K. Sakamura', 'affiliations': [], 'homepage': None, 'paperCount': 311, 'citationCount': 2514}]}",10.0,"{'DBLP': 'conf/uist/KoshizukaS93', 'MAG': '1971314683', 'DOI': '10.1145/168642.168667', 'CorpusId': 3132680}",['Computer Science'],0.0,False,{'pages': '237-247'},12/1/1993,['JournalArticle'],35.0,Window real objects: a distributed shared memory for distributed implementation of GUI applications,https://www.semanticscholar.org/paper/d8e199110ec43ed93787aec0ffe54b015a94658a,UIST,1993
1973812891,"In this paper, we propose a novel stick-type interface, the ""HaCHIStick,"" for musical performance on a tablet PC. The HaCHIStick is composed of a stick with an embedded vibrotactile actuator, a visual display, and an elastic sheet on the display. By combining the kinesthetic sensation induced by striking the elastic sheet with vibrotactile sensation, the system provides natural haptic cues that enable the user to feel what they strike with the stick, such as steel or wood. This haptic interaction would enrich the user's experience when playing the instruments. The interface is regarded as a type of haptic augmented reality (AR) system, with a relatively simple setup.",0.0,"In this paper, we propose a novel stick-type interface, the ""HaCHIStick,"" for musical performance on a tablet PC. The HaCHIStick is composed of a stick with an embedded vibrotactile actuator, a visual display, and an elastic sheet on the display. By combining the kinesthetic sensation induced by striking the elastic sheet with vibrotactile sensation, the system provides natural haptic cues that enable the user to feel what they strike with the stick, such as steel or wood. This haptic interaction would enrich the user's experience when playing the instruments. The interface is regarded as a type of haptic augmented reality (AR) system, with a relatively simple setup.","['[The University of Electro-Communications, Tokyo, JAPAN]', 'The University of Electro-Commnications, Tokyo, Japan', '[The University of Electro-Communications, Tokyo, JAPAN]', '[The University of Electro-Communications, Tokyo, JAPAN]']","['1685525802', '1983207792', '2144845276', '2185535620']",1973812891.0,"{'offset': 0, 'data': [{'authorId': '3242743', 'url': 'https://www.semanticscholar.org/author/3242743', 'name': 'Taku Hachisu', 'affiliations': [], 'homepage': None, 'paperCount': 121, 'citationCount': 486}, {'authorId': '2494645', 'url': 'https://www.semanticscholar.org/author/2494645', 'name': 'Michi Sato', 'affiliations': [], 'homepage': None, 'paperCount': 53, 'citationCount': 334}, {'authorId': '143679569', 'url': 'https://www.semanticscholar.org/author/143679569', 'name': 'S. Fukushima', 'affiliations': [], 'homepage': None, 'paperCount': 124, 'citationCount': 616}, {'authorId': '1776927', 'url': 'https://www.semanticscholar.org/author/1776927', 'name': 'H. Kajimoto', 'affiliations': [], 'homepage': None, 'paperCount': 464, 'citationCount': 3604}]}",13.0,"{'DBLP': 'conf/uist/HachisuSFK11', 'MAG': '1973812891', 'DOI': '10.1145/2046396.2046429', 'CorpusId': 11499849}",['Computer Science'],0.0,False,{'pages': '73-74'},10/16/2011,['JournalArticle'],12.0,HaCHIStick: simulating haptic sensation on tablet pc for musical instruments application,https://www.semanticscholar.org/paper/e9f2b54b0b74777e16d542660005ef11c89b1357,UIST,2011
1983748693,"In this paper, we describe Augur, a groupware calendar system to support personal calendaring practices, informal workplace communication, and the socio-technical evolution of the calendar system within a workgroup. Successful design and deployment of groupware calendar systems have been shown to depend on several converging, interacting perspectives. We describe calendar-based work practices as viewed from these perspectives, and present the Augur system in support of them. Augur allows users to retain the flexibility of personal calendars by anticipating and compensating for inaccurate calendar entries and idiosyncratic event names. We employ predictive user models of event attendance, intelligent processing of calendar text, and discovery of shared events to drive novel calendar visualizations that facilitate interpersonal communication. In addition, we visualize calendar access to support privacy management and long-term evolution of the calendar system.",1.0,"In this paper, we describe Augur, a groupware calendar system to support personal calendaring practices, informal workplace communication, and the socio-technical evolution of the calendar system within a workgroup. Successful design and deployment of groupware calendar systems have been shown to depend on several converging, interacting perspectives. We describe calendar-based work practices as viewed from these perspectives, and present the Augur system in support of them. Augur allows users to retain the flexibility of personal calendars by anticipating and compensating for inaccurate calendar entries and idiosyncratic event names. We employ predictive user models of event attendance, intelligent processing of calendar text, and discovery of shared events to drive novel calendar visualizations that facilitate interpersonal communication. In addition, we visualize calendar access to support privacy management and long-term evolution of the calendar system.","['Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#', 'Georgia Tech. Atlanta, GA#TAB#']","['18171182', '2002910627', '2022470897', '2147352064']",1983748693.0,"{'offset': 0, 'data': [{'authorId': '3126077', 'url': 'https://www.semanticscholar.org/author/3126077', 'name': 'J. Tullio', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 1137}, {'authorId': '2718156', 'url': 'https://www.semanticscholar.org/author/2718156', 'name': 'J. Goecks', 'affiliations': [], 'homepage': None, 'paperCount': 91, 'citationCount': 8790}, {'authorId': '1752751', 'url': 'https://www.semanticscholar.org/author/1752751', 'name': 'Elizabeth D. Mynatt', 'affiliations': [], 'homepage': None, 'paperCount': 262, 'citationCount': 13114}, {'authorId': '50652544', 'url': 'https://www.semanticscholar.org/author/50652544', 'name': 'David H. Nguyen', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 1359}]}",76.0,"{'DBLP': 'conf/uist/TullioGMN02', 'MAG': '1983748693', 'DOI': '10.1145/571985.571988', 'CorpusId': 15933502}",['Computer Science'],2.0,False,{'pages': '11-20'},10/27/2002,['JournalArticle'],24.0,Augmenting shared personal calendars,https://www.semanticscholar.org/paper/2370006bd6289b693e9854882d68806c6e4c9407,UIST,2002
1986283786,"Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose LiveSphere, immersive experience sharing system with wearable camera headgear that provide 360 degrees spherical images of the user's surrounding environment. LiveSphere system performs spherical video stabilization and transmits it to other users, so that they are enable to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the LiveSphere system implementation, stabilization and viewing experience.",0.0,"Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose LiveSphere, immersive experience sharing system with wearable camera headgear that provide 360 degrees spherical images of the user's surrounding environment. LiveSphere system performs spherical video stabilization and transmits it to other users, so that they are enable to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the LiveSphere system implementation, stabilization and viewing experience.","['University of Tokyo / Sony CSL, Bunkyo-ku, Tokyo, Japan#TAB#', 'SonyCSL, Shinagawa, Tokyo, Japan', 'The University of Tokyo, Toyko, Japan']","['1994375393', '2136921316', '2228213928']",1986283786.0,"{'offset': 0, 'data': [{'authorId': '2622335', 'url': 'https://www.semanticscholar.org/author/2622335', 'name': 'Shunichi Kasahara', 'affiliations': [], 'homepage': None, 'paperCount': 44, 'citationCount': 676}, {'authorId': '2892489', 'url': 'https://www.semanticscholar.org/author/2892489', 'name': 'Shohei Nagai', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 97}, {'authorId': '1685962', 'url': 'https://www.semanticscholar.org/author/1685962', 'name': 'J. Rekimoto', 'affiliations': [], 'homepage': None, 'paperCount': 312, 'citationCount': 11594}]}",21.0,"{'DBLP': 'conf/uist/KasaharaNR14', 'MAG': '1986283786', 'DOI': '10.1145/2658779.2659114', 'CorpusId': 25242007}",['Computer Science'],0.0,False,{'pages': '61-62'},10/5/2014,"['JournalArticle', 'Review']",2.0,LiveSphere: immersive experience sharing with 360 degrees head-mounted cameras,https://www.semanticscholar.org/paper/341530b6e9f08d696520bedef585da57f960f574,UIST,2014
1991701776,"We propose a new evolutionary method of extracting user preferences from examples shown to an automatic graph layout system. Using stochastic methods such as simulated annealing and genetic algorithms, automatic layout systems can find a good layout using an evaluation function which can calculate how good a given layout is. However, the evaluation function is usually not known beforehand, and it might vary from user to user. In our system, users show the system several pairs of good and bad layout examples, and the system infers the evaluation function from the examples using genetic programming technique. After the evaluation function evolves to reflect the preferences of the user, it is used as a general evaluation function for laying out graphs. The same technique can be used for a wide range of adaptive user interface systems.",1.0,"We propose a new evolutionary method of extracting user preferences from examples shown to an automatic graph layout system. Using stochastic methods such as simulated annealing and genetic algorithms, automatic layout systems can find a good layout using an evaluation function which can calculate how good a given layout is. However, the evaluation function is usually not known beforehand, and it might vary from user to user. In our system, users show the system several pairs of good and bad layout examples, and the system infers the evaluation function from the examples using genetic programming technique. After the evaluation function evolves to reflect the preferences of the user, it is used as a general evaluation function for laying out graphs. The same technique can be used for a wide range of adaptive user interface systems.","['Software Research Laboratories, SHARP Corporation, 2613-1 Ichinomoto-cho, Tenri, Nara 632, Japan#TAB#']",['2917446821'],1991701776.0,"{'offset': 0, 'data': [{'authorId': '2133959', 'url': 'https://www.semanticscholar.org/author/2133959', 'name': 'T. Masui', 'affiliations': [], 'homepage': None, 'paperCount': 52, 'citationCount': 747}]}",40.0,"{'MAG': '1991701776', 'DBLP': 'conf/uist/Masui94', 'DOI': '10.1145/192426.192468', 'CorpusId': 5985277}",['Computer Science'],3.0,False,{'pages': '103-108'},11/2/1994,['JournalArticle'],28.0,Evolutionary learning of graph layout constraints from examples,https://www.semanticscholar.org/paper/1f4cd45724359ad2dd7aada42e3e0229df90a772,UIST,1994
1991798211,"This TechNote introduces WebCard, an integrated maillnews reader and Web browser. As a maillnews reader, WebCard is fairly conventional; the innovation is that Web pages are fully integrated in the mail/news reader. The user interface is based on folders, where an “item” in a folder can be a mail message, news article or Web page. When displaying a Web page, users can follow links, and the new pages will appear as items in the current folder. Users can copy and move items between folders, forward items, and can also use folders to organize material on the Web, such as hotlists, query results, and breadth-first expansions. INTRODUCTION As people are spending increasing amounts of time sending e-mail, reading news, and surfing the Web, an integrated user interface to the three activities is increasingly appealing. Leading-edge Web browsers are moving toward this goal by starting to include features for handling e-mail and news. For instance, Netscape (version 1.IN) [4] provides a polished interface for reading news and posting articles. Netscape, like most Web browsers, allows a user to send e-mail; however, no browsers support reading e-mail. Leading-edge mail readers are moving in the same direction, by starting to provide access to the Web. For example, Z -Mai 1 [5] allows e-mail to reference HTML documents as MIME attachments. The text part of the mail message is shown in the standard fashion, and clicking on the “attachment” button invokes a Web browser of the user’s choice that displays the attachment. Once the Web browser is running, it is completely independent of the mail reader, however. This TechNote introduces WebCard, a folder-based mail/news reader (think of it as a combination of xmh and xrn) that has been enhanced to handle Web pages using the same inter-face that it already uses for handling mail messages and newsgroup posting. Moreover, folders of Web pages proPennission to make digitrd/hnrd copies of all or part of this material for personal or classroom use is grrtntecl without fee provided thnt lhe copies are not made or distributed for profit or commercial advantage, the copyright notice, the tine of the puhlicatimt and its date appear, and notice is given that crrpy right ia by pennissiun of IIIC ACM. Inc. TO copy otftenvise. to republish, to post on sewers or 10 redistribute to lists, requires specific permission and/or fee. UIST 95 Pittsburgh PA USA @1995 ACM O-89791-709-x/95/l 1..$3.50 vide a general-purpose way to organize Web material, such as hotlists, query results, and breadth-first expansions. WEBCARD At first blush, WebCard is a conventional folder-based e-mail reader. At any given time there is an open folder, and one message from the open folder, called the selected message, is displayed in the display pane of the WebCard window. The user can respond to the selected message, forward it, copy or move it to another folder, and so on. (Actually, more than one message can be selected, but some operations, like ‘<Reply,” are only valid when there is a single selected message.) Users can also perform standard folder operations, such as creating, renaming, and deleting folders. WebCard handles newsgroups in the same way: a folder whose name refers to a newsgroup contains the postings in the newsgroup. Users can respond to a news article by using the “Reply” button, copy postings to folders, and so on. The only difference between a newsgroup folder and any other folder is that newsgroup folders are “read-only;” users cannot delete messages from a news folder or copy messages into a news folder. WebCard uses the name of the folder to distinguish between newsgroup folders and ordinary folders; a period in the folder name indicates a newsgroup. The Andrew Messages System [3] is the only other system we know of that deals uniformly with e-mail and bulletin boards. WebCard also integrates Web pages in the same way: folders can contain Web pages. Web pages are rendered just as they would be in a stand-alone graphical Web browser, and displayed in WebCard’s display pane where the contents of a message normally are displayed (textually). The subject of the Web page “message” is the “<TITLE>” field of the Web page. The only operation that doesn’t make sense for a Web page “message” is “Reply.” There are three ways to display a Web page: clicking on a URL that appears in the currently displayed mail message or news article, primary-selecting a URL (in any window on the screen) and then middle-clicking on the “URE’ button, and using the “Open URL7 dialog. Once a Web page is displayed. clicking on a link fetches the new page. Depending on keyboard modifiers, the new page either replaces the existing page or is added as a “message” in the current folder. November 14-17, 1995 UIST ’95 197 These screen dumps show WebCard in action. The top part of the WebCard window contains a browser listing the subjects of mail messages or news postings and titles of Web pages. The bottom part of the window displays the mail message, news posting or Web page. At the left, WebCard is displaying a URL that had been mentioned in a news article posted by a user named “websurfer”. That article had been copied into the inbox as message 10. The image at the right shows WebCard after clicking on the “Mr. Showbiz” icon, and then on a link to retrieve a review of the movie Dangerous Minds. M, Stww+,zw,l, do wha&,,,ake$ roauermk?~. Grmttnam,cs F,Rm m,-, m ,, w,,, ,Iw w“ uc-,c-!k-ml,nl, enfert.”.mi m m Ikbrl,, O@ ,1= ..”,, re”leva ,elwlwm Mix,. WebCard allows users to “Detach” items from the folder, and temporarily display them in a separate pane in the WebCard window. When a detached item is a Web page, the user can click on links and the new page is added to the folder and it appears back on the display pane rather than obscuring the detached page. Thus, the user can have a page, such as a table of contents or index, visible for an extended period, even while following another chain of links on the main body of the folder. Folders provide a convenient way for users to organize material. For example, a user can keep the home pages of all of his or her colleagues together in a folder named “Colleagues,” or keep several “hotlists”, each in its own folder. WebCard also uses folders to return the results of certain operations. For example, WebCard has an “Expand One Level” command which traverses every link on a particular page, and returns all resulting pages in a new folder. IMPLEMENTATION WebCard is implemented in Modula–3 [2], and makes use of two existing Modula–3 applications, Postcard and DeckScape. Postcard is a folder-based maillnews reader that has been in daily use at SRC since 1988. DeckScape[l] is an experimental Web browser based on the metaphor of a deck, collections of Web pages only one of which is visible at a time. The DeckScape display consists of multiple decks, all in a single top-level window. Users can move, resize or iconify decks, move or copy pages between decks, and so on. Decks in DeckScape serve the function of folders in WebCard. WebCard support HTML2.0, including inline images (rendered in black-and-white), forms, and active maps. WebCard does not support external viewers. CONCLUSION WebCard has introduced a new interaction technique for browsing the Web. It integrates e-mail, news, and Web browsing into a single user interface, thereby avoiding the contextswitching inherent when using independent applications. As a Web browser, WebCard supports folders, a flexible way to organize, browse, and store large numbers of documents. We do not claim that WebCard the correct way to browse the Web, to organize material on the Web, to integrate mail and news, or even to integrate mailfnews with Web browsing. Discovering and quantifying the strengths and weaknesses of the interaction techniques introduced by WebCard are challenges for the future. ACKNOWLEDGMENTS Andrew Birrell implemented Postcard; Rob Shillner implemented a large part of DeckScape.",1.0,"This TechNote introduces WebCard, an integrated maillnews reader and Web browser. As a maillnews reader, WebCard is fairly conventional; the innovation is that Web pages are fully integrated in the mail/news reader. The user interface is based on folders, where an “item” in a folder can be a mail message, news article or Web page. When displaying a Web page, users can follow links, and the new pages will appear as items in the current folder. Users can copy and move items between folders, forward items, and can also use folders to organize material on the Web, such as hotlists, query results, and breadth-first expansions. INTRODUCTION As people are spending increasing amounts of time sending e-mail, reading news, and surfing the Web, an integrated user interface to the three activities is increasingly appealing. Leading-edge Web browsers are moving toward this goal by starting to include features for handling e-mail and news. For instance, Netscape (version 1.IN) [4] provides a polished interface for reading news and posting articles. Netscape, like most Web browsers, allows a user to send e-mail; however, no browsers support reading e-mail. Leading-edge mail readers are moving in the same direction, by starting to provide access to the Web. For example, Z -Mai 1 [5] allows e-mail to reference HTML documents as MIME attachments. The text part of the mail message is shown in the standard fashion, and clicking on the “attachment” button invokes a Web browser of the user’s choice that displays the attachment. Once the Web browser is running, it is completely independent of the mail reader, however. This TechNote introduces WebCard, a folder-based mail/news reader (think of it as a combination of xmh and xrn) that has been enhanced to handle Web pages using the same inter-face that it already uses for handling mail messages and newsgroup posting. Moreover, folders of Web pages proPennission to make digitrd/hnrd copies of all or part of this material for personal or classroom use is grrtntecl without fee provided thnt lhe copies are not made or distributed for profit or commercial advantage, the copyright notice, the tine of the puhlicatimt and its date appear, and notice is given that crrpy right ia by pennissiun of IIIC ACM. Inc. TO copy otftenvise. to republish, to post on sewers or 10 redistribute to lists, requires specific permission and/or fee. UIST 95 Pittsburgh PA USA @1995 ACM O-89791-709-x/95/l 1..$3.50 vide a general-purpose way to organize Web material, such as hotlists, query results, and breadth-first expansions. WEBCARD At first blush, WebCard is a conventional folder-based e-mail reader. At any given time there is an open folder, and one message from the open folder, called the selected message, is displayed in the display pane of the WebCard window. The user can respond to the selected message, forward it, copy or move it to another folder, and so on. (Actually, more than one message can be selected, but some operations, like ‘<Reply,” are only valid when there is a single selected message.) Users can also perform standard folder operations, such as creating, renaming, and deleting folders. WebCard handles newsgroups in the same way: a folder whose name refers to a newsgroup contains the postings in the newsgroup. Users can respond to a news article by using the “Reply” button, copy postings to folders, and so on. The only difference between a newsgroup folder and any other folder is that newsgroup folders are “read-only;” users cannot delete messages from a news folder or copy messages into a news folder. WebCard uses the name of the folder to distinguish between newsgroup folders and ordinary folders; a period in the folder name indicates a newsgroup. The Andrew Messages System [3] is the only other system we know of that deals uniformly with e-mail and bulletin boards. WebCard also integrates Web pages in the same way: folders can contain Web pages. Web pages are rendered just as they would be in a stand-alone graphical Web browser, and displayed in WebCard’s display pane where the contents of a message normally are displayed (textually). The subject of the Web page “message” is the “<TITLE>” field of the Web page. The only operation that doesn’t make sense for a Web page “message” is “Reply.” There are three ways to display a Web page: clicking on a URL that appears in the currently displayed mail message or news article, primary-selecting a URL (in any window on the screen) and then middle-clicking on the “URE’ button, and using the “Open URL7 dialog. Once a Web page is displayed. clicking on a link fetches the new page. Depending on keyboard modifiers, the new page either replaces the existing page or is added as a “message” in the current folder. November 14-17, 1995 UIST ’95 197 These screen dumps show WebCard in action. The top part of the WebCard window contains a browser listing the subjects of mail messages or news postings and titles of Web pages. The bottom part of the window displays the mail message, news posting or Web page. At the left, WebCard is displaying a URL that had been mentioned in a news article posted by a user named “websurfer”. That article had been copied into the inbox as message 10. The image at the right shows WebCard after clicking on the “Mr. Showbiz” icon, and then on a link to retrieve a review of the movie Dangerous Minds. M, Stww+,zw,l, do wha&,,,ake$ roauermk?~. Grmttnam,cs F,Rm m,-, m ,, w,,, ,Iw w“ uc-,c-!k-ml,nl, enfert.”.mi m m Ikbrl,, O@ ,1= ..”,, re”leva ,elwlwm Mix,. WebCard allows users to “Detach” items from the folder, and temporarily display them in a separate pane in the WebCard window. When a detached item is a Web page, the user can click on links and the new page is added to the folder and it appears back on the display pane rather than obscuring the detached page. Thus, the user can have a page, such as a table of contents or index, visible for an extended period, even while following another chain of links on the main body of the folder. Folders provide a convenient way for users to organize material. For example, a user can keep the home pages of all of his or her colleagues together in a folder named “Colleagues,” or keep several “hotlists”, each in its own folder. WebCard also uses folders to return the results of certain operations. For example, WebCard has an “Expand One Level” command which traverses every link on a particular page, and returns all resulting pages in a new folder. IMPLEMENTATION WebCard is implemented in Modula–3 [2], and makes use of two existing Modula–3 applications, Postcard and DeckScape. Postcard is a folder-based maillnews reader that has been in daily use at SRC since 1988. DeckScape[l] is an experimental Web browser based on the metaphor of a deck, collections of Web pages only one of which is visible at a time. The DeckScape display consists of multiple decks, all in a single top-level window. Users can move, resize or iconify decks, move or copy pages between decks, and so on. Decks in DeckScape serve the function of folders in WebCard. WebCard support HTML2.0, including inline images (rendered in black-and-white), forms, and active maps. WebCard does not support external viewers. CONCLUSION WebCard has introduced a new interaction technique for browsing the Web. It integrates e-mail, news, and Web browsing into a single user interface, thereby avoiding the contextswitching inherent when using independent applications. As a Web browser, WebCard supports folders, a flexible way to organize, browse, and store large numbers of documents. We do not claim that WebCard the correct way to browse the Web, to organize material on the Web, to integrate mail and news, or even to integrate mailfnews with Web browsing. Discovering and quantifying the strengths and weaknesses of the interaction techniques introduced by WebCard are challenges for the future. ACKNOWLEDGMENTS Andrew Birrell implemented Postcard; Rob Shillner implemented a large part of DeckScape.","['DEC Systems Research Center, 130 Lytton Ave., Palo Alto, CA']",['2345597949'],1991798211.0,"{'offset': 0, 'data': [{'authorId': '2111170240', 'url': 'https://www.semanticscholar.org/author/2111170240', 'name': 'M. Brown', 'affiliations': [], 'homepage': None, 'paperCount': 52, 'citationCount': 4279}]}",8.0,"{'MAG': '1991798211', 'DBLP': 'conf/uist/Brown95', 'DOI': '10.1145/215585.215975', 'CorpusId': 12718644}",['Computer Science'],0.0,False,{'pages': '197-198'},12/1/1995,"['JournalArticle', 'Review']",0.0,Browsing the Web with a mail/news reader,https://www.semanticscholar.org/paper/5432a21b98a1cee333c87f71b365cd697893cebc,UIST,1995
1993853297,"In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.",1.0,"In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.","['Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#']","['1974057294', '2025996706', '2132214060', '2205021947', '2293388633', '2617989366', '740796856', '810855395']",1993853297.0,"{'offset': 0, 'data': [{'authorId': '1805795', 'url': 'https://www.semanticscholar.org/author/1805795', 'name': 'P. Dietz', 'affiliations': [], 'homepage': None, 'paperCount': 58, 'citationCount': 3307}, {'authorId': '145222187', 'url': 'https://www.semanticscholar.org/author/145222187', 'name': 'B. Harsham', 'affiliations': [], 'homepage': None, 'paperCount': 20, 'citationCount': 517}, {'authorId': '1694854', 'url': 'https://www.semanticscholar.org/author/1694854', 'name': 'C. Forlines', 'affiliations': [], 'homepage': None, 'paperCount': 106, 'citationCount': 5818}, {'authorId': '144478480', 'url': 'https://www.semanticscholar.org/author/144478480', 'name': 'D. Leigh', 'affiliations': [], 'homepage': None, 'paperCount': 41, 'citationCount': 2928}, {'authorId': '49632846', 'url': 'https://www.semanticscholar.org/author/49632846', 'name': 'W. Yerazunis', 'affiliations': [], 'homepage': None, 'paperCount': 44, 'citationCount': 1391}, {'authorId': '143832521', 'url': 'https://www.semanticscholar.org/author/143832521', 'name': 'S. Shipman', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 215}, {'authorId': '1405586994', 'url': 'https://www.semanticscholar.org/author/1405586994', 'name': 'B. Schmidt-Nielsen', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 110}, {'authorId': '8905733', 'url': 'https://www.semanticscholar.org/author/8905733', 'name': 'K. Ryall', 'affiliations': [], 'homepage': None, 'paperCount': 63, 'citationCount': 3358}]}",12.0,"{'DBLP': 'conf/uist/DietzHFLYSSR05', 'MAG': '1993853297', 'DOI': '10.1145/1095034.1095075', 'CorpusId': 3152194}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['Book', 'JournalArticle', 'Conference']",12.0,DT controls: adding identity to physical interfaces,https://www.semanticscholar.org/paper/86b3d6af2925e31d6c292ec44d4012358c4a7304,UIST,2005
1996803286,"We propose WindowScape, a window manager that uses a photograph metaphor for lightweight, post hoc task management. This is the first task management windowing model to provide intuitive accessibility while allowing windows to exist simultaneously in multiple tasks. WindowScape exploits users' spatial and visual memories by providing a stable thumbnail layout in which to search for windows. A function is provided to let users search the window space while maintaining a largely consistent screen image to minimize distractions. A novel keyboard interaction technique is also presented.",1.0,"We propose WindowScape, a window manager that uses a photograph metaphor for lightweight, post hoc task management. This is the first task management windowing model to provide intuitive accessibility while allowing windows to exist simultaneously in multiple tasks. WindowScape exploits users' spatial and visual memories by providing a stable thumbnail layout in which to search for windows. A function is provided to let users search the window space while maintaining a largely consistent screen image to minimize distractions. A novel keyboard interaction technique is also presented.","[', Georgia Institute of Technology, Atlanta, GA']",['2315620655'],1996803286.0,"{'offset': 0, 'data': [{'authorId': '2437287', 'url': 'https://www.semanticscholar.org/author/2437287', 'name': 'Craig S. Tashman', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 235}]}",26.0,"{'DBLP': 'conf/uist/Tashman06', 'MAG': '1996803286', 'DOI': '10.1145/1166253.1166266', 'CorpusId': 7972353}",['Computer Science'],3.0,False,{'pages': '77-80'},10/15/2006,"['JournalArticle', 'Conference']",10.0,WindowScape: a task oriented window manager,https://www.semanticscholar.org/paper/b8269255dc4c4de7c0aa4a321d2d215092c2ed35,UIST,2006
1999342495,"In order to automate repetitive tasks performed in computer applications, users are required to acquire special skills for writing macros or probwams Pro&~amming by demonstration (PBD), a method of converting a user demonstration into an executable code, is one possible solution to this problem, However, many PBD systems require users to spend much time and care in macro definition. This paper describes a PBD system, DemoOffice, which employs two techniques, actzon sllcug and macro auto-d<jinltlon, to simplifi macro definition significantly. The system is able to detect user actions which might be expected to be performed again in the fhture and to automatically convert those actions into a macro, for which no further definition is required,",0.0,"In order to automate repetitive tasks performed in computer applications, users are required to acquire special skills for writing macros or probwams Pro&~amming by demonstration (PBD), a method of converting a user demonstration into an executable code, is one possible solution to this problem, However, many PBD systems require users to spend much time and care in macro definition. This paper describes a PBD system, DemoOffice, which employs two techniques, actzon sllcug and macro auto-d<jinltlon, to simplifi macro definition significantly. The system is able to detect user actions which might be expected to be performed again in the fhture and to automatically convert those actions into a macro, for which no further definition is required,","['C&C Research Laboratories, NEC Corporation, 4-1-1 Miyazaki, Miyamae-Ku, Kawasaki, 216 Japan', 'C&C Research Laboratories, NEC Corporation, 4-1-1 Miyazaki, Miyamae-Ku, Kawasaki, 216 Japan']","['2043921849', '2157250339']",1999342495.0,"{'offset': 0, 'data': [{'authorId': '46889592', 'url': 'https://www.semanticscholar.org/author/46889592', 'name': 'Atsushi Sugiura', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 408}, {'authorId': '1772335', 'url': 'https://www.semanticscholar.org/author/1772335', 'name': 'Y. Koseki', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 566}]}",20.0,"{'DBLP': 'conf/uist/SugiuraK96', 'MAG': '1999342495', 'DOI': '10.1145/237091.237118', 'CorpusId': 7624321}",['Computer Science'],1.0,False,{'pages': '173-182'},11/1/1996,['JournalArticle'],15.0,Simplifying macro definition in programming by demonstration,https://www.semanticscholar.org/paper/ba7c478bc77c600b6de126cf15ff5ce1bb961445,UIST,1996
2000756694,"Many applications provide a form-like interface for requesting information: the user fills in some fields, submits the form, and the application presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a trial-and-error search, (2) results from one application are to be used as inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs.",1.0,"Many applications provide a form-like interface for requesting information: the user fills in some fields, submits the form, and the application presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a trial-and-error search, (2) results from one application are to be used as inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs.","['Hokkaido University, Sapporo, Japan ', 'University of Copenhagen, Copenhagen#N#, Denmark', 'Hokkaido University, Sapporo, Japan ', 'Hokkaido University, Sapporo, Japan ']","['1270784708', '162880119', '2037917844', '2274530537']",2000756694.0,"{'offset': 0, 'data': [{'authorId': '2490602', 'url': 'https://www.semanticscholar.org/author/2490602', 'name': 'J. Fujima', 'affiliations': [], 'homepage': None, 'paperCount': 48, 'citationCount': 445}, {'authorId': '3306182', 'url': 'https://www.semanticscholar.org/author/3306182', 'name': 'A. Lunzer', 'affiliations': [], 'homepage': None, 'paperCount': 41, 'citationCount': 568}, {'authorId': '1679367', 'url': 'https://www.semanticscholar.org/author/1679367', 'name': 'Kasper Hornbæk', 'affiliations': [], 'homepage': None, 'paperCount': 218, 'citationCount': 9181}, {'authorId': '144865865', 'url': 'https://www.semanticscholar.org/author/144865865', 'name': 'Yuzuru Tanaka', 'affiliations': [], 'homepage': None, 'paperCount': 281, 'citationCount': 2304}]}",119.0,"{'DBLP': 'conf/uist/FujimaLHT04', 'MAG': '2000756694', 'DOI': '10.1145/1029632.1029664', 'CorpusId': 3971787}",['Computer Science'],6.0,False,{'pages': '175-184'},10/24/2004,['JournalArticle'],26.0,"Clip, connect, clone: combining application elements to build custom interfaces for information access",https://www.semanticscholar.org/paper/aa793918fb845711880eb5ab2e8ffbd346a9facf,UIST,2004
2002846225,"Over the past ten years a broad consensus has developed that decomposing applications into separate computational and interface modules is desirable. The motivations for modularity in user interfaces are similar to those elsewhere in computer science, and center around the benefits of hiding internal implementations from other modules in a system. Modularity allows reuse of each component with compatible versions of the other components, and allows separate development processes for each component. In user interfaces, modularity allows the reuse of applications with different interfaces, perhaps for different user classes, window systems, or devices. Modularity also allows application and interface experts, including human factors engineers and graphic designers, to cooperate while minimizing the coordination required among them.",1.0,"Over the past ten years a broad consensus has developed that decomposing applications into separate computational and interface modules is desirable. The motivations for modularity in user interfaces are similar to those elsewhere in computer science, and center around the benefits of hiding internal implementations from other modules in a system. Modularity allows reuse of each component with compatible versions of the other components, and allows separate development processes for each component. In user interfaces, modularity allows the reuse of applications with different interfaces, perhaps for different user classes, window systems, or devices. Modularity also allows application and interface experts, including human factors engineers and graphic designers, to cooperate while minimizing the coordination required among them.","['IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY', 'IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY', 'IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY', 'IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY', 'IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY']","['2076680043', '2095617668', '2128443719', '222389444', '2481144362']",2002846225.0,"{'offset': 0, 'data': [{'authorId': '49906316', 'url': 'https://www.semanticscholar.org/author/49906316', 'name': 'W. E. Bennett', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 330}, {'authorId': '2882275', 'url': 'https://www.semanticscholar.org/author/2882275', 'name': 'S. Boies', 'affiliations': [], 'homepage': None, 'paperCount': 50, 'citationCount': 3145}, {'authorId': '2006080', 'url': 'https://www.semanticscholar.org/author/2006080', 'name': 'J. D. Gould', 'affiliations': [], 'homepage': None, 'paperCount': 79, 'citationCount': 5992}, {'authorId': '4602856', 'url': 'https://www.semanticscholar.org/author/4602856', 'name': 'S. L. Greene', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 789}, {'authorId': '3182960', 'url': 'https://www.semanticscholar.org/author/3182960', 'name': 'Charles Wiecha', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 462}]}",26.0,"{'MAG': '2002846225', 'DBLP': 'conf/uist/BennettBGGW89', 'DOI': '10.1145/73660.73669', 'CorpusId': 7126512}",['Computer Science'],0.0,False,{'pages': '67-75'},11/13/1989,['JournalArticle'],16.0,Transformations on a dialog tree: rule-based maping of content to style,https://www.semanticscholar.org/paper/ee6626c16fb8d2409aa4f39c22aee944855e7cd1,UIST,1989
2002910582,"Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.",1.0,"Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.","[', Georgia Institute of Technology, Atlanta, GA', 'Mitsubishi Electric Res. Lab., Cambridge, MA#TAB#', 'Carnegie Mellon University, Pittsburgh, PA and Mitsubishi Electric Research Labs, Cambridge, MA', 'Carnegie Mellon University, Pittsburgh, PA.']","['1688263250', '1974057294', '2135985964', '2171298838']",2002910582.0,"{'offset': 0, 'data': [{'authorId': '34741611', 'url': 'https://www.semanticscholar.org/author/34741611', 'name': 'J. C. Lee', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 2446}, {'authorId': '1749296', 'url': 'https://www.semanticscholar.org/author/1749296', 'name': 'S. Hudson', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 13652}, {'authorId': '2585396', 'url': 'https://www.semanticscholar.org/author/2585396', 'name': 'J. Summet', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 974}, {'authorId': '1805795', 'url': 'https://www.semanticscholar.org/author/1805795', 'name': 'P. Dietz', 'affiliations': [], 'homepage': None, 'paperCount': 58, 'citationCount': 3307}]}",110.0,"{'MAG': '2002910582', 'DBLP': 'conf/uist/LeeHSD05', 'DOI': '10.1145/1095034.1095045', 'CorpusId': 15003860}",['Computer Science'],5.0,True,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['Book', 'JournalArticle', 'Conference']",28.0,Moveable interactive projected displays using projector based tracking,https://www.semanticscholar.org/paper/148048342938e43ceb4b9df05e50c4fef4f5ed3b,UIST,2005
2003602347,"We describe local tools, a general interaction technique that replaces traditional tool palettes. A collection of tools sit on the worksurface along with the data. Each tool can be picked up (where it replaces the cursor), used, and then put down anywhere on the worksurface. There k a toolbox for organizing the tools. These local tools were implemented in Pad++ as part of KidPad, an application for children. INTRODUCTION Sitting at a desk, many people find that they work with several tools simultaneously. Perhaps they have a pencil, a red pen, a stapler, or some paper clips all on their worksurface together. This is a very natural way to work, yet most computer interfaces don’t support this style of interaction. Rather, traditional computer tool palettes allow only a single tool to be active at a time. Inl the real world, this would be equivalent to being forced to put away every tool before another could be used. In the Pad++ research group at the University of New Mexico, we have been experimenting with an alternative style of interaction we call local tools. Motivated by the above scenario, we allow the user to place several tools directly on the work surface, and then pick them up and use them, When a tool is held, it becomes the cursor and can be used just like a regular tool. But these tools can easily be put down anywhere on the work surface, and other tools can be picked up in their place. One of the driving reasons behind this development of local tools has been our work in developing a zooming application for children using Pad++. We tested Ithe standard tool palette interface in Pad++ with computer-novice fourth graders. What we found was that children had a Permission to make digitsi/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to repubtish, to post on servers or to redkibute to fists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @ 1996 ACM ()_8979 l_798_7/96/l 1 ..$3.50 Figure 1: Screen snapshot from KidPad showing use of local tools. difficult time with the interface. Despite the fact that the interface used direct manipulation, itwasn’t direct enough. Tool palettes are hard to use. The user must first find the tool palette (sometimes having to access it from a menu). Then she must press the correct button resulting in the cursor changing which is quite confusing to the uninitiated computer user. Finally, she must select various attributes of the tool, such as color and width. A local tool can embody all of these characteristics at once. Local tools remain on the surface and can be picked up and used with all of their attributes, potentially reducing cognitive load. We are building these local tools within Pad++, a zoomable environment [1] [2] [3]. Pad++ provides a huge worksurface where graphical objects can be put on the surface at any position and at any size. The user can navigate through this planar space by panning and zooming. While local tools could be implemented in a non-zooming environment, there are some drawbacks because the local tools sit on the worksurface taking up valuable screen real estate. In a zoomable environment, however, the tools can easily be made as large or small as desired. In addition, they can be pushed off the screen and then easily brought back with a special toolbox (see below). Developments similar to local tools have recently been iescribed by several research groups. Bier et. al. [4] and >ur own research group [1] have recently discussed the",1.0,"We describe local tools, a general interaction technique that replaces traditional tool palettes. A collection of tools sit on the worksurface along with the data. Each tool can be picked up (where it replaces the cursor), used, and then put down anywhere on the worksurface. There k a toolbox for organizing the tools. These local tools were implemented in Pad++ as part of KidPad, an application for children. INTRODUCTION Sitting at a desk, many people find that they work with several tools simultaneously. Perhaps they have a pencil, a red pen, a stapler, or some paper clips all on their worksurface together. This is a very natural way to work, yet most computer interfaces don’t support this style of interaction. Rather, traditional computer tool palettes allow only a single tool to be active at a time. Inl the real world, this would be equivalent to being forced to put away every tool before another could be used. In the Pad++ research group at the University of New Mexico, we have been experimenting with an alternative style of interaction we call local tools. Motivated by the above scenario, we allow the user to place several tools directly on the work surface, and then pick them up and use them, When a tool is held, it becomes the cursor and can be used just like a regular tool. But these tools can easily be put down anywhere on the work surface, and other tools can be picked up in their place. One of the driving reasons behind this development of local tools has been our work in developing a zooming application for children using Pad++. We tested Ithe standard tool palette interface in Pad++ with computer-novice fourth graders. What we found was that children had a Permission to make digitsi/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to repubtish, to post on servers or to redkibute to fists, requires specific permission and/or fee. UIST ’96 Seattle Washington USA @ 1996 ACM ()_8979 l_798_7/96/l 1 ..$3.50 Figure 1: Screen snapshot from KidPad showing use of local tools. difficult time with the interface. Despite the fact that the interface used direct manipulation, itwasn’t direct enough. Tool palettes are hard to use. The user must first find the tool palette (sometimes having to access it from a menu). Then she must press the correct button resulting in the cursor changing which is quite confusing to the uninitiated computer user. Finally, she must select various attributes of the tool, such as color and width. A local tool can embody all of these characteristics at once. Local tools remain on the surface and can be picked up and used with all of their attributes, potentially reducing cognitive load. We are building these local tools within Pad++, a zoomable environment [1] [2] [3]. Pad++ provides a huge worksurface where graphical objects can be put on the surface at any position and at any size. The user can navigate through this planar space by panning and zooming. While local tools could be implemented in a non-zooming environment, there are some drawbacks because the local tools sit on the worksurface taking up valuable screen real estate. In a zoomable environment, however, the tools can easily be made as large or small as desired. In addition, they can be pushed off the screen and then easily brought back with a special toolbox (see below). Developments similar to local tools have recently been iescribed by several research groups. Bier et. al. [4] and >ur own research group [1] have recently discussed the","['Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#', 'Computer Science Dept., University of New Mexico Albuquerque, NM#TAB#']","['2046653803', '2187430686', '2227470276', '2503118683', '400323076', '935383222']",2003602347.0,"{'offset': 0, 'data': [{'authorId': '1799187', 'url': 'https://www.semanticscholar.org/author/1799187', 'name': 'B. Bederson', 'affiliations': [], 'homepage': None, 'paperCount': 350, 'citationCount': 19703}, {'authorId': '1698170', 'url': 'https://www.semanticscholar.org/author/1698170', 'name': 'J. Hollan', 'affiliations': [], 'homepage': None, 'paperCount': 155, 'citationCount': 10261}, {'authorId': '145957514', 'url': 'https://www.semanticscholar.org/author/145957514', 'name': 'A. Druin', 'affiliations': [], 'homepage': None, 'paperCount': 267, 'citationCount': 12442}, {'authorId': '145551150', 'url': 'https://www.semanticscholar.org/author/145551150', 'name': 'Jason Stewart', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 1215}, {'authorId': '2054530764', 'url': 'https://www.semanticscholar.org/author/2054530764', 'name': 'David Rogers', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 178}, {'authorId': '47324929', 'url': 'https://www.semanticscholar.org/author/47324929', 'name': 'David Proft', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 337}]}",86.0,"{'DBLP': 'conf/uist/BedersonHDSRP96', 'MAG': '2003602347', 'DOI': '10.1145/237091.237116', 'CorpusId': 10854855}",['Computer Science'],4.0,False,{'pages': '169-170'},11/1/1996,['JournalArticle'],9.0,Local tools: an alternative to tool palettes,https://www.semanticscholar.org/paper/b218fce1e8ec5a48307c1b55aae5400a8e66d094,UIST,1996
2006053442,"1 The World-Wide Web as a Universal User Interface Our work is motivated by three trends. First, the ubiquitous migration of services to the World Wide Web is due in part to its simple, consistent, and now universal user interface: navigation by following links and filling out HTML forms are interactions familiar to even novice Internet users. Second, client-side extension technologies such as Java and JavaScript allow sites to extend and “personalize” the behaviors and interfaces of their services, with portable user-interface elements that integrate transparently into the browser’s existing interface. Finally, there has been a recent surge of interest in proxymediated access to the Web, in which proxy agents in the network infrastructure provide caching [ 11, anonymize user requests [2], or accelerate Web access via datatype-specific lossy compression [3,4,5]. Recent results show that these services can be built scalably and cost-effectively, and can shield the user from the limitations of their Internet connections or client platforms. Not surprisingly, the services have become increasingly powerful and therefore parameterizable and customizable by each user, resulting in increased attention on the design and implementation of the user interface by which the service can be controlled [S]",1.0,"1 The World-Wide Web as a Universal User Interface Our work is motivated by three trends. First, the ubiquitous migration of services to the World Wide Web is due in part to its simple, consistent, and now universal user interface: navigation by following links and filling out HTML forms are interactions familiar to even novice Internet users. Second, client-side extension technologies such as Java and JavaScript allow sites to extend and “personalize” the behaviors and interfaces of their services, with portable user-interface elements that integrate transparently into the browser’s existing interface. Finally, there has been a recent surge of interest in proxymediated access to the Web, in which proxy agents in the network infrastructure provide caching [ 11, anonymize user requests [2], or accelerate Web access via datatype-specific lossy compression [3,4,5]. Recent results show that these services can be built scalably and cost-effectively, and can shield the user from the limitations of their Internet connections or client platforms. Not surprisingly, the services have become increasingly powerful and therefore parameterizable and customizable by each user, resulting in increased attention on the design and implementation of the user interface by which the service can be controlled [S]","['UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#', 'UC Berkeley, Computer Science Division#TAB#']","['1975802069', '2044013107', '2069032792', '2161511064', '2264216759', '2486019822', '2698057404']",2006053442.0,"{'offset': 0, 'data': [{'authorId': '143608596', 'url': 'https://www.semanticscholar.org/author/143608596', 'name': 'A. Fox', 'affiliations': [], 'homepage': None, 'paperCount': 240, 'citationCount': 31634}, {'authorId': '1700451', 'url': 'https://www.semanticscholar.org/author/1700451', 'name': 'S. Gribble', 'affiliations': [], 'homepage': None, 'paperCount': 123, 'citationCount': 15039}, {'authorId': '2729365', 'url': 'https://www.semanticscholar.org/author/2729365', 'name': 'Y. Chawathe', 'affiliations': [], 'homepage': None, 'paperCount': 32, 'citationCount': 5930}, {'authorId': '32021132', 'url': 'https://www.semanticscholar.org/author/32021132', 'name': 'Anthony S. Polito', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 35}, {'authorId': '1799407', 'url': 'https://www.semanticscholar.org/author/1799407', 'name': 'A. Huang', 'affiliations': [], 'homepage': None, 'paperCount': 9, 'citationCount': 178}, {'authorId': '2112968', 'url': 'https://www.semanticscholar.org/author/2112968', 'name': 'B. Ling', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 231}, {'authorId': '1759010', 'url': 'https://www.semanticscholar.org/author/1759010', 'name': 'E. Brewer', 'affiliations': [], 'homepage': None, 'paperCount': 224, 'citationCount': 23172}]}",10.0,"{'DBLP': 'conf/uist/FoxGCPHLB97', 'MAG': '2006053442', 'DOI': '10.1145/263407.263516', 'CorpusId': 32570097}",['Computer Science'],0.0,False,{'pages': '83-84'},10/1/1997,['JournalArticle'],8.0,Orthogonal extensions to the WWW user interface using client-side technologies,https://www.semanticscholar.org/paper/4eb8ff6b61b1c2a6e756066ae1321c259ae39fe2,UIST,1997
2006563349,"We describe the design of and experience with PointRight, a peer-to-peer pointer and keyboard redirection system that operates in multi-machine, multi-user environments. PointRight employs a geometric model for redirecting input across screens driven by multiple independent machines and operating systems. It was created for interactive workspaces that include large, shared displays and individual laptops, but is a general tool that supports many different configurations and modes of use. Although previous systems have provided for re-routing pointer and keyboard control, in this paper we present a more general and flexible system, along with an analysis of the types of re-binding that must be handled by any pointer redirection system This paper describes the system, the ways in which it has been used, and the lessons that have been learned from its use over the last two years.",1.0,"We describe the design of and experience with PointRight, a peer-to-peer pointer and keyboard redirection system that operates in multi-machine, multi-user environments. PointRight employs a geometric model for redirecting input across screens driven by multiple independent machines and operating systems. It was created for interactive workspaces that include large, shared displays and individual laptops, but is a general tool that supports many different configurations and modes of use. Although previous systems have provided for re-routing pointer and keyboard control, in this paper we present a more general and flexible system, along with an analysis of the types of re-binding that must be handled by any pointer redirection system This paper describes the system, the ways in which it has been used, and the lessons that have been learned from its use over the last two years.","['Stanford University, Stanford, CA, ', 'Stanford University, Stanford, CA, ', 'Stanford University, Stanford, CA, ', 'StoneSoup Consulting, Los Altos, CA']","['2091637845', '2099856657', '2142744349', '2778064811']",2006563349.0,"{'offset': 0, 'data': [{'authorId': '40387028', 'url': 'https://www.semanticscholar.org/author/40387028', 'name': 'B. Johanson', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 2139}, {'authorId': '39556068', 'url': 'https://www.semanticscholar.org/author/39556068', 'name': 'Gregory Hutchins', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 404}, {'authorId': '1699245', 'url': 'https://www.semanticscholar.org/author/1699245', 'name': 'T. Winograd', 'affiliations': [], 'homepage': None, 'paperCount': 217, 'citationCount': 40096}, {'authorId': '9144611', 'url': 'https://www.semanticscholar.org/author/9144611', 'name': 'M. Stone', 'affiliations': [], 'homepage': None, 'paperCount': 74, 'citationCount': 4745}]}",216.0,"{'MAG': '2006563349', 'DBLP': 'conf/uist/JohansonHWS02', 'DOI': '10.1145/571985.572019', 'CorpusId': 2112595}",['Computer Science'],21.0,False,{'pages': '227-234'},10/27/2002,['JournalArticle'],19.0,PointRight: experience with flexible input redirection in interactive workspaces,https://www.semanticscholar.org/paper/e793fc8ffce13d57ab19950e8909c58f4dd5d707,UIST,2002
2010076961,"Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.",1.0,"Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.","[', IBM T.J. Watson Research Center, Hawthorne, NY', ', IBM T.J. Watson Research Center, Hawthorne, NY', ', IBM T.J. Watson Research Center, Hawthorne, NY', ', IBM T.J. Watson Research Center, Hawthorne, NY']","['2127407938', '2132335929', '2155522140', '2171628738']",2010076961.0,"{'offset': 0, 'data': [{'authorId': '4299644', 'url': 'https://www.semanticscholar.org/author/4299644', 'name': 'L. Bergman', 'affiliations': [], 'homepage': None, 'paperCount': 73, 'citationCount': 1842}, {'authorId': '2879453', 'url': 'https://www.semanticscholar.org/author/2879453', 'name': 'V. Castelli', 'affiliations': [], 'homepage': None, 'paperCount': 132, 'citationCount': 2891}, {'authorId': '143907654', 'url': 'https://www.semanticscholar.org/author/143907654', 'name': 'T. Lau', 'affiliations': [], 'homepage': None, 'paperCount': 84, 'citationCount': 3514}, {'authorId': '2104782', 'url': 'https://www.semanticscholar.org/author/2104782', 'name': 'Daniel Oblinger', 'affiliations': [], 'homepage': None, 'paperCount': 27, 'citationCount': 454}]}",105.0,"{'DBLP': 'conf/uist/BergmanCLO05', 'MAG': '2010076961', 'DOI': '10.1145/1095034.1095067', 'CorpusId': 7456390}",['Computer Science'],10.0,False,{'name': 'Proceedings of the 18th annual ACM symposium on User interface software and technology'},10/23/2005,"['Book', 'JournalArticle', 'Conference']",15.0,DocWizards: a system for authoring follow-me documentation wizards,https://www.semanticscholar.org/paper/d68e97ba3481e429367a1a1742e06607194000de,UIST,2005
2010813084,"Conté is a small input device inspired by the way artists manipulate a real Conté crayon. By changing which corner, edge, end, or side is contacting the display, the operator can switch interaction modes using a single hand. Conté's rectangular prism shape enables both precise pen-like input and tangible handle interaction. Conté also has a natural compatibility with multi-touch input: it can be tucked in the palm to interleave same-hand touch input, or used to expand the vocabulary of bimanual touch. Inspired by informal interviews with artists, we catalogue Conté's characteristics, and use these to outline a design space. We describe a prototype device using common materials and simple electronics. With this device, we demonstrate interaction techniques in a test-bed drawing application. Finally, we discuss alternate hardware designs and future human factors research to study this new class of input.",1.0,"Conté is a small input device inspired by the way artists manipulate a real Conté crayon. By changing which corner, edge, end, or side is contacting the display, the operator can switch interaction modes using a single hand. Conté's rectangular prism shape enables both precise pen-like input and tangible handle interaction. Conté also has a natural compatibility with multi-touch input: it can be tucked in the palm to interleave same-hand touch input, or used to expand the vocabulary of bimanual touch. Inspired by informal interviews with artists, we catalogue Conté's characteristics, and use these to outline a design space. We describe a prototype device using common materials and simple electronics. With this device, we demonstrate interaction techniques in a test-bed drawing application. Finally, we discuss alternate hardware designs and future human factors research to study this new class of input.","['University of Waterloo Waterloo, ON Canada', 'LIFL, INRIA Lille & University of Lille, Lille, France']","['2154794983', '2256161107']",2010813084.0,"{'offset': 0, 'data': [{'authorId': '145731603', 'url': 'https://www.semanticscholar.org/author/145731603', 'name': 'Daniel Vogel', 'affiliations': [], 'homepage': None, 'paperCount': 111, 'citationCount': 3712}, {'authorId': '3051289', 'url': 'https://www.semanticscholar.org/author/3051289', 'name': 'Géry Casiez', 'affiliations': [], 'homepage': None, 'paperCount': 119, 'citationCount': 2460}]}",42.0,"{'DBLP': 'conf/uist/VogelC11', 'MAG': '2010813084', 'DOI': '10.1145/2047196.2047242', 'CorpusId': 2655769}",['Computer Science'],5.0,False,{'name': 'Proceedings of the 24th annual ACM symposium on User interface software and technology'},10/16/2011,"['Book', 'JournalArticle', 'Conference']",37.0,Conté: multimodal input inspired by an artist's crayon,https://www.semanticscholar.org/paper/3c182729cb12ed0b7f14dcac3fc8b33ad5036e76,UIST,2011
2016668445,"This paper explores architectural support for interfaces combining pen, paper, and PC. We show how the event-based approach common to GUIs can apply to augmented paper, and describe additions to address paper's distinguishing characteristics. To understand the developer experience of this architecture, we deployed the toolkit to 17 student teams for six weeks. Analysis of the developers' code provided insight into the appropriateness of events for paper UIs. The usage patterns we distilled informed a second iteration of the toolkit, which introduces techniques for integrating interactive and batched input handling, coordinating interactions across devices, and debugging paper applications. The study also revealed that programmers created gesture handlers by composing simple ink measurements. This desire for informal interactions inspired us to include abstractions for recognition. This work has implications beyond paper - designers of graphical tools can examine API usage to inform iterative toolkit development.",1.0,"This paper explores architectural support for interfaces combining pen, paper, and PC. We show how the event-based approach common to GUIs can apply to augmented paper, and describe additions to address paper's distinguishing characteristics. To understand the developer experience of this architecture, we deployed the toolkit to 17 student teams for six weeks. Analysis of the developers' code provided insight into the appropriateness of events for paper UIs. The usage patterns we distilled informed a second iteration of the toolkit, which introduces techniques for integrating interactive and batched input handling, coordinating interactions across devices, and debugging paper applications. The study also revealed that programmers created gesture handlers by composing simple ink measurements. This desire for informal interactions inspired us to include abstractions for recognition. This work has implications beyond paper - designers of graphical tools can examine API usage to inform iterative toolkit development.","['Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)']","['2135027868', '220422686', '261822931']",2016668445.0,"{'offset': 0, 'data': [{'authorId': '2105696', 'url': 'https://www.semanticscholar.org/author/2105696', 'name': 'Ron B. Yeh', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 892}, {'authorId': '1750481', 'url': 'https://www.semanticscholar.org/author/1750481', 'name': 'A. Paepcke', 'affiliations': [], 'homepage': None, 'paperCount': 171, 'citationCount': 8872}, {'authorId': '21520403', 'url': 'https://www.semanticscholar.org/author/21520403', 'name': 'Scott R. Klemmer', 'affiliations': [], 'homepage': None, 'paperCount': 182, 'citationCount': 8620}]}",61.0,"{'DBLP': 'conf/uist/YehPK08', 'MAG': '2016668445', 'DOI': '10.1145/1449715.1449734', 'CorpusId': 5959927}",['Computer Science'],2.0,True,{'pages': '111-120'},10/19/2008,['JournalArticle'],41.0,Iterative design and evaluation of an event architecture for pen-and-paper interfaces,https://www.semanticscholar.org/paper/5571ed95ba2469ed10350d0158cdafb830fe100e,UIST,2008
2020796004,"Cameras are a useful source of input for many interactive applications, but computer vision programming is difficult and requires specialized knowledge that is out of reach for many HCI practitioners. In an effort to learn what makes a useful computer vision design tool, we created Eyepatch, a tool for designing camera-based interactions, and evaluated the Eyepatch prototype through deployment to students in an HCI course. This paper describes the lessons we learned about making computer vision more accessible, while retaining enough power and flexibility to be useful in a wide variety of interaction scenarios.",0.0,"Cameras are a useful source of input for many interactive applications, but computer vision programming is difficult and requires specialized knowledge that is out of reach for many HCI practitioners. In an effort to learn what makes a useful computer vision design tool, we created Eyepatch, a tool for designing camera-based interactions, and evaluated the Eyepatch prototype through deployment to students in an HCI course. This paper describes the lessons we learned about making computer vision more accessible, while retaining enough power and flexibility to be useful in a wide variety of interaction scenarios.","['Stanford University, Stanford, CA, ', 'Stanford University, Stanford, CA, ', 'University of Tokyo,    Tokyo, Japan']","['2064390276', '2091637845', '2152110089']",2020796004.0,"{'offset': 0, 'data': [{'authorId': '1405776592', 'url': 'https://www.semanticscholar.org/author/1405776592', 'name': 'D. Maynes-Aminzade', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 744}, {'authorId': '1699245', 'url': 'https://www.semanticscholar.org/author/1699245', 'name': 'T. Winograd', 'affiliations': [], 'homepage': None, 'paperCount': 217, 'citationCount': 40096}, {'authorId': '1717356', 'url': 'https://www.semanticscholar.org/author/1717356', 'name': 'T. Igarashi', 'affiliations': [], 'homepage': None, 'paperCount': 465, 'citationCount': 11915}]}",62.0,"{'MAG': '2020796004', 'DBLP': 'conf/uist/Maynes-AminzadeWI07', 'DOI': '10.1145/1294211.1294219', 'CorpusId': 8350916}",['Computer Science'],2.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference']",47.0,Eyepatch: prototyping camera-based interaction through examples,https://www.semanticscholar.org/paper/661110f41096cd45fb1cb15c522706ea324b755a,UIST,2007
2023045585,"The large scale of online classes and the diversity of the students that participate in them can enable new educational systems. This massive scale and diversity can enable always-available systems that help students share diverse ideas, and inspire and learn from each other. We introduce systems for two core educational processes at scale: discussion and assessment. To date, several thousand students in a dozen online classes have used our discussion system. Controlled experiments suggest that participants in more diverse discussions perform better on tests and that discussion improves engagement. Similarly, more than 100,000 students have reviewed peer work for both summative assessment and feedback. Through these systems, we argue that to create new educational experiences at scale, pedagogical strategies and software that leverage scale and diversity must be co-developed. More broadly, we suggest the key to creating new educational experiences online lies in leveraging massive networks of peers.",0.0,"The large scale of online classes and the diversity of the students that participate in them can enable new educational systems. This massive scale and diversity can enable always-available systems that help students share diverse ideas, and inspire and learn from each other. We introduce systems for two core educational processes at scale: discussion and assessment. To date, several thousand students in a dozen online classes have used our discussion system. Controlled experiments suggest that participants in more diverse discussions perform better on tests and that discussion improves engagement. Similarly, more than 100,000 students have reviewed peer work for both summative assessment and feedback. Through these systems, we argue that to create new educational experiences at scale, pedagogical strategies and software that leverage scale and diversity must be co-developed. More broadly, we suggest the key to creating new educational experiences online lies in leveraging massive networks of peers.","['Stanford Univ., Stanford, CA (USA)']",['2114573301'],2023045585.0,"{'offset': 0, 'data': [{'authorId': '145665851', 'url': 'https://www.semanticscholar.org/author/145665851', 'name': 'Chinmay Kulkarni', 'affiliations': [], 'homepage': None, 'paperCount': 53, 'citationCount': 1474}]}",2.0,"{'DBLP': 'conf/uist/Kulkarni14', 'MAG': '2023045585', 'DOI': '10.1145/2658779.2661169', 'CorpusId': 14055090}",['Computer Science'],1.0,False,{'pages': '25-28'},10/5/2014,"['JournalArticle', 'Review']",22.0,Making distance matter: leveraging scale and diversity in massive online classes,https://www.semanticscholar.org/paper/aebb8ec710cb4b8ef7496fdb197fdabe6a053d2b,UIST,2014
2026445814,"Ubiquitous and Wearable Computing both have the goal of pushing the computer into the background, supporting all kinds of human activities. Application areas include areas such as everyday environments (e.g. clothing, home, office), promoting new forms of creative learning via physical/virtual objects, and new tools for interactive design. In this paper, we thrust ubiquitous computing into the extremely hostile environment of the sparring ring of a martial art competition. Our system uses piezoelectric force sensors that transmit signals wirelessly to enable the detection of when a significant impact has been delivered to a competitor's body. The objective is to support the judges in scoring the sparring matches accurately, while preserving the goal of merging and blending into the background of the activity. The system therefore must take into account of the rules of the game, be responsive in real-time asynchronously, and often cope with untrained operators of the system. We present a pilot study of the finished prototype and detail our experience.",0.0,"Ubiquitous and Wearable Computing both have the goal of pushing the computer into the background, supporting all kinds of human activities. Application areas include areas such as everyday environments (e.g. clothing, home, office), promoting new forms of creative learning via physical/virtual objects, and new tools for interactive design. In this paper, we thrust ubiquitous computing into the extremely hostile environment of the sparring ring of a martial art competition. Our system uses piezoelectric force sensors that transmit signals wirelessly to enable the detection of when a significant impact has been delivered to a competitor's body. The objective is to support the judges in scoring the sparring matches accurately, while preserving the goal of merging and blending into the background of the activity. The system therefore must take into account of the rules of the game, be responsive in real-time asynchronously, and often cope with untrained operators of the system. We present a pilot study of the finished prototype and detail our experience.","['Impact Measurement, San Jose, CA', 'Palo Alto Research Center Palo Alto CA', 'Impact Measurement, San Jose, CA']","['2227081523', '2237883256', '2711211348']",2026445814.0,"{'offset': 0, 'data': [{'authorId': '2226805', 'url': 'https://www.semanticscholar.org/author/2226805', 'name': 'Ed H. Chi', 'affiliations': ['Google'], 'homepage': 'http://edchi.net', 'paperCount': 239, 'citationCount': 15987}, {'authorId': '2112623146', 'url': 'https://www.semanticscholar.org/author/2112623146', 'name': 'Jin Song', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 78}, {'authorId': '71584823', 'url': 'https://www.semanticscholar.org/author/71584823', 'name': 'G. Corbin', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 81}]}",78.0,"{'MAG': '2026445814', 'DBLP': 'conf/uist/ChiSC04', 'DOI': '10.1145/1029632.1029680', 'CorpusId': 3975795}",['Computer Science'],3.0,False,{'pages': '277-285'},10/24/2004,['JournalArticle'],23.0,"""Killer App"" of wearable computing: wireless force sensing body protectors for martial arts",https://www.semanticscholar.org/paper/e5ea8fc3b15b8e117638586a6ae2f86a36a9992c,UIST,2004
2027559041,We have implemented a computer interface that renders synchronized auditory and haptic stimuli with very low (0.5ms) latency. The audio and haptic interface (AHI) includes a Pantograph haptic device that reads position input from a user and renders force output based on this input. We synthesize audio by convolving the force profile generated by user interaction with the impulse response of the virtual surface. Auditory and haptic modes are tightly coupled because we produce both stimuli from the same force profile. We have conducted a user study with the AHI to verify that the 0.5ms system latency lies below the perceptual threshold for detecting separation between auditory and haptic contact events. We discuss future applications of the AHI for further perceptual studies and for synthesizing continuous contact interactions in virtual environments.,1.0,We have implemented a computer interface that renders synchronized auditory and haptic stimuli with very low (0.5ms) latency. The audio and haptic interface (AHI) includes a Pantograph haptic device that reads position input from a user and renders force output based on this input. We synthesize audio by convolving the force profile generated by user interaction with the impulse response of the virtual surface. Auditory and haptic modes are tightly coupled because we produce both stimuli from the same force profile. We have conducted a user study with the AHI to verify that the 0.5ms system latency lies below the perceptual threshold for detecting separation between auditory and haptic contact events. We discuss future applications of the AHI for further perceptual studies and for synthesizing continuous contact interactions in virtual environments.,"['Department of Computer Science; University of British Columbia; 2366 Main Mall Vancouver BC V6T 1Z4 Canada', 'Department of Computer Science; University of British Columbia; 2366 Main Mall Vancouver BC V6T 1Z4 Canada']","['1921985643', '1978646557']",2027559041.0,"{'offset': 0, 'data': [{'authorId': '31800575', 'url': 'https://www.semanticscholar.org/author/31800575', 'name': 'D. DiFilippo', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 75}, {'authorId': '1694975', 'url': 'https://www.semanticscholar.org/author/1694975', 'name': 'D. Pai', 'affiliations': [], 'homepage': None, 'paperCount': 255, 'citationCount': 7967}]}",54.0,"{'DBLP': 'conf/uist/DiFilippoP00', 'MAG': '2027559041', 'DOI': '10.1145/354401.354437', 'CorpusId': 5679090}",['Computer Science'],1.0,False,{'pages': '149-158'},11/1/2000,['JournalArticle'],26.0,The AHI: an audio and haptic interface for contact interactions,https://www.semanticscholar.org/paper/d753b8f130345e47f782ca9265aab825fbb3e973,UIST,2000
2030417747,"INTRODUCTION A significant amount of innovation from research labs and universities is wastec& it is never applied in systems that the actual users can or want to use. The process of going frc~m a novel concept to a usable, useful system is poorly understood by most researchers. The purpose of this panel is to address how research done in research labs and universities can be converted into systems that the end users would use. Each of the panelists has built at least one substantial system which is currently being used by a large community of real users (other than the team that built the system). Based on their experience, they will make recommendations that, if followed early enough in the project, would make the conversion to usable systems faster and easier.",0.0,"INTRODUCTION A significant amount of innovation from research labs and universities is wastec& it is never applied in systems that the actual users can or want to use. The process of going frc~m a novel concept to a usable, useful system is poorly understood by most researchers. The purpose of this panel is to address how research done in research labs and universities can be converted into systems that the end users would use. Each of the panelists has built at least one substantial system which is currently being used by a large community of real users (other than the team that built the system). Based on their experience, they will make recommendations that, if followed early enough in the project, would make the conversion to usable systems faster and easier.","['Computer Science Department Carnegie Mellon University#TAB#', 'Silicon Graphics, Inc.', 'Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 0511#TAB#', 'NASA Goddard Space-Flight Center']","['2117127927', '2153301466', '2165991770', '2230878929']",2030417747.0,"{'offset': 0, 'data': [{'authorId': '36581969', 'url': 'https://www.semanticscholar.org/author/36581969', 'name': 'Gurminder Singh', 'affiliations': [], 'homepage': None, 'paperCount': 79, 'citationCount': 1120}, {'authorId': '1801068', 'url': 'https://www.semanticscholar.org/author/1801068', 'name': 'M. Linton', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 1872}, {'authorId': '1707801', 'url': 'https://www.semanticscholar.org/author/1707801', 'name': 'B. Myers', 'affiliations': [], 'homepage': None, 'paperCount': 498, 'citationCount': 26319}, {'authorId': '3250278', 'url': 'https://www.semanticscholar.org/author/3250278', 'name': 'M. Szczur', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 40}]}",0.0,"{'MAG': '2030417747', 'DBLP': 'conf/uist/SinghLMS93', 'DOI': '10.1145/168642.168656', 'CorpusId': 6928412}",['Computer Science'],0.0,False,{'pages': '139-143'},12/1/1993,['JournalArticle'],6.0,"From research prototypes to usable, useful systems: lessons learned in the trenches",https://www.semanticscholar.org/paper/00c59629d79b7ee928b315a9c4d6f619f167a860,UIST,1993
2031793755,"Abstract This paper is concerned with pen-based (also called stylus-based) computers. Two of the key questions for such computers are how to interface to handwriting recognition algorithms, and whether there are interfaces that can effectively exploit the differences between a stylus and a keyboard/mouse. We describe prototypes that explore each of these questions. Our text entry tool is designed around the idea that handwriting recognition algorithms will always be error prone, and has a different flavor from existing systems. Our prototype editor goes beyond the usual gesture editors used with styli and is based on the idea of leaving the markups visible.",1.0,"Abstract This paper is concerned with pen-based (also called stylus-based) computers. Two of the key questions for such computers are how to interface to handwriting recognition algorithms, and whether there are interfaces that can effectively exploit the differences between a stylus and a keyboard/mouse. We describe prototypes that explore each of these questions. Our text entry tool is designed around the idea that handwriting recognition algorithms will always be error prone, and has a different flavor from existing systems. Our prototype editor goes beyond the usual gesture editors used with styli and is based on the idea of leaving the markups visible.","['Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA', 'Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA and Dept. of Computer Science, Massachusetts, Institute of Technology, Cambridge, MA']","['2102678951', '2762236433']",2031793755.0,"{'offset': 0, 'data': [{'authorId': '38111918', 'url': 'https://www.semanticscholar.org/author/38111918', 'name': 'David Goldberg', 'affiliations': ['Google'], 'homepage': 'https://scholar.google.com/citations?user=9Mapl04AAAAJ&hl=en&oi=ao', 'paperCount': 57, 'citationCount': 10125}, {'authorId': '2686642', 'url': 'https://www.semanticscholar.org/author/2686642', 'name': 'Aaron Goodisman', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 72}]}",65.0,"{'MAG': '2031793755', 'DBLP': 'conf/uist/GoldbergG91', 'DOI': '10.1145/120782.120796', 'CorpusId': 7818106}",['Computer Science'],4.0,False,{'pages': '127-135'},11/11/1991,['JournalArticle'],26.0,Stylus user interfaces for manipulating text,https://www.semanticscholar.org/paper/33feba32751f0090c6fa1f807ba6bc0904de20b5,UIST,1991
2032158330,"ABSTRACTAdding animation to interfaces is a very difficult task withtoday’s toolkits, even though there are many situations inwhich it would be useful and effective. The Amulet toolkitcontains a new form of animation constraint that allowsanimations to be added to interfaces extremely easily with-out changing the logic of the application or the graphicalobjects themselves. An animation constraint detectschanges to the value of the slot to which it is attached, andcauses the slot to instead take on a series of values interpo-lated between the original and new values. The advantageover previous approaches is that animation constraintsprovide significantly better modularity and reuse. Theprogrammer has independent control over the graphics tobe animated, the start and end values of the animation, thepath through value space, and the timing of the animation.Animations can be attached to any object, even existingwidgets from the toolkit, and any type of value can beanimated: scalars, coordinates, fonts, colors, line widths,point lists (for polygons), booleans (for visibility), etc. Alibrary of useful animation constraints is provided in thetoolkit, including support for exaggerated, cartoon-styleeffects such as slow-in-slow-out, anticipation, and follow-through. Because animations can be added to an existingapplication with only a single extra line of code, we expectthat this new mechanism will make it easy for researchersand developers to investigate the use of animations in awide variety of applications.Keywords: Animation, Constraints, Toolkits, User Inter-face Development Environments, Amulet.To appear in UIST’96: ACM Symposium onUser Interface Software and Technology, Nov.6-8, 1996. Seattle, WAINTRODUCTIONBy providing better modularity for the software for userinterfaces, the Amulet toolkit [8] achieves increased reuseand decreased code size, and makes it easier for research-ers and developers to create applications. For example, inAmulet, the interactive behavior of objects can be definedentirely independently from their graphical look by attach-ing “Interactor” objects to the graphics. Command obje cts[9] encapsulate the complete information about operations,and can be hierarchically linked so each application layercan be separately defined. We have followed this philoso-phy in our new support for animations and other time-based behaviors. The goal is to make simple animationsextremely easy to add to an interface, and still supportcomplex animations. This is achieved by allowing theanimations to be defined independently from the actionsthat start the animation and the graphics that are ani-mated. Furthermore, the animation itself is modularized:the start and end values of the animation, the path throughthe value space, and the timing of the animation can all beindependently specified.We are able to provide this new level of modularity by us-ing the flexible Amulet constraint system. If an animationconstraint is attached to a slot (instance variable) of anobject, then changes to that slot trigger the animation,causing the slot’s value to change smoothly from its old toits new value. For example, if a slot contains 10 and is setto be 100, an animation constraint might immediatelyoverride the value and set the slot with 11, and then set theslot with a series of values up to 100 over a period of twoseconds. Since there are a wide variety of parameterizedanimation constraints provided in the library, an anima-tion can be added to an existing interface with a single lineof code. All the messy details of timing, redrawing, set-upand clean-up of animations, and interactions with otherparts of the system, are automatically handled by the ani-mation constraint mechanism.We are particularly interested in the use of animations toenhance interaction in regular user interfaces. Althoughanimations are widely used in games and specializedvisualization software ([10] has a survey), their use in con-ventional interfaces is limited to a few predefined cases,",0.0,"ABSTRACTAdding animation to interfaces is a very difficult task withtoday’s toolkits, even though there are many situations inwhich it would be useful and effective. The Amulet toolkitcontains a new form of animation constraint that allowsanimations to be added to interfaces extremely easily with-out changing the logic of the application or the graphicalobjects themselves. An animation constraint detectschanges to the value of the slot to which it is attached, andcauses the slot to instead take on a series of values interpo-lated between the original and new values. The advantageover previous approaches is that animation constraintsprovide significantly better modularity and reuse. Theprogrammer has independent control over the graphics tobe animated, the start and end values of the animation, thepath through value space, and the timing of the animation.Animations can be attached to any object, even existingwidgets from the toolkit, and any type of value can beanimated: scalars, coordinates, fonts, colors, line widths,point lists (for polygons), booleans (for visibility), etc. Alibrary of useful animation constraints is provided in thetoolkit, including support for exaggerated, cartoon-styleeffects such as slow-in-slow-out, anticipation, and follow-through. Because animations can be added to an existingapplication with only a single extra line of code, we expectthat this new mechanism will make it easy for researchersand developers to investigate the use of animations in awide variety of applications.Keywords: Animation, Constraints, Toolkits, User Inter-face Development Environments, Amulet.To appear in UIST’96: ACM Symposium onUser Interface Software and Technology, Nov.6-8, 1996. Seattle, WAINTRODUCTIONBy providing better modularity for the software for userinterfaces, the Amulet toolkit [8] achieves increased reuseand decreased code size, and makes it easier for research-ers and developers to create applications. For example, inAmulet, the interactive behavior of objects can be definedentirely independently from their graphical look by attach-ing “Interactor” objects to the graphics. Command obje cts[9] encapsulate the complete information about operations,and can be hierarchically linked so each application layercan be separately defined. We have followed this philoso-phy in our new support for animations and other time-based behaviors. The goal is to make simple animationsextremely easy to add to an interface, and still supportcomplex animations. This is achieved by allowing theanimations to be defined independently from the actionsthat start the animation and the graphics that are ani-mated. Furthermore, the animation itself is modularized:the start and end values of the animation, the path throughthe value space, and the timing of the animation can all beindependently specified.We are able to provide this new level of modularity by us-ing the flexible Amulet constraint system. If an animationconstraint is attached to a slot (instance variable) of anobject, then changes to that slot trigger the animation,causing the slot’s value to change smoothly from its old toits new value. For example, if a slot contains 10 and is setto be 100, an animation constraint might immediatelyoverride the value and set the slot with 11, and then set theslot with a series of values up to 100 over a period of twoseconds. Since there are a wide variety of parameterizedanimation constraints provided in the library, an anima-tion can be added to an existing interface with a single lineof code. All the messy details of timing, redrawing, set-upand clean-up of animations, and interactions with otherparts of the system, are automatically handled by the ani-mation constraint mechanism.We are particularly interested in the use of animations toenhance interaction in regular user interfaces. Althoughanimations are widely used in games and specializedvisualization software ([10] has a survey), their use in con-ventional interfaces is limited to a few predefined cases,","['Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA']","['209317017', '2104582966', '2117127927', '2158239202']",2032158330.0,"{'offset': 0, 'data': [{'authorId': '1707801', 'url': 'https://www.semanticscholar.org/author/1707801', 'name': 'B. Myers', 'affiliations': [], 'homepage': None, 'paperCount': 498, 'citationCount': 26319}, {'authorId': '1930409460', 'url': 'https://www.semanticscholar.org/author/1930409460', 'name': 'Robert C. Miller', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 1373}, {'authorId': '48139068', 'url': 'https://www.semanticscholar.org/author/48139068', 'name': 'Richard G. McDaniel', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 704}, {'authorId': '2422253', 'url': 'https://www.semanticscholar.org/author/2422253', 'name': 'A. Ferrency', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 419}]}",51.0,"{'MAG': '2032158330', 'DBLP': 'conf/uist/MyersMMF96', 'DOI': '10.1145/237091.237109', 'CorpusId': 1231633}",['Computer Science'],0.0,False,{'pages': '119-128'},11/1/1996,"['JournalArticle', 'Review']",18.0,Easily adding animations to interfaces using constraints,https://www.semanticscholar.org/paper/c10cdb745a7524b9993fec73d11257013cf0ca37,UIST,1996
2032598656,"Laboratory studies present difficulties in the understanding of how usage evolves over time. Employed observations are obtrusive and not naturalistic. Our system employs a remote capture tool that provides longitudinal low-level interaction data. It is easily deployable into any Web site allowing deployments in-the-wild and is completely unobtrusive. Web application interfaces are designed assuming users' goals. Requirement specifications contain well defined use cases and scenarios that drive design and subsequent optimisations. Users' interaction patterns outside the expected ones are not considered. This results in an optimisation for a stylised user rather than a real one. A bottom-up analysis from low-level interaction data makes possible the emergence of users' tasks. Similarities among users can be found and solutions that are effective for real users can be designed. Factors such as learnability and how interface changes affect users are difficult to observe in laboratory studies. Our solution makes it possible, adding a longitudinal point of view to traditional laboratory studies. The capture tool is deployed in real world Web applications capturing in-situ data from users. These data serve to explore analysis and visualisation possibilities. We present an example of the exploration results with one Web application.",0.0,"Laboratory studies present difficulties in the understanding of how usage evolves over time. Employed observations are obtrusive and not naturalistic. Our system employs a remote capture tool that provides longitudinal low-level interaction data. It is easily deployable into any Web site allowing deployments in-the-wild and is completely unobtrusive. Web application interfaces are designed assuming users' goals. Requirement specifications contain well defined use cases and scenarios that drive design and subsequent optimisations. Users' interaction patterns outside the expected ones are not considered. This results in an optimisation for a stylised user rather than a real one. A bottom-up analysis from low-level interaction data makes possible the emergence of users' tasks. Similarities among users can be found and solutions that are effective for real users can be designed. Factors such as learnability and how interface changes affect users are difficult to observe in laboratory studies. Our solution makes it possible, adding a longitudinal point of view to traditional laboratory studies. The capture tool is deployed in real world Web applications capturing in-situ data from users. These data serve to explore analysis and visualisation possibilities. We present an example of the exploration results with one Web application.","['University of Manchester,Manchester,United Kingdom']",['2227275790'],2032598656.0,"{'offset': 0, 'data': [{'authorId': '2884921', 'url': 'https://www.semanticscholar.org/author/2884921', 'name': 'Aitor Apaolaza', 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 92}]}",4.0,"{'DBLP': 'conf/uist/Apaolaza13', 'MAG': '2032598656', 'DOI': '10.1145/2508468.2508475', 'CorpusId': 1766350}",['Computer Science'],0.0,False,{'pages': '53-56'},10/8/2013,['JournalArticle'],10.0,Identifying emergent behaviours from longitudinal web use,https://www.semanticscholar.org/paper/efdc10eb417a668ec6c1d6985025e1e824280cfe,UIST,2013
2034683285,"The interaction history of a document can be modelled as a tree of command objects. This model not only supports recovery (undo/redo), but is also suitable for cooperation between distributed users working on a common document. Various coupling modes can be supported. Switching between modes is supported by regarding different versions of a document as different branches of the history. Branches can then be merged using a selective redo mechanism. Synchronous cooperation is supported by replicating the document state and exchanging command objects. Optimistic concurrency control can be applied, because conflicting actions can later be undone automatically.",1.0,"The interaction history of a document can be modelled as a tree of command objects. This model not only supports recovery (undo/redo), but is also suitable for cooperation between distributed users working on a common document. Various coupling modes can be supported. Switching between modes is supported by regarding different versions of a document as different branches of the history. Branches can then be merged using a selective redo mechanism. Synchronous cooperation is supported by replicating the document state and exchanging command objects. Optimistic concurrency control can be applied, because conflicting actions can later be undone automatically.","['GMD (German National Research Center for Computer Science), P.O. Box 1316, 53731 Sankt Augustin, Germany', 'GMD (German National Research Center for Computer Science), P.O. Box 1316, 53731 Sankt Augustin, Germany']","['2152767087', '2707341445']",2034683285.0,"{'offset': 0, 'data': [{'authorId': '1697526', 'url': 'https://www.semanticscholar.org/author/1697526', 'name': 'T. Berlage', 'affiliations': [], 'homepage': None, 'paperCount': 56, 'citationCount': 960}, {'authorId': '2493391', 'url': 'https://www.semanticscholar.org/author/2493391', 'name': 'Andreas Genau', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 175}]}",89.0,"{'DBLP': 'conf/uist/BerlageG93', 'MAG': '2034683285', 'DOI': '10.1145/168642.168668', 'CorpusId': 8539866}",['Computer Science'],6.0,False,{'pages': '249-257'},12/1/1993,['JournalArticle'],31.0,A framework for shared applications with a replicated architecture,https://www.semanticscholar.org/paper/55c732fbdaebbcc0cd79d4e4d2c7057b993bf0cd,UIST,1993
2036625304,"Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting.",1.0,"Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting.","['Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Adobe Syst., San Francisco, CA, USA', 'Massachusetts Institute of Technology Cambridge, MA USA']","['1974803209', '2104582966', '2121327176', '2305153757']",2036625304.0,"{'offset': 0, 'data': [{'authorId': '145879842', 'url': 'https://www.semanticscholar.org/author/145879842', 'name': 'Michael S. Bernstein', 'affiliations': ['Stanford University'], 'homepage': 'https://hci.stanford.edu/msb/', 'paperCount': 187, 'citationCount': 41173}, {'authorId': '145671153', 'url': 'https://www.semanticscholar.org/author/145671153', 'name': 'Joel Brandt', 'affiliations': [], 'homepage': None, 'paperCount': 51, 'citationCount': 2285}, {'authorId': '152160465', 'url': 'https://www.semanticscholar.org/author/152160465', 'name': 'Rob Miller', 'affiliations': [], 'homepage': None, 'paperCount': 120, 'citationCount': 9034}, {'authorId': '1743286', 'url': 'https://www.semanticscholar.org/author/1743286', 'name': 'D. Karger', 'affiliations': [], 'homepage': None, 'paperCount': 336, 'citationCount': 48522}]}",343.0,"{'DBLP': 'conf/uist/BernsteinBMK11', 'MAG': '2036625304', 'DOI': '10.1145/2047196.2047201', 'CorpusId': 6921166}",['Computer Science'],19.0,True,{'name': 'Proceedings of the 24th annual ACM symposium on User interface software and technology'},10/16/2011,"['Book', 'JournalArticle', 'Conference']",42.0,Crowds in two seconds: enabling realtime crowd-powered interfaces,https://www.semanticscholar.org/paper/4874157301f8d21dbe7acb304fe2fb2081bb4435,UIST,2011
2039898452,"The index (first) finger of the dominant hand seems to be an intuitively natural and efficient means for pointing tasks. This paper presents the design of a device to enable pointing with the index finger as an interaction technique in mobile computers. The device, which uses infrared emission and detection to determine where on a screen the finger is pointing, is inexpensive and can easily be incorporated into a laptop computer. KEY W 0 R DS : Pointing, Interaction devices, Input devices, Infrared detection. INTRODUCTION Pointing with the index finger seems to be a fundamental means of communication among humans. Studies have shown [2] that infants as young as two months will extend the index finger as a means of indicating a desire for attention. By twelve months, infants show facility for both responding to the directional gaze of their mothers by pointing in the direction of the gaze and pointing at an object in order to direct their mothers’ gaze towards it. It is interesting that most pointing devices used with computers (mice, styli, trackballs, joysticks etc.) have little or nothing in common with natural pointing. Notable exceptions are [l] which exploited natural pointing with the entire hand arm system and, more recently the FingerMouse [3] which uses video and image processing to determine both the direction the finger is pointing and the configuration of the hand. Perwission to mnke digiirnl/hnrd cofiirs ofall or pnrl oflhis mlcrinl for perso~nl or clnssroonl use is granted without fee provided thnt the copies are not nlndr or distributed tbr pro12 or conunercinl ndvnnlrgr. IIIC copy. right notice. the title of the publication and its dale appear. and notice is given lhnt copyright is by pcrmissiorl ofthe ACM, hlc. To copy otherwise. . IO republish. lo post 011 wvers or IO redistribute to lists, requires specific permission nnd/or fee UIST 97 I3clnfl.4lbertn, c.hxiff Copyright 1997 ACM 0-89791~881-9/97/lO..S3.50 PHYSIOLOGICAL BASIS Pointing with index finger extended is not only an intuitive behavior, it is also a behavior that humans can perform with a high degree of facility. A relatively large proportion of the primary motor cortex is dedicated to controlling the fingers and the neural connection path from the cortex to the finger muscles is relatively direct [4] yielding a high degree of fine motor control. If you take a moment to point at a few objects in the surrounding environment you will note that your intuitive pointing motion is to extend your index finger in the direction of the target object. If you assume your hand is positioned on the keyboard of a laptop and point only with the index finger you will note that the range of motion is more than sufficient to cover the area of a typical laptop screen. This is an additional advantage of pointing with the finger: since you don’t have to take your hand off the keyboard you avoid the problem of alternating device acquisition. To benefit from these physiological advantages, you must be pointing at the target directly with the finger itself, not using one or more fingers indirectly to control a cursor through a device such as a mouse or trackball. DESIGN FOR MOBILE COMPUTING Mobile computing introduces constraints on input devices that are not present in a workstation environment. Input devices cannot be too large or expensive and they must be contained within the mobile unit (e.g. laptop) or easily attached to it. The input devices most frequently available with laptop computers, trackballs, scratchpads, and isometric joysticks, satisfy these constraints but are used in a relatively difficult indirect control method which most users find less than fully satisfactory. This is why so many users plug a mouse into their laptops whenever possible.",1.0,"The index (first) finger of the dominant hand seems to be an intuitively natural and efficient means for pointing tasks. This paper presents the design of a device to enable pointing with the index finger as an interaction technique in mobile computers. The device, which uses infrared emission and detection to determine where on a screen the finger is pointing, is inexpensive and can easily be incorporated into a laptop computer. KEY W 0 R DS : Pointing, Interaction devices, Input devices, Infrared detection. INTRODUCTION Pointing with the index finger seems to be a fundamental means of communication among humans. Studies have shown [2] that infants as young as two months will extend the index finger as a means of indicating a desire for attention. By twelve months, infants show facility for both responding to the directional gaze of their mothers by pointing in the direction of the gaze and pointing at an object in order to direct their mothers’ gaze towards it. It is interesting that most pointing devices used with computers (mice, styli, trackballs, joysticks etc.) have little or nothing in common with natural pointing. Notable exceptions are [l] which exploited natural pointing with the entire hand arm system and, more recently the FingerMouse [3] which uses video and image processing to determine both the direction the finger is pointing and the configuration of the hand. Perwission to mnke digiirnl/hnrd cofiirs ofall or pnrl oflhis mlcrinl for perso~nl or clnssroonl use is granted without fee provided thnt the copies are not nlndr or distributed tbr pro12 or conunercinl ndvnnlrgr. IIIC copy. right notice. the title of the publication and its dale appear. and notice is given lhnt copyright is by pcrmissiorl ofthe ACM, hlc. To copy otherwise. . IO republish. lo post 011 wvers or IO redistribute to lists, requires specific permission nnd/or fee UIST 97 I3clnfl.4lbertn, c.hxiff Copyright 1997 ACM 0-89791~881-9/97/lO..S3.50 PHYSIOLOGICAL BASIS Pointing with index finger extended is not only an intuitive behavior, it is also a behavior that humans can perform with a high degree of facility. A relatively large proportion of the primary motor cortex is dedicated to controlling the fingers and the neural connection path from the cortex to the finger muscles is relatively direct [4] yielding a high degree of fine motor control. If you take a moment to point at a few objects in the surrounding environment you will note that your intuitive pointing motion is to extend your index finger in the direction of the target object. If you assume your hand is positioned on the keyboard of a laptop and point only with the index finger you will note that the range of motion is more than sufficient to cover the area of a typical laptop screen. This is an additional advantage of pointing with the finger: since you don’t have to take your hand off the keyboard you avoid the problem of alternating device acquisition. To benefit from these physiological advantages, you must be pointing at the target directly with the finger itself, not using one or more fingers indirectly to control a cursor through a device such as a mouse or trackball. DESIGN FOR MOBILE COMPUTING Mobile computing introduces constraints on input devices that are not present in a workstation environment. Input devices cannot be too large or expensive and they must be contained within the mobile unit (e.g. laptop) or easily attached to it. The input devices most frequently available with laptop computers, trackballs, scratchpads, and isometric joysticks, satisfy these constraints but are used in a relatively difficult indirect control method which most users find less than fully satisfactory. This is why so many users plug a mouse into their laptops whenever possible.","['The George Washington University                      Washington, D.C.', 'The George Washington University                      Washington, D.C.']","['2058589586', '361874762']",2039898452.0,"{'offset': 0, 'data': [{'authorId': '1796576', 'url': 'https://www.semanticscholar.org/author/1796576', 'name': 'J. Sibert', 'affiliations': [], 'homepage': None, 'paperCount': 62, 'citationCount': 2946}, {'authorId': '1739667', 'url': 'https://www.semanticscholar.org/author/1739667', 'name': 'Mehmet Gokturk', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 292}]}",12.0,"{'MAG': '2039898452', 'DBLP': 'conf/uist/SibertG97', 'DOI': '10.1145/263407.263506', 'CorpusId': 6209828}",['Computer Science'],1.0,False,{'pages': '41-42'},10/1/1997,['JournalArticle'],6.0,"A finger-mounted, direct pointing device for mobile computing",https://www.semanticscholar.org/paper/05358fedf1562055635592e57e9b188969b80769,UIST,1997
2042346957,"One of the challenges with using mobile touch-screen devices is that they do not provide tactile feedback to the user. Thus, the user is required to look at the screen to interact with these devices. In this paper, we present SemFeel, a tactile feedback system which informs the user about the presence of an object where she touches on the screen and can offer additional semantic information about that item. Through multiple vibration motors that we attached to the backside of a mobile touch-screen device, SemFeel can generate different patterns of vibration, such as ones that flow from right to left or from top to bottom, to help the user interact with a mobile device. Through two user studies, we show that users can distinguish ten different patterns, including linear patterns and a circular pattern, at approximately 90% accuracy, and that SemFeel supports accurate eyes-free interactions.",1.0,"One of the challenges with using mobile touch-screen devices is that they do not provide tactile feedback to the user. Thus, the user is required to look at the screen to interact with these devices. In this paper, we present SemFeel, a tactile feedback system which informs the user about the presence of an object where she touches on the screen and can offer additional semantic information about that item. Through multiple vibration motors that we attached to the backside of a mobile touch-screen device, SemFeel can generate different patterns of vibration, such as ones that flow from right to left or from top to bottom, to help the user interact with a mobile device. Through two user studies, we show that users can distinguish ten different patterns, including linear patterns and a circular pattern, at approximately 90% accuracy, and that SemFeel supports accurate eyes-free interactions.","['Univ. of Toronto, Toronto ON Canada', 'Univ. of Toronto, Toronto ON Canada']","['1932678941', '2144340954']",2042346957.0,"{'offset': 0, 'data': [{'authorId': '3260704', 'url': 'https://www.semanticscholar.org/author/3260704', 'name': 'K. Yatani', 'affiliations': [], 'homepage': None, 'paperCount': 75, 'citationCount': 2303}, {'authorId': '1752847', 'url': 'https://www.semanticscholar.org/author/1752847', 'name': 'K. Truong', 'affiliations': [], 'homepage': None, 'paperCount': 155, 'citationCount': 4951}]}",165.0,"{'DBLP': 'conf/uist/YataniT09', 'MAG': '2042346957', 'DOI': '10.1145/1622176.1622198', 'CorpusId': 9277268}",['Computer Science'],16.0,False,{'pages': '111-120'},10/4/2009,['JournalArticle'],23.0,SemFeel: a user interface with semantic tactile feedback for mobile touch-screen devices,https://www.semanticscholar.org/paper/3868fc6a82247c4150948b43389998bf794c96f6,UIST,2009
2042751882,"The Go-Go immersive interaction technique uses the metaphor of interactively growing the user’s arm and non-linear mapping for reaching and manipulating distant objects. Unlike others, our technique allows for seamless direct manipulation of both nearby objects and those at a distance.",1.0,"The Go-Go immersive interaction technique uses the metaphor of interactively growing the user’s arm and non-linear mapping for reaching and manipulating distant objects. Unlike others, our technique allows for seamless direct manipulation of both nearby objects and those at a distance.","['HIT Lab, University of Washington, Box 352142, Seattle, WA#TAB#', 'IS Lab, Hiroshima University, 1-4-1 Kagamiyama, Higashi-Hiroshima-shi, 739 Japan#TAB#', 'IS Lab, Hiroshima University, 1-4-1 Kagamiyama, Higashi-Hiroshima-shi, 739 Japan#TAB#', 'HIT Lab, University of Washington, Box 352142, Seattle, WA#TAB#']","['2069749540', '2083962833', '2728936', '358915003']",2042751882.0,"{'offset': 0, 'data': [{'authorId': '1736819', 'url': 'https://www.semanticscholar.org/author/1736819', 'name': 'I. Poupyrev', 'affiliations': [], 'homepage': None, 'paperCount': 119, 'citationCount': 12831}, {'authorId': '1684805', 'url': 'https://www.semanticscholar.org/author/1684805', 'name': 'M. Billinghurst', 'affiliations': [], 'homepage': None, 'paperCount': 705, 'citationCount': 24553}, {'authorId': '2846946', 'url': 'https://www.semanticscholar.org/author/2846946', 'name': 'S. Weghorst', 'affiliations': [], 'homepage': None, 'paperCount': 71, 'citationCount': 4983}, {'authorId': '1737975', 'url': 'https://www.semanticscholar.org/author/1737975', 'name': 'T. Ichikawa', 'affiliations': [], 'homepage': None, 'paperCount': 84, 'citationCount': 2831}]}",766.0,"{'DBLP': 'conf/uist/PoupyrevBWI96', 'MAG': '2042751882', 'DOI': '10.1145/237091.237102', 'CorpusId': 1098140}",['Computer Science'],67.0,False,{'pages': '79-80'},11/1/1996,['JournalArticle'],7.0,The go-go interaction technique: non-linear mapping for direct manipulation in VR,https://www.semanticscholar.org/paper/45a42326b191c3a16aa70353118b0e060e414f4d,UIST,1996
2047191331,"Based on previous studies of the associations between color and music, we introduce a scalable way of using colors to label songs and a visualization of music archives that facilitates music exploration. We present Visimu, an online game that attracted users to generate 926 color labels for 102 songs, with over 75% of the songs having color labels reaching high consensus in the Lab color space. We implemented a music archive visualization using the color labels generated by Visimu, and conducted an experiment to show that labeling music by color is more effective than text tags when the user is looking for songs of a particular mood or use scenario. Our results showed that Visimu is effective to produce meaningful color labels for music mood classification, and such approach enables a wide range of applications for music visualization and discovery.",0.0,"Based on previous studies of the associations between color and music, we introduce a scalable way of using colors to label songs and a visualization of music archives that facilitates music exploration. We present Visimu, an online game that attracted users to generate 926 color labels for 102 songs, with over 75% of the songs having color labels reaching high consensus in the Lab color space. We implemented a music archive visualization using the color labels generated by Visimu, and conducted an experiment to show that labeling music by color is more effective than text tags when the user is looking for songs of a particular mood or use scenario. Our results showed that Visimu is effective to produce meaningful color labels for music mood classification, and such approach enables a wide range of applications for music visualization and discovery.","['Stanford Univeristy, Stanford, CA, USA', 'Stanford Univeristy, Stanford, CA, USA']","['2224734793', '2641022071']",2047191331.0,"{'offset': 0, 'data': [{'authorId': '2203187', 'url': 'https://www.semanticscholar.org/author/2203187', 'name': 'Borui Wang', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 192}, {'authorId': '2108760789', 'url': 'https://www.semanticscholar.org/author/2108760789', 'name': 'Jingshu Chen', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 105}]}",1.0,"{'DBLP': 'conf/uist/WangC13', 'MAG': '2047191331', 'DOI': '10.1145/2508468.2514726', 'CorpusId': 13182948}",['Computer Science'],0.0,False,{'pages': '93-94'},10/8/2013,['JournalArticle'],8.0,Visimu: a game for music color label collection,https://www.semanticscholar.org/paper/cbdcf2af972c10ea26285a7d0b4cc44c9abb189e,UIST,2013
2047799303,"This panel explores the dialog and interplay between artists and technologists. In the process, the panelists aim to bring considerations of art and the artistic process to the attention of the technology-oriented UIST community. We invite readers to think about how your work relates to art. We encourage the research community to look for ways to integrate art and artists within their own programs, for example, by starting artist-in-residence activities, introducing courses on art and design into CS curricula, or inviting artists to participate in projects.",0.0,"This panel explores the dialog and interplay between artists and technologists. In the process, the panelists aim to bring considerations of art and the artistic process to the attention of the technology-oriented UIST community. We invite readers to think about how your work relates to art. We encourage the research community to look for ways to integrate art and artists within their own programs, for example, by starting artist-in-residence activities, introducing courses on art and design into CS curricula, or inviting artists to participate in projects.","['NYU Media Research Lab, 719 Broadway, New York, NY 11238', 'Interval Research Corp. (Palo Alto, CA)', 'University of Michigan, School of Art & Design, 2000 Bonisteel Blvd., Ann Arbor, MI#TAB#', 'Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA#TAB#']","['2164490646', '2234311260', '2666733252', '2973567629', '2973777991']",2047799303.0,"{'offset': 0, 'data': [{'authorId': '145702850', 'url': 'https://www.semanticscholar.org/author/145702850', 'name': 'Jon Meyer', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 1086}, {'authorId': '50066723', 'url': 'https://www.semanticscholar.org/author/50066723', 'name': 'L. Staples', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 76}, {'authorId': '2830501', 'url': 'https://www.semanticscholar.org/author/2830501', 'name': 'S. Minneman', 'affiliations': [], 'homepage': None, 'paperCount': 31, 'citationCount': 1871}, {'authorId': '48345967', 'url': 'https://www.semanticscholar.org/author/48345967', 'name': 'M. Naimark', 'affiliations': [], 'homepage': None, 'paperCount': 83, 'citationCount': 2557}, {'authorId': '1731467', 'url': 'https://www.semanticscholar.org/author/1731467', 'name': 'A. Glassner', 'affiliations': [], 'homepage': None, 'paperCount': 161, 'citationCount': 4576}]}",16.0,"{'DBLP': 'conf/uist/MeyerSMNG98', 'MAG': '2047799303', 'DOI': '10.1145/288392.289101', 'CorpusId': 10363998}",['Computer Science'],0.0,False,{'pages': '67-69'},11/1/1998,['JournalArticle'],0.0,Artists and technologists working together (panel),https://www.semanticscholar.org/paper/3c1860af617417a0506d06aea7e15e3c1be65eba,UIST,1998
2048376563,"User interface toolkits and higher-level tools built on top of them play an ever increasing part in developing graphical user interfaces. This paper describes the XIT system, a user interface development tool for the X Window System, based on Common Lisp, comprising user interface toolkits as well as high-level interactive tools organized into a layered architecture. We especially focus on the object-oriented design of the lower-level toolkits and show how advanced features for describing automatic screen layout, visual feedback, application links, complex interaction, and dialog control, usually not included in traditional user interface toolkits, are integrated.",1.0,"User interface toolkits and higher-level tools built on top of them play an ever increasing part in developing graphical user interfaces. This paper describes the XIT system, a user interface development tool for the X Window System, based on Common Lisp, comprising user interface toolkits as well as high-level interactive tools organized into a layered architecture. We especially focus on the object-oriented design of the lower-level toolkits and show how advanced features for describing automatic screen layout, visual feedback, application links, complex interaction, and dialog control, usually not included in traditional user interface toolkits, are integrated.","['', '', '']","['1963510881', '2052294306', '2251518097']",2048376563.0,"{'offset': 0, 'data': [{'authorId': '21809881', 'url': 'https://www.semanticscholar.org/author/21809881', 'name': 'J. Herczeg', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 88}, {'authorId': '6730118', 'url': 'https://www.semanticscholar.org/author/6730118', 'name': 'H. Hohl', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 133}, {'authorId': '2074780101', 'url': 'https://www.semanticscholar.org/author/2074780101', 'name': 'Matthias Ressel', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 424}]}",9.0,"{'DBLP': 'conf/uist/HerczegHR92', 'MAG': '2048376563', 'DOI': '10.1145/142621.142647', 'CorpusId': 15692838}",['Computer Science'],0.0,False,{'pages': '181-190'},12/1/1992,['JournalArticle'],30.0,Progress in building user interface toolkits: the world according to XIT,https://www.semanticscholar.org/paper/518834e1c2c19d165b6daf65c7d941523fb038cd,UIST,1992
2056413517,"Alice is a rapid prototyping system used to create three dimensional graphics simulations like those seen in virtual reality applications. Alice uses an interpreted language called Python as its scripting language to implement user actions. This interactive development environment allows users to explore many more design options than is possible in a compiled language environment. The alpha version of Alice for Windows 95 is available for free over the intemet, with the beta release scheduled for August.",0.0,"Alice is a rapid prototyping system used to create three dimensional graphics simulations like those seen in virtual reality applications. Alice uses an interpreted language called Python as its scripting language to implement user actions. This interactive development environment allows users to explore many more design options than is possible in a compiled language environment. The alpha version of Alice for Windows 95 is available for free over the intemet, with the beta release scheduled for August.","['Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#', 'Computer Science Department, Carnegie Mellon University 5000 Forbes Ave., Pittsburgh PA#TAB#']","['1560725665', '2036143025', '2112273523', '2121099458', '2138818861', '2148195976', '2156669975', '2222752504', '2223011922', '2309941565', '2577702118', '2611034914', '2683190129', '2718373209', '395054399']",2056413517.0,"{'offset': 0, 'data': [{'authorId': '4630329', 'url': 'https://www.semanticscholar.org/author/4630329', 'name': 'Jeffrey S. Pierce', 'affiliations': [], 'homepage': None, 'paperCount': 49, 'citationCount': 2789}, {'authorId': '15654864', 'url': 'https://www.semanticscholar.org/author/15654864', 'name': 'Steve Audia', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 274}, {'authorId': '49056003', 'url': 'https://www.semanticscholar.org/author/49056003', 'name': 'T. Burnette', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 743}, {'authorId': '34546116', 'url': 'https://www.semanticscholar.org/author/34546116', 'name': 'K. Christiansen', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 310}, {'authorId': '145065139', 'url': 'https://www.semanticscholar.org/author/145065139', 'name': 'D. Cosgrove', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 882}, {'authorId': '144895846', 'url': 'https://www.semanticscholar.org/author/144895846', 'name': 'M. Conway', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 3568}, {'authorId': '1738072', 'url': 'https://www.semanticscholar.org/author/1738072', 'name': 'K. Hinckley', 'affiliations': [], 'homepage': None, 'paperCount': 163, 'citationCount': 9081}, {'authorId': '2066741', 'url': 'https://www.semanticscholar.org/author/2066741', 'name': 'Kristen Monkaitis', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 12}, {'authorId': '144524944', 'url': 'https://www.semanticscholar.org/author/144524944', 'name': 'James Patten', 'affiliations': [], 'homepage': None, 'paperCount': 34, 'citationCount': 1206}, {'authorId': '2396063', 'url': 'https://www.semanticscholar.org/author/2396063', 'name': 'Joe Shochet', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 77}, {'authorId': '1976266', 'url': 'https://www.semanticscholar.org/author/1976266', 'name': 'D. Staack', 'affiliations': [], 'homepage': None, 'paperCount': 157, 'citationCount': 2037}, {'authorId': '40037604', 'url': 'https://www.semanticscholar.org/author/40037604', 'name': 'Brian C. Stearns', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 232}, {'authorId': '3009807', 'url': 'https://www.semanticscholar.org/author/3009807', 'name': 'Christopher B. Sturgill', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 48}, {'authorId': '2111171539', 'url': 'https://www.semanticscholar.org/author/2111171539', 'name': 'George H. Williams', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 649}, {'authorId': '1717974', 'url': 'https://www.semanticscholar.org/author/1717974', 'name': 'R. Pausch', 'affiliations': [], 'homepage': None, 'paperCount': 138, 'citationCount': 11436}]}",12.0,"{'DBLP': 'conf/uist/PierceABCCCHMPSSSSWP97', 'MAG': '2056413517', 'DOI': '10.1145/263407.263512', 'CorpusId': 5139278}",['Computer Science'],1.0,False,{'pages': '77-78'},10/1/1997,['JournalArticle'],4.0,Alice: easy to use interactive 3D graphics,https://www.semanticscholar.org/paper/2a69430e5aeb7cea5b8a9b112a3ea623610d22d7,UIST,1997
2058770803,"Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.",1.0,"Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.","['Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA', 'Center for Human Computer Interaction, Computer Science Department, Virginia Polytechnic Institute and State University, Blacksburg, VA']","['151371205', '1963687483', '2144459651', '2150671435', '274344141', '296214965', '387951467']",2058770803.0,"{'offset': 0, 'data': [{'authorId': '1784389', 'url': 'https://www.semanticscholar.org/author/1784389', 'name': 'C. Ganoe', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 1796}, {'authorId': '2949902', 'url': 'https://www.semanticscholar.org/author/2949902', 'name': 'J. Somervell', 'affiliations': [], 'homepage': None, 'paperCount': 19, 'citationCount': 547}, {'authorId': '2871459', 'url': 'https://www.semanticscholar.org/author/2871459', 'name': 'D. Neale', 'affiliations': [], 'homepage': None, 'paperCount': 29, 'citationCount': 1390}, {'authorId': '2042202', 'url': 'https://www.semanticscholar.org/author/2042202', 'name': 'Philip L. Isenhour', 'affiliations': [], 'homepage': None, 'paperCount': 40, 'citationCount': 1390}, {'authorId': '145066246', 'url': 'https://www.semanticscholar.org/author/145066246', 'name': 'John Millar Carroll', 'affiliations': ['Pennsylvania State University'], 'homepage': 'https://jcarroll.ist.psu.edu/', 'paperCount': 831, 'citationCount': 29160}, {'authorId': '1715072', 'url': 'https://www.semanticscholar.org/author/1715072', 'name': 'M. Rosson', 'affiliations': [], 'homepage': None, 'paperCount': 412, 'citationCount': 17630}, {'authorId': '1693753', 'url': 'https://www.semanticscholar.org/author/1693753', 'name': 'D. S. McCrickard', 'affiliations': [], 'homepage': None, 'paperCount': 221, 'citationCount': 3039}]}",94.0,"{'DBLP': 'conf/uist/GanoeSNICRM03', 'MAG': '2058770803', 'DOI': '10.1145/964696.964699', 'CorpusId': 12037407}",['Computer Science'],2.0,False,{'pages': '21-30'},11/2/2003,['JournalArticle'],12.0,Classroom BRIDGE: using collaborative public and desktop timelines to support activity awareness,https://www.semanticscholar.org/paper/c8de817b1d97ff8c8db321570352a900d6cdf22e,UIST,2003
2060546968,"Current user interface too&its provide components that are complex and expensive. Programmers cannot use these components for many kinds of application data because the resulting implementation would be awkward and inefficient. We have defined a set of small, simple components, called glyphs, that programmers can use in large numbers to build user interfaces. To show that glyphs are simple and efficient, we have implemented a WYSIWYG document editor. The editor’s performance is comparable to that of similar editors built with current tools, but its implementation is much simpler. We used the editor to create and print this paper.",1.0,"Current user interface too&its provide components that are complex and expensive. Programmers cannot use these components for many kinds of application data because the resulting implementation would be awkward and inefficient. We have defined a set of small, simple components, called glyphs, that programmers can use in large numbers to build user interfaces. To show that glyphs are simple and efficient, we have implemented a WYSIWYG document editor. The editor’s performance is comparable to that of similar editors built with current tools, but its implementation is much simpler. We used the editor to create and print this paper.","['Center for Integrated Systems, Stanford University, Stanford, California', 'Center for Integrated Systems, Stanford University, Stanford, California']","['2153301466', '2201169625']",2060546968.0,"{'offset': 0, 'data': [{'authorId': '2398494', 'url': 'https://www.semanticscholar.org/author/2398494', 'name': 'P. Calder', 'affiliations': [], 'homepage': None, 'paperCount': 56, 'citationCount': 1190}, {'authorId': '1801068', 'url': 'https://www.semanticscholar.org/author/1801068', 'name': 'M. Linton', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 1872}]}",78.0,"{'MAG': '2060546968', 'DBLP': 'conf/uist/CalderL90', 'DOI': '10.1145/97924.97935', 'CorpusId': 17178050}",['Computer Science'],2.0,False,{'pages': '92-101'},8/1/1990,['JournalArticle'],12.0,Glyphs: flyweight objects for user interfaces,https://www.semanticscholar.org/paper/1c627f64ea780c982f247a13a5cb6709dc699ec4,UIST,1990
2070601038,"In physiotherapy, the traditional approach of using fixed aids to train patients to keep their balance is often ineffective, due to the tendency of people to lose interest in the training or to lose confidence in their ability to finish the training. A Trainer system is proposed on traditional physiotherapy treatment methods to allow patients to play qualified and immersive games with a mobile aid. Using RF localization and self-balancing technology, the system allows patients to control a vehicle with their sense of balance. This platform provides a series of game feedback interface which involves part-body motion in sitting manipulation therapy to make the rehabilitation more flexible and more effective. This paper reports the designing and the control of the Trainer, the experimental evaluations of the performance of system, as well as an exploration of the future work in detail. Our work is intended to improve the patient experience of the physiotherapy rehabilitation using games with instinctive ways of controlling mobile instruments.",0.0,"In physiotherapy, the traditional approach of using fixed aids to train patients to keep their balance is often ineffective, due to the tendency of people to lose interest in the training or to lose confidence in their ability to finish the training. A Trainer system is proposed on traditional physiotherapy treatment methods to allow patients to play qualified and immersive games with a mobile aid. Using RF localization and self-balancing technology, the system allows patients to control a vehicle with their sense of balance. This platform provides a series of game feedback interface which involves part-body motion in sitting manipulation therapy to make the rehabilitation more flexible and more effective. This paper reports the designing and the control of the Trainer, the experimental evaluations of the performance of system, as well as an exploration of the future work in detail. Our work is intended to improve the patient experience of the physiotherapy rehabilitation using games with instinctive ways of controlling mobile instruments.","['Zhejiang University Hangzhou CHINA', 'Zhejiang University Hangzhou CHINA', 'Zhejiang University Hangzhou CHINA', ' Macquarie University, Sydney, Australia.', 'College of Computer Science and Technology, Hangzhou, China', 'Zhejiang University Hangzhou CHINA']","['2100394852', '2222850515', '2227451066', '2251201438', '2308501200', '2600999513']",2070601038.0,"{'offset': 0, 'data': [{'authorId': '1960053', 'url': 'https://www.semanticscholar.org/author/1960053', 'name': 'Guanyun Wang', 'affiliations': [], 'homepage': None, 'paperCount': 69, 'citationCount': 856}, {'authorId': '145790918', 'url': 'https://www.semanticscholar.org/author/145790918', 'name': 'Ye Tao', 'affiliations': [], 'homepage': None, 'paperCount': 47, 'citationCount': 554}, {'authorId': '2123519083', 'url': 'https://www.semanticscholar.org/author/2123519083', 'name': 'Dian Yu', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 3}, {'authorId': '2064669386', 'url': 'https://www.semanticscholar.org/author/2064669386', 'name': 'Chuan Cao', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 3}, {'authorId': '2108377217', 'url': 'https://www.semanticscholar.org/author/2108377217', 'name': 'Hongyu Chen', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 10}, {'authorId': '145952222', 'url': 'https://www.semanticscholar.org/author/145952222', 'name': 'Cheng Yao', 'affiliations': [], 'homepage': None, 'paperCount': 59, 'citationCount': 146}]}",3.0,"{'DBLP': 'conf/uist/WangTYCCY14', 'MAG': '2070601038', 'DOI': '10.1145/2658779.2658783', 'CorpusId': 6226377}",['Computer Science'],0.0,False,{'pages': '75-76'},10/5/2014,['JournalArticle'],3.0,Trainer: a motion-based interactive game for balance rehabilitation training,https://www.semanticscholar.org/paper/15d42cdbfd0014123e8cd2eb52d538b5f2a80c5d,UIST,2014
2079750751,"Although graphical user interfaces started as imitations of the physical world, many interaction techniques have since been invented that are not available in the real world. This paper focuses on one of these ""previewing"", and how a sensory enhanced input device called ""PreSense Keypad"" can provide a preview for users before they actually execute the commands. Preview important in the real world because it is often not possible to undo an action. This previewable feature helps users to see what will occur next. It is also helpful when the command assignment of the keypad dynamically changes, such as for universal commanders. We present several interaction techniques based on this input device, including menu and map browsing systems and a text input system. We also discuss finger gesture recognition for the PreSense Keypad.",1.0,"Although graphical user interfaces started as imitations of the physical world, many interaction techniques have since been invented that are not available in the real world. This paper focuses on one of these ""previewing"", and how a sensory enhanced input device called ""PreSense Keypad"" can provide a preview for users before they actually execute the commands. Preview important in the real world because it is often not possible to undo an action. This previewable feature helps users to see what will occur next. It is also helpful when the command assignment of the keypad dynamically changes, such as for universal commanders. We present several interaction techniques based on this input device, including menu and map browsing systems and a text input system. We also discuss finger gesture recognition for the PreSense Keypad.","['Interaction Laboratory, Sony Computer Science Laboratories, Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 141-0022 Japan', 'Graduate School of Media and Governance, Keio University, Shonan Fujisawa Campus, 5322 Endo, Fujisawa 252-8520 Japan#TAB#', 'Interaction Laboratory, Sony Computer Science Laboratories, Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 141-0022 Japan', 'Interaction Laboratory, Sony Computer Science Laboratories, Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 141-0022 Japan']","['1994375393', '2137433080', '2275566519', '2582093778']",2079750751.0,"{'offset': 0, 'data': [{'authorId': '1685962', 'url': 'https://www.semanticscholar.org/author/1685962', 'name': 'J. Rekimoto', 'affiliations': [], 'homepage': None, 'paperCount': 312, 'citationCount': 11594}, {'authorId': '2430508', 'url': 'https://www.semanticscholar.org/author/2430508', 'name': 'Takaaki Ishizawa', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 234}, {'authorId': '2270984', 'url': 'https://www.semanticscholar.org/author/2270984', 'name': 'Carsten Schwesig', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 1244}, {'authorId': '2370803', 'url': 'https://www.semanticscholar.org/author/2370803', 'name': 'Haruo Oba', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 451}]}",73.0,"{'DBLP': 'conf/uist/RekimotoISO03', 'MAG': '2079750751', 'DOI': '10.1145/964696.964719', 'CorpusId': 2552425}",['Computer Science'],3.0,False,{'pages': '203-212'},11/2/2003,"['JournalArticle', 'Review']",17.0,PreSense: interaction techniques for finger sensing input devices,https://www.semanticscholar.org/paper/d285e720e2b94947940ace119bbdb71cf859c123,UIST,2003
2082073215,"We demonstrate a realtime system which infers and tracks the assembly process of a snap-together block model using a Kinect® sensor. The inference enables us to build a virtual replica of the model at every step. Tracking enables us to provide context specific visual feedback on a screen by augmenting the rendered virtual model aligned with the physical model. The system allows users to author a new model and uses the inferred assembly process to guide its recreation by others. We propose a novel way of assembly guidance where the next block to be added is rendered in blinking mode with the tracked virtual model on screen. The system is also able to detect any mistakes made and helps correct them by providing appropriate feedback. We focus on assemblies of Duplo® blocks. We discuss the shortcomings of existing methods of guidance - static figures or recorded videos - and demonstrate how our method avoids those shortcomings. We also report on a user study to compare our system with standard figure-based guidance methods found in user manuals. The results of the user study suggest that our method is able to aid users' structural perception of the model better, leads to fewer assembly errors, and reduces model construction time.",1.0,"We demonstrate a realtime system which infers and tracks the assembly process of a snap-together block model using a Kinect® sensor. The inference enables us to build a virtual replica of the model at every step. Tracking enables us to provide context specific visual feedback on a screen by augmenting the rendered virtual model aligned with the physical model. The system allows users to author a new model and uses the inferred assembly process to guide its recreation by others. We propose a novel way of assembly guidance where the next block to be added is rendered in blinking mode with the tracked virtual model on screen. The system is also able to detect any mistakes made and helps correct them by providing appropriate feedback. We focus on assemblies of Duplo® blocks. We discuss the shortcomings of existing methods of guidance - static figures or recorded videos - and demonstrate how our method avoids those shortcomings. We also report on a user study to compare our system with standard figure-based guidance methods found in user manuals. The results of the user study suggest that our method is able to aid users' structural perception of the model better, leads to fewer assembly errors, and reduces model construction time.","['University of Washington Seattle, Washington, USA#TAB#', 'University of Washington Seattle, Washington, USA#TAB#', 'Microsoft Research Redmond Washington USA', 'University of Washington Seattle, Washington, USA#TAB#']","['2210585852', '2231782831', '3049172691', '511526681']",2082073215.0,"{'offset': 0, 'data': [{'authorId': '2110760400', 'url': 'https://www.semanticscholar.org/author/2110760400', 'name': 'Ankit Gupta', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 684}, {'authorId': '145197953', 'url': 'https://www.semanticscholar.org/author/145197953', 'name': 'D. Fox', 'affiliations': [], 'homepage': None, 'paperCount': 404, 'citationCount': 56741}, {'authorId': '143800609', 'url': 'https://www.semanticscholar.org/author/143800609', 'name': 'B. Curless', 'affiliations': [], 'homepage': None, 'paperCount': 135, 'citationCount': 24154}, {'authorId': '1400248273', 'url': 'https://www.semanticscholar.org/author/1400248273', 'name': 'Michael F. Cohen', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 20971}]}",103.0,"{'DBLP': 'conf/uist/GuptaFCC12', 'MAG': '2082073215', 'DOI': '10.1145/2380116.2380167', 'CorpusId': 14544844}",['Computer Science'],4.0,False,{'name': 'Proceedings of the 25th annual ACM symposium on User interface software and technology'},10/7/2012,"['Book', 'JournalArticle', 'Conference']",27.0,DuploTrack: a real-time system for authoring and guiding duplo block assembly,https://www.semanticscholar.org/paper/563c01818b54ca85ed5970dc12c5ca379f146f7f,UIST,2012
2084385548,"The construction and maintenance of interactive user interfaces have been simplified by the development of a generation of software tools. The tools range from window managers, toolkits, and widget sets to user interface management systems and knowledge-based design assistants. However, only a small number of the tools attempt to incorporate principles of good design. They offer no help with decisions regarding the variety of input devices and methods available. In this paper we briefly describe a methodology for interaction technique selection based on natural physical analogs of the application tasks. Special emphasis is given to the physical characteristics of input devices and the pragmatics of their use. The methodology is incorporated in a software environment named Toto which includes knowledge acquired from a variety of disciplines such as: semiotics, ergonomics, and industrial design. Toto also incorporates a set of interactive tools for modifying the knowledge and for supporting the selection of natural interaction techniques. A two phased design process (matching followed by sequencing) is embedded in the Toto rule base. Examples of the use of Toto tools are provided to illustrate the design process.",1.0,"The construction and maintenance of interactive user interfaces have been simplified by the development of a generation of software tools. The tools range from window managers, toolkits, and widget sets to user interface management systems and knowledge-based design assistants. However, only a small number of the tools attempt to incorporate principles of good design. They offer no help with decisions regarding the variety of input devices and methods available. In this paper we briefly describe a methodology for interaction technique selection based on natural physical analogs of the application tasks. Special emphasis is given to the physical characteristics of input devices and the pragmatics of their use. The methodology is incorporated in a software environment named Toto which includes knowledge acquired from a variety of disciplines such as: semiotics, ergonomics, and industrial design. Toto also incorporates a set of interactive tools for modifying the knowledge and for supporting the selection of natural interaction techniques. A two phased design process (matching followed by sequencing) is embedded in the Toto rule base. Examples of the use of Toto tools are provided to illustrate the design process.","['Department of Electrical Engineering and Computer Science, The George Washington University, Washington DC#TAB#', 'Department of Electrical Engineering and Computer Science, The George Washington University, Washington DC#TAB#']","['2000806650', '361874762']",2084385548.0,"{'offset': 0, 'data': [{'authorId': '2617338', 'url': 'https://www.semanticscholar.org/author/2617338', 'name': 'T. Bleser', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 259}, {'authorId': '1796576', 'url': 'https://www.semanticscholar.org/author/1796576', 'name': 'J. Sibert', 'affiliations': [], 'homepage': None, 'paperCount': 62, 'citationCount': 2946}]}",37.0,"{'DBLP': 'conf/uist/BleserS90', 'MAG': '2084385548', 'DOI': '10.1145/97924.97940', 'CorpusId': 14428543}",['Computer Science'],1.0,False,{'pages': '135-142'},8/1/1990,['JournalArticle'],18.0,Toto: a tool for selecting interaction techniques,https://www.semanticscholar.org/paper/083b8ca6082b20ba165867fc0e77ea01b8c25f06,UIST,1990
2086937593,"A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done “by hand,” making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler.",1.0,"A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done “by hand,” making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler.","['Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA', 'Human–Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA']","['2120182675', '2125109296', '2165755729', '2171298838']",2086937593.0,"{'offset': 0, 'data': [{'authorId': '1749296', 'url': 'https://www.semanticscholar.org/author/1749296', 'name': 'S. Hudson', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 13652}, {'authorId': '1688844', 'url': 'https://www.semanticscholar.org/author/1688844', 'name': 'Bonnie E. John', 'affiliations': [], 'homepage': None, 'paperCount': 178, 'citationCount': 6939}, {'authorId': '145437567', 'url': 'https://www.semanticscholar.org/author/145437567', 'name': 'K. Knudsen', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 96}, {'authorId': '1731836', 'url': 'https://www.semanticscholar.org/author/1731836', 'name': 'M. Byrne', 'affiliations': [], 'homepage': None, 'paperCount': 164, 'citationCount': 6954}]}",70.0,"{'MAG': '2086937593', 'DBLP': 'conf/uist/HudsonJKB99', 'DOI': '10.1145/320719.322590', 'CorpusId': 9380814}",['Computer Science'],5.0,False,{'pages': '93-102'},11/7/1999,['JournalArticle'],40.0,A tool for creating predictive performance models from user interface demonstrations,https://www.semanticscholar.org/paper/5da763d094e56e692a76cdb102e24d0f5436ff02,UIST,1999
2089932758,"While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These highresolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a “sweet spot” within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary lowresohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface. CR",1.0,"While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These highresolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a “sweet spot” within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary lowresohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface. CR","['Department of Computer Science, Columbia University New York, NY#TAB#', 'Department of Computer Science, Columbia University New York, NY#TAB#']","['2227862677', '252695556']",2089932758.0,"{'offset': 0, 'data': [{'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}, {'authorId': '3137941', 'url': 'https://www.semanticscholar.org/author/3137941', 'name': 'Ari Shamash', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 117}]}",117.0,"{'DBLP': 'conf/uist/FeinerS91', 'MAG': '2089932758', 'DOI': '10.1145/120782.120783', 'CorpusId': 17837941}",['Computer Science'],9.0,False,{'pages': '9-17'},11/11/1991,['JournalArticle'],40.0,Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers,https://www.semanticscholar.org/paper/1ae17e45e8b32969a3127400eb6525d2ccd93c3a,UIST,1991
2102895743,"We describe some common problems experienced by users of computer-based text chat, and show how many of these problems relate to the loss of timing-specific information. We suggest that thinking of chat as a real-time streaming media data type, with status and channel indicators, might solve some of these problems. We then present a number of alternative chat interfaces along with results from user studies comparing and contrasting them both with each other and with the standard chat interface. These studies show some potential, but indicate that more work needs to be done.",1.0,"We describe some common problems experienced by users of computer-based text chat, and show how many of these problems relate to the loss of timing-specific information. We suggest that thinking of chat as a real-time streaming media data type, with status and channel indicators, might solve some of these problems. We then present a number of alternative chat interfaces along with results from user studies comparing and contrasting them both with each other and with the standard chat interface. These studies show some potential, but indicate that more work needs to be done.","['Virtual Worlds Group, Microsoft Research, One Microsoft Way, Redmond, WA#TAB#', 'Virtual Worlds Group, Microsoft Research, One Microsoft Way, Redmond, WA#TAB#', 'Virtual Worlds Group, Microsoft Research, One Microsoft Way, Redmond, WA#TAB#']","['14227721', '2086485667', '2119113135']",2102895743.0,"{'offset': 0, 'data': [{'authorId': '2615189', 'url': 'https://www.semanticscholar.org/author/2615189', 'name': 'David Vronay', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 231}, {'authorId': '1410176686', 'url': 'https://www.semanticscholar.org/author/1410176686', 'name': 'Marc A. Smith', 'affiliations': [], 'homepage': None, 'paperCount': 135, 'citationCount': 8690}, {'authorId': '2311676', 'url': 'https://www.semanticscholar.org/author/2311676', 'name': 'S. Drucker', 'affiliations': [], 'homepage': None, 'paperCount': 151, 'citationCount': 11156}]}",84.0,"{'MAG': '2102895743', 'DBLP': 'conf/uist/VronaySD99', 'DOI': '10.1145/320719.322579', 'CorpusId': 11817470}",['Computer Science'],13.0,False,{'pages': '19-26'},11/7/1999,['JournalArticle'],18.0,Alternative interfaces for chat,https://www.semanticscholar.org/paper/48ba8504f713ee080323ffc4d490b3063908c6b6,UIST,1999
2111995343,"Current asynchronous voice messaging interfaces, like voicemail, fail to take advantage of our conversational skills. TalkBack restores conversational turn-taking to voicemail retrieval by dividing voice messages into smaller sections based on the most significant silent and filled pauses and pausing after each to record a response. The responses are composed into a reply, alternating with snippets of the original message for context. TalkBack is built into a digital picture frame; the recipient touches a picture of the caller to hear each segment of the message in turn. The minimal interface models synchronous interaction and facilitates asynchronous voice messaging. TalkBack can also present a voice-annotated slide show which it receives over the Internet.",1.0,"Current asynchronous voice messaging interfaces, like voicemail, fail to take advantage of our conversational skills. TalkBack restores conversational turn-taking to voicemail retrieval by dividing voice messages into smaller sections based on the most significant silent and filled pauses and pausing after each to record a response. The responses are composed into a reply, alternating with snippets of the original message for context. TalkBack is built into a digital picture frame; the recipient touches a picture of the caller to hear each segment of the message in turn. The minimal interface models synchronous interaction and facilitates asynchronous voice messaging. TalkBack can also present a voice-annotated slide show which it receives over the Internet.","['MIT Media Lab, Cambridge, Ma.#TAB#', 'MIT Media Lab, Cambridge, Ma.#TAB#', 'MIT Media Lab, Cambridge, Ma.#TAB#']","['170421923', '1999277998', '2042299866']",2111995343.0,"{'offset': 0, 'data': [{'authorId': '2098674', 'url': 'https://www.semanticscholar.org/author/2098674', 'name': 'V. Lakshmipathy', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 27}, {'authorId': '1729321', 'url': 'https://www.semanticscholar.org/author/1729321', 'name': 'C. Schmandt', 'affiliations': [], 'homepage': None, 'paperCount': 132, 'citationCount': 5206}, {'authorId': '2584860', 'url': 'https://www.semanticscholar.org/author/2584860', 'name': 'Natalia Marmasse', 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 1017}]}",19.0,"{'DBLP': 'conf/uist/LakshmipathySM03', 'MAG': '2111995343', 'DOI': '10.1145/964696.964701', 'CorpusId': 2883745}",['Computer Science'],0.0,False,{'pages': '41-50'},11/2/2003,['JournalArticle'],24.0,TalkBack: a conversational answering machine,https://www.semanticscholar.org/paper/1ca941f5daa78d146cd48eeb0a2c7f5f40099775,UIST,2003
2115181155,"Software libraries for most of the modern programming languages are numerous, large and complex. Remembering the syntax and usage of APIs is a difficult task for not just novices but also expert programmers. IDEs (Integrated Development Environment) provide capabilities like autocomplete and intellisense to assist programmers; however, programmers still need to visit search engines like Google to find API (Application Program Interface) documentation and samples. This paper evaluates Redprint - a browser based development environment for PHP that integrates API specific ""Instant Example"" and ""Instant Documentation"" display interfaces. A comparative laboratory study shows that integrating API specific ""Instant Example"" and ""Instant Documentation"" display interfaces into a development environment significantly reduces the cost of searching and thus significantly reduces the time to develop software.",0.0,"Software libraries for most of the modern programming languages are numerous, large and complex. Remembering the syntax and usage of APIs is a difficult task for not just novices but also expert programmers. IDEs (Integrated Development Environment) provide capabilities like autocomplete and intellisense to assist programmers; however, programmers still need to visit search engines like Google to find API (Application Program Interface) documentation and samples. This paper evaluates Redprint - a browser based development environment for PHP that integrates API specific ""Instant Example"" and ""Instant Documentation"" display interfaces. A comparative laboratory study shows that integrating API specific ""Instant Example"" and ""Instant Documentation"" display interfaces into a development environment significantly reduces the cost of searching and thus significantly reduces the time to develop software.","['Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)']","['2158820271', '2225213881', '261822931']",2115181155.0,"{'offset': 0, 'data': [{'authorId': '3202553', 'url': 'https://www.semanticscholar.org/author/3202553', 'name': 'Anant P. Bhardwaj', 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 319}, {'authorId': '70657866', 'url': 'https://www.semanticscholar.org/author/70657866', 'name': 'David Luciano', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 3}, {'authorId': '21520403', 'url': 'https://www.semanticscholar.org/author/21520403', 'name': 'Scott R. Klemmer', 'affiliations': [], 'homepage': None, 'paperCount': 182, 'citationCount': 8620}]}",3.0,"{'MAG': '2952447451', 'ArXiv': '1409.6678', 'DBLP': 'conf/uist/BhardwajLK11', 'DOI': '10.1145/2046396.2046408', 'CorpusId': 23031454}",['Computer Science'],1.0,False,"{'name': 'ArXiv', 'volume': 'abs/1409.6678'}",10/16/2011,['JournalArticle'],5.0,"Redprint: integrating API specific ""instant example"" and ""instant documentation"" display interface in IDEs",https://www.semanticscholar.org/paper/8feb891b1b211310a35289e95e381844dbec6eea,UIST,2011
2118424978,"The Power-up Button is a physical button that combines pressure and proximity sensing to enable gestural interaction with one thumb. Combined with a gesture recognizer that takes the hand's anatomy into account, the Power-up Button can recognize six different mid-air gestures performed on the side of a mobile device. This gives it, for instance, enough expressive power to provide full one-handed control of interface widgets displayed on screen. This technology can complement touch input, and can be particularly useful when interacting eyes-free. It also opens up a larger design space for widget organization on screen: the button enables a more compact layout of interface components than what touch input alone would allow. This can be useful when, e.g., filling the numerous fields of a long Web form, or for very small devices.",1.0,"The Power-up Button is a physical button that combines pressure and proximity sensing to enable gestural interaction with one thumb. Combined with a gesture recognizer that takes the hand's anatomy into account, the Power-up Button can recognize six different mid-air gestures performed on the side of a mobile device. This gives it, for instance, enough expressive power to provide full one-handed control of interface widgets displayed on screen. This technology can complement touch input, and can be particularly useful when interacting eyes-free. It also opens up a larger design space for widget organization on screen: the button enables a more compact layout of interface components than what touch input alone would allow. This can be useful when, e.g., filling the numerous fields of a long Web form, or for very small devices.","['INRIA Orsay, France#TAB#', 'Univ Paris-Sud, Orsay, France', 'Univ Paris-Sud, Orsay, France', 'INRIA, Orsay, France & INRIA Chile, Orsay, France']","['1285430303', '166732564', '1927534563', '36478446']",2118424978.0,"{'offset': 0, 'data': [{'authorId': '2264859', 'url': 'https://www.semanticscholar.org/author/2264859', 'name': 'Daniel Spelmezan', 'affiliations': [], 'homepage': None, 'paperCount': 17, 'citationCount': 532}, {'authorId': '1802574', 'url': 'https://www.semanticscholar.org/author/1802574', 'name': 'Caroline Appert', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 1393}, {'authorId': '3342979', 'url': 'https://www.semanticscholar.org/author/3342979', 'name': 'O. Chapuis', 'affiliations': [], 'homepage': None, 'paperCount': 81, 'citationCount': 1925}, {'authorId': '1728256', 'url': 'https://www.semanticscholar.org/author/1728256', 'name': 'E. Pietriga', 'affiliations': [], 'homepage': None, 'paperCount': 89, 'citationCount': 2435}]}",19.0,"{'MAG': '2118424978', 'DBLP': 'conf/uist/SpelmezanACP13', 'DOI': '10.1145/2501988.2502025', 'CorpusId': 220627190}",['Computer Science'],2.0,True,{'name': 'Proceedings of the 26th annual ACM symposium on User interface software and technology'},10/8/2013,"['Book', 'JournalArticle', 'Conference']",11.0,Controlling widgets with one power-up button,https://www.semanticscholar.org/paper/405a7367e8233ee204b6d173eb1caeacbfd8810f,UIST,2013
2118803988,"We present a variant of hierarchical marking menus where items are selected using a series of inflection-free simple marks, rather than the single ""zig-zag"" compound mark used in the traditional design. Theoretical analysis indicates that this simple mark approach has the potential to significantly increase the number of items in a marking menu that can be selected efficiently and accurately. A user experiment is presented that compares the simple and compound mark techniques. Results show that the simple mark technique allows for significantly more accurate and faster menu selections overall, but most importantly also in menus with a large number of items where performance of the compound mark technique is particularly poor. The simple mark technique also requires significantly less physical input space to perform the selections, making it particularly suitable for small footprint pen-based input devices. Visual design alternatives are also discussed.",1.0,"We present a variant of hierarchical marking menus where items are selected using a series of inflection-free simple marks, rather than the single ""zig-zag"" compound mark used in the traditional design. Theoretical analysis indicates that this simple mark approach has the potential to significantly increase the number of items in a marking menu that can be selected efficiently and accurately. A user experiment is presented that compares the simple and compound mark techniques. Results show that the simple mark technique allows for significantly more accurate and faster menu selections overall, but most importantly also in menus with a large number of items where performance of the compound mark technique is particularly poor. The simple mark technique also requires significantly less physical input space to perform the selections, making it particularly suitable for small footprint pen-based input devices. Visual design alternatives are also discussed.","['University of Toronto', 'University of Toronto']","['2130130894', '2168325223']",2118803988.0,"{'offset': 0, 'data': [{'authorId': '2645457', 'url': 'https://www.semanticscholar.org/author/2645457', 'name': 'Shengdong Zhao', 'affiliations': [], 'homepage': None, 'paperCount': 142, 'citationCount': 2765}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",150.0,"{'MAG': '2118803988', 'DBLP': 'conf/uist/ZhaoB04', 'DOI': '10.1145/1029632.1029639', 'CorpusId': 3978129}",['Computer Science'],17.0,True,{'pages': '33-42'},10/24/2004,['JournalArticle'],18.0,Simple vs. compound mark hierarchical marking menus,https://www.semanticscholar.org/paper/932bd00f75d2db4bc2249c428c65165f8a39bf35,UIST,2004
2119581172,"In this paper we present a novel interface for selecting sounds in audio mixtures. Traditional interfaces in audio editors provide a graphical representation of sounds which is either a waveform, or some variation of a time/frequency transform. Although with these representations a user might be able to visually identify elements of sounds in a mixture, they do not facilitate object-specific editing (e.g. selecting only the voice of a singer in a song). This interface uses audio guidance from a user in order to select a target sound within a mixture. The user is asked to vocalize (or otherwise sonically represent) the desired target sound, and an automatic process identifies and isolates the elements of the mixture that best relate to the user's input. This way of pointing to specific parts of an audio stream allows a user to perform audio selections which would have been infeasible otherwise.",0.0,"In this paper we present a novel interface for selecting sounds in audio mixtures. Traditional interfaces in audio editors provide a graphical representation of sounds which is either a waveform, or some variation of a time/frequency transform. Although with these representations a user might be able to visually identify elements of sounds in a mixture, they do not facilitate object-specific editing (e.g. selecting only the voice of a singer in a song). This interface uses audio guidance from a user in order to select a target sound within a mixture. The user is asked to vocalize (or otherwise sonically represent) the desired target sound, and an automatic process identifies and isolates the elements of the mixture that best relate to the user's input. This way of pointing to specific parts of an audio stream allows a user to perform audio selections which would have been infeasible otherwise.","['Adobe Systems Inc Newton, MA USA']",['1984479034'],2119581172.0,"{'offset': 0, 'data': [{'authorId': '1718742', 'url': 'https://www.semanticscholar.org/author/1718742', 'name': 'P. Smaragdis', 'affiliations': [], 'homepage': None, 'paperCount': 194, 'citationCount': 8883}]}",11.0,"{'DBLP': 'conf/uist/Smaragdis09', 'MAG': '2119581172', 'DOI': '10.1145/1622176.1622193', 'CorpusId': 15791063}",['Computer Science'],0.0,False,{'pages': '89-92'},10/4/2009,['JournalArticle'],13.0,User guided audio selection from complex sound mixtures,https://www.semanticscholar.org/paper/f07d4bb1978ad29f13059e6453632e606e8637c1,UIST,2009
2125228090,"Modern brain sensing technologies provide a variety of methods for detecting specific forms of brain activity. In this paper, we present an initial step in exploring how these technologies may be used to perform task classification and applied in a relevant manner to HCI research. We describe two experiments showing successful classification between tasks using a low-cost off-the-shelf electroencephalograph (EEG) system. In the first study, we achieved a mean classification accuracy of 84.0% in subjects performing one of three cognitive tasks - rest, mental arithmetic, and mental rotation - while sitting in a controlled posture. In the second study, conducted in more ecologically valid setting for HCI research, we attained a mean classification accuracy of 92.4% using three tasks that included non-cognitive features: a relaxation task, playing a PC based game without opponents, and engaging opponents within the game. Throughout the paper, we provide lessons learned and discuss how HCI researchers may utilize these technologies in their work.",1.0,"Modern brain sensing technologies provide a variety of methods for detecting specific forms of brain activity. In this paper, we present an initial step in exploring how these technologies may be used to perform task classification and applied in a relevant manner to HCI research. We describe two experiments showing successful classification between tasks using a low-cost off-the-shelf electroencephalograph (EEG) system. In the first study, we achieved a mean classification accuracy of 84.0% in subjects performing one of three cognitive tasks - rest, mental arithmetic, and mental rotation - while sitting in a controlled posture. In the second study, conducted in more ecologically valid setting for HCI research, we attained a mean classification accuracy of 92.4% using three tasks that included non-cognitive features: a relaxation task, playing a PC based game without opponents, and engaging opponents within the game. Throughout the paper, we provide lessons learned and discuss how HCI researchers may utilize these technologies in their work.","['Carnegie Mellon University, Pittsburgh, PA.', '[Microsoft research, Redmond, WA]']","['2135985964', '2168727892']",2125228090.0,"{'offset': 0, 'data': [{'authorId': '34741611', 'url': 'https://www.semanticscholar.org/author/34741611', 'name': 'J. C. Lee', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 2446}, {'authorId': '1719056', 'url': 'https://www.semanticscholar.org/author/1719056', 'name': 'Desney S. Tan', 'affiliations': [], 'homepage': None, 'paperCount': 151, 'citationCount': 9894}]}",184.0,"{'DBLP': 'conf/uist/LeeT06', 'MAG': '2125228090', 'DOI': '10.1145/1166253.1166268', 'CorpusId': 3069929}",['Computer Science'],19.0,False,{'pages': '81-90'},10/15/2006,"['JournalArticle', 'Conference']",27.0,Using a low-cost electroencephalograph for task classification in HCI research,https://www.semanticscholar.org/paper/1692fcf70d6515fc7269b765282daa954bcbe03f,UIST,2006
2128039206,"We present <i>content-aware free-space transparency</i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.",1.0,"We present <i>content-aware free-space transparency</i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.","['Columbia University, New York , NY', 'Columbia University, New York , NY']","['1974926465', '252695556']",2128039206.0,"{'offset': 0, 'data': [{'authorId': '2192767', 'url': 'https://www.semanticscholar.org/author/2192767', 'name': 'Edward W. Ishak', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 509}, {'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}]}",63.0,"{'DBLP': 'conf/uist/IshakF04', 'MAG': '2128039206', 'DOI': '10.1145/1029632.1029666', 'CorpusId': 3988832}",['Computer Science'],5.0,False,{'pages': '189-192'},10/24/2004,['JournalArticle'],16.0,Interacting with hidden content using content-aware free-space transparency,https://www.semanticscholar.org/paper/18868cc02732042da589d0206322aab463f92dfa,UIST,2004
2130972848,"The availability of low-cost digital fabrication devices enables new groups of users to participate in the design and fabrication of things. However, software to assist in the transition from design to actual fabrication is currently overlooked. In this paper, we introduce PacCAM, a system for packing 2D parts within a given source material for fabrication using 2D cutting machines. Our solution combines computer vision to capture the source material shape with a user interface that incorporates 2D rigid body simulation and snapping. A user study demonstrated that participants could make layouts faster with our system compared with using traditional drafting tools. PacCAM caters to a variety of 2D fabrication applications and can contribute to the reduction of material waste.",0.0,"The availability of low-cost digital fabrication devices enables new groups of users to participate in the design and fabrication of things. However, software to assist in the transition from design to actual fabrication is currently overlooked. In this paper, we introduce PacCAM, a system for packing 2D parts within a given source material for fabrication using 2D cutting machines. Our solution combines computer vision to capture the source material shape with a user interface that incorporates 2D rigid body simulation and snapping. A user study demonstrated that participants could make layouts faster with our system compared with using traditional drafting tools. PacCAM caters to a variety of 2D fabrication applications and can contribute to the reduction of material waste.","['JST ERATO Igarashi Design Interface Project and University of Tsukuba', 'JST ERATO Igarashi Design Interface Project', 'JST ERATO Igarashi Design Interface Project and The University of Tokyo#TAB#', 'JST ERATO Igarashi Design Interface Project']","['2047412455', '2146764157', '2152110089', '2222376969']",2130972848.0,"{'offset': 0, 'data': [{'authorId': '2691594', 'url': 'https://www.semanticscholar.org/author/2691594', 'name': 'D. Saakes', 'affiliations': [], 'homepage': None, 'paperCount': 85, 'citationCount': 635}, {'authorId': '2577384', 'url': 'https://www.semanticscholar.org/author/2577384', 'name': 'Thomas Cambazard', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 33}, {'authorId': '2618827', 'url': 'https://www.semanticscholar.org/author/2618827', 'name': 'J. Mitani', 'affiliations': [], 'homepage': None, 'paperCount': 169, 'citationCount': 2130}, {'authorId': '1717356', 'url': 'https://www.semanticscholar.org/author/1717356', 'name': 'T. Igarashi', 'affiliations': [], 'homepage': None, 'paperCount': 465, 'citationCount': 11915}]}",33.0,"{'DBLP': 'conf/uist/SaakesCMI13', 'MAG': '2130972848', 'DOI': '10.1145/2501988.2501990', 'CorpusId': 1337857}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 26th annual ACM symposium on User interface software and technology'},10/8/2013,"['Book', 'JournalArticle', 'Conference']",35.0,PacCAM: material capture and interactive 2D packing for efficient material usage on CNC cutting machines,https://www.semanticscholar.org/paper/736e23c32c58f68ac865320bf336c6e84112dd00,UIST,2013
2131543246,"Video is a complex information space that requires advanced navigational aids for effective browsing. The increasing number of temporal video annotations offers new opportunities to provide video navigation according to a user's needs. We present a novel video browsing interface called TAV (Temporal Annotation Viewing) that provides the user with a visual overview of temporal video annotations. TAV enables the user to quickly determine the general content of a video, the location of scenes of interest and the type of annotations that are displayed while watching the video. An ongoing user study will evaluate our novel approach.",0.0,"Video is a complex information space that requires advanced navigational aids for effective browsing. The increasing number of temporal video annotations offers new opportunities to provide video navigation according to a user's needs. We present a novel video browsing interface called TAV (Temporal Annotation Viewing) that provides the user with a visual overview of temporal video annotations. TAV enables the user to quickly determine the general content of a video, the location of scenes of interest and the type of annotations that are displayed while watching the video. An ongoing user study will evaluate our novel approach.","['University of British Columbia, Vancouver, BC,  Canada;', 'University of British Columbia, Vancouver, BC,  Canada;', 'University of Applied Sciences Harz, Harz, Germany']","['2044722117', '2186658169', '2717236272']",2131543246.0,"{'offset': 0, 'data': [{'authorId': '2112730313', 'url': 'https://www.semanticscholar.org/author/2112730313', 'name': 'Stefanie Müller', 'affiliations': [], 'homepage': None, 'paperCount': 9, 'citationCount': 26}, {'authorId': '40637073', 'url': 'https://www.semanticscholar.org/author/40637073', 'name': 'G. Miller', 'affiliations': [], 'homepage': None, 'paperCount': 49, 'citationCount': 511}, {'authorId': '23111666', 'url': 'https://www.semanticscholar.org/author/23111666', 'name': 'S. Fels', 'affiliations': [], 'homepage': None, 'paperCount': 445, 'citationCount': 7065}]}",8.0,"{'DBLP': 'conf/uist/MullerMF10', 'MAG': '2131543246', 'DOI': '10.1145/1866218.1866263', 'CorpusId': 12985466}",['Computer Science'],0.0,False,{'pages': '445-446'},10/3/2010,"['JournalArticle', 'Review']",3.0,Using temporal video annotation as a navigational aid for video browsing,https://www.semanticscholar.org/paper/23f39271ee39279b2e95fae0f31e9e1fe9e0e5b0,UIST,2010
2133241474,"Data quality is critical for many information-intensive applications. One of the best opportunities to improve data quality is during entry. Usher provides a theoretical, data-driven foundation for improving data quality during entry. Based on prior data, Usher learns a probabilistic model of the dependencies between form questions and values. Using this information, Usher maximizes information gain. By asking the most unpredictable questions first, Usher is better able to predict answers for the remaining questions. In this paper, we use Usher's predictive ability to design a number of intelligent user interface adaptations that improve data entry accuracy and efficiency. Based on an underlying cognitive model of data entry, we apply these modifications before, during and after committing an answer. We evaluated these mechanisms with professional data entry clerks working with real patient data from six clinics in rural Uganda. The results show that our adaptations have the potential to reduce error (by up to 78%), with limited effect on entry time (varying between -14% and +6%). We believe this approach has wide applicability for improving the quality and availability of data, which is increasingly important for decision-making and resource allocation.",0.0,"Data quality is critical for many information-intensive applications. One of the best opportunities to improve data quality is during entry. Usher provides a theoretical, data-driven foundation for improving data quality during entry. Based on prior data, Usher learns a probabilistic model of the dependencies between form questions and values. Using this information, Usher maximizes information gain. By asking the most unpredictable questions first, Usher is better able to predict answers for the remaining questions. In this paper, we use Usher's predictive ability to design a number of intelligent user interface adaptations that improve data entry accuracy and efficiency. Based on an underlying cognitive model of data entry, we apply these modifications before, during and after committing an answer. We evaluated these mechanisms with professional data entry clerks working with real patient data from six clinics in rural Uganda. The results show that our adaptations have the potential to reduce error (by up to 78%), with limited effect on entry time (varying between -14% and +6%). We believe this approach has wide applicability for improving the quality and availability of data, which is increasingly important for decision-making and resource allocation.","['University of California Berkeley, Berkeley, CA, USA ', 'University of California Berkeley, Berkeley, CA, USA ', 'University of California Berkeley, Berkeley, CA, USA ']","['2063640528', '2152765731', '2325120967']",2133241474.0,"{'offset': 0, 'data': [{'authorId': '1643678934', 'url': 'https://www.semanticscholar.org/author/1643678934', 'name': 'Kuang Chen', 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 506}, {'authorId': '1695576', 'url': 'https://www.semanticscholar.org/author/1695576', 'name': 'J. Hellerstein', 'affiliations': [], 'homepage': None, 'paperCount': 397, 'citationCount': 40068}, {'authorId': '1755518', 'url': 'https://www.semanticscholar.org/author/1755518', 'name': 'Tapan S. Parikh', 'affiliations': [], 'homepage': None, 'paperCount': 97, 'citationCount': 3027}]}",27.0,"{'DBLP': 'conf/uist/ChenHP10', 'MAG': '2133241474', 'DOI': '10.1145/1866029.1866068', 'CorpusId': 3247659}",['Computer Science'],0.0,True,{'name': 'Proceedings of the 23nd annual ACM symposium on User interface software and technology'},10/3/2010,"['Book', 'JournalArticle', 'Conference']",39.0,Designing adaptive feedback for improving data entry accuracy,https://www.semanticscholar.org/paper/f9b370cafdc0808aba85dad6597b7f6a92b9c4b2,UIST,2010
2135539886,"SpeechSkimmer is an interactive system for quickly browsing and finding information in speech recordings. Skimming speech recordings is much more difficult than visually scanning images, text, or video because of the slow, linear, temporal nature of the audio channel. The SpeechSkimmer system uses a combination of (1) time compression and pause removal, (2) automatically finding segments that summarize a recording, and (3) interaction techniques, to enable a speech recording to be heard quickly and at several levels of detail.",0.0,"SpeechSkimmer is an interactive system for quickly browsing and finding information in speech recordings. Skimming speech recordings is much more difficult than visually scanning images, text, or video because of the slow, linear, temporal nature of the audio channel. The SpeechSkimmer system uses a combination of (1) time compression and pause removal, (2) automatically finding segments that summarize a recording, and (3) interaction techniques, to enable a speech recording to be heard quickly and at several levels of detail.","['Speech Interaction Research, PO Box 14, Cambridge MA']",['2145867285'],2135539886.0,"{'offset': 0, 'data': [{'authorId': '1931458', 'url': 'https://www.semanticscholar.org/author/1931458', 'name': 'B. Arons', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 1475}]}",3.0,"{'DBLP': 'conf/uist/Arons95', 'MAG': '2135539886', 'DOI': '10.1145/215585.215657', 'CorpusId': 29661396}",['Computer Science'],0.0,False,{'pages': '71-72'},12/1/1995,['JournalArticle'],5.0,Hands-on demonstration: interacting with SpeechSkimmer,https://www.semanticscholar.org/paper/adffb9bc35fc768006fc19975f002b79fc7bb3d1,UIST,1995
2138482372,"Interfaces for conceptual and creative design should recognize and interpret drawings. They should also capture users’ intended ambiguity, vagueness, and imprecision and convey these qualities visually and through interactive behavior. Freehand drawing can provide this information and it is a natural input mode for design. We describe a pen-based interface that acquires information about ambiguity and precision from freehand input, represents it internally, and echoes it to users visually and through constraint based edit behavior.",1.0,"Interfaces for conceptual and creative design should recognize and interpret drawings. They should also capture users’ intended ambiguity, vagueness, and imprecision and convey these qualities visually and through interactive behavior. Freehand drawing can provide this information and it is a natural input mode for design. We describe a pen-based interface that acquires information about ambiguity and precision from freehand input, represents it internally, and echoes it to users visually and through constraint based edit behavior.","['College of Architecture Georgia Institute of Technology Atlanta, GA', 'College of Architecture and Planning and Institute of Cognitive Science, University of Colorado at Denver and Boulder, Boulder, CO#TAB#']","['2149245698', '2155651480']",2138482372.0,"{'offset': 0, 'data': [{'authorId': '1700028', 'url': 'https://www.semanticscholar.org/author/1700028', 'name': 'M. Gross', 'affiliations': [], 'homepage': None, 'paperCount': 206, 'citationCount': 4637}, {'authorId': '1689168', 'url': 'https://www.semanticscholar.org/author/1689168', 'name': 'E. Do', 'affiliations': [], 'homepage': None, 'paperCount': 171, 'citationCount': 3190}]}",324.0,"{'DBLP': 'conf/uist/GrossD96', 'MAG': '2138482372', 'DOI': '10.1145/237091.237119', 'CorpusId': 11351028}",['Computer Science'],14.0,False,{'pages': '183-192'},11/1/1996,['JournalArticle'],31.0,Ambiguous intentions: a paper-like interface for creative design,https://www.semanticscholar.org/paper/278605aab2c62b7a7c790bdb9b18907854829a10,UIST,1996
2140190783,"This paper explores the intersection of emerging surface technologies, capable of sensing multiple contacts and of-ten shape information, and advanced games physics engines. We define a technique for modeling the data sensed from such surfaces as input within a physics simulation. This affords the user the ability to interact with digital objects in ways analogous to manipulation of real objects. Our technique is capable of modeling both multiple contact points and more sophisticated shape information, such as the entire hand or other physical objects, and of mapping this user input to contact forces due to friction and collisions within the physics simulation. This enables a variety of fine-grained and casual interactions, supporting finger-based, whole-hand, and tangible input. We demonstrate how our technique can be used to add real-world dynamics to interactive surfaces such as a vision-based tabletop, creating a fluid and natural experience. Our approach hides from application developers many of the complexities inherent in using physics engines, allowing the creation of applications without preprogrammed interaction behavior or gesture recognition.",1.0,"This paper explores the intersection of emerging surface technologies, capable of sensing multiple contacts and of-ten shape information, and advanced games physics engines. We define a technique for modeling the data sensed from such surfaces as input within a physics simulation. This affords the user the ability to interact with digital objects in ways analogous to manipulation of real objects. Our technique is capable of modeling both multiple contact points and more sophisticated shape information, such as the entire hand or other physical objects, and of mapping this user input to contact forces due to friction and collisions within the physics simulation. This enables a variety of fine-grained and casual interactions, supporting finger-based, whole-hand, and tangible input. We demonstrate how our technique can be used to add real-world dynamics to interactive surfaces such as a vision-based tabletop, creating a fluid and natural experience. Our approach hides from application developers many of the complexities inherent in using physics engines, allowing the creation of applications without preprogrammed interaction behavior or gesture recognition.","['Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research, redmond, WA, USA#TAB#', 'Microsoft Research, Cambridge, Cambridge, United Kingdom', 'Microsoft Research, Cambridge, Cambridge, United Kingdom']","['2075214526', '2098553916', '2105571773', '2132704291', '2617703163']",2140190783.0,"{'offset': 0, 'data': [{'authorId': '145771244', 'url': 'https://www.semanticscholar.org/author/145771244', 'name': 'Andrew D. Wilson', 'affiliations': [], 'homepage': None, 'paperCount': 123, 'citationCount': 12219}, {'authorId': '79406746', 'url': 'https://www.semanticscholar.org/author/79406746', 'name': 'S. Izadi', 'affiliations': [], 'homepage': None, 'paperCount': 212, 'citationCount': 20595}, {'authorId': '2531379', 'url': 'https://www.semanticscholar.org/author/2531379', 'name': 'Otmar Hilliges', 'affiliations': [], 'homepage': 'https://ait.ethz.ch/people/hilliges', 'paperCount': 148, 'citationCount': 11219}, {'authorId': '1405813730', 'url': 'https://www.semanticscholar.org/author/1405813730', 'name': 'A. Garcia-Mendoza', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 510}, {'authorId': '1698434', 'url': 'https://www.semanticscholar.org/author/1698434', 'name': 'David S. Kirk', 'affiliations': [], 'homepage': None, 'paperCount': 42, 'citationCount': 2815}]}",234.0,"{'DBLP': 'conf/uist/WilsonIHGK08', 'MAG': '2140190783', 'DOI': '10.1145/1449715.1449728', 'CorpusId': 1130666}",['Computer Science'],16.0,False,{'pages': '67-76'},10/19/2008,['JournalArticle'],28.0,Bringing physics to the surface,https://www.semanticscholar.org/paper/0564c581517d6ca954635451d8d87150ba889242,UIST,2008
2140280838,"ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.",1.0,"ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.","['Microsoft Research Limited, Cambridge, United Kngdm', 'Microsoft Research Limited, Cambridge, United Kngdm', 'Microsoft Research Limited, Cambridge, United Kngdm', 'Microsoft Research Limited, Cambridge, United Kngdm', 'Microsoft Research Limited, Redmond']","['1255462760', '2098553916', '2148014207', '2168200088', '2893200430']",2140280838.0,"{'offset': 0, 'data': [{'authorId': '144356949', 'url': 'https://www.semanticscholar.org/author/144356949', 'name': 'Steve Hodges', 'affiliations': [], 'homepage': None, 'paperCount': 139, 'citationCount': 12297}, {'authorId': '79406746', 'url': 'https://www.semanticscholar.org/author/79406746', 'name': 'S. Izadi', 'affiliations': [], 'homepage': None, 'paperCount': 212, 'citationCount': 20595}, {'authorId': '120225597', 'url': 'https://www.semanticscholar.org/author/120225597', 'name': 'Alex Butler', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 2353}, {'authorId': '2630153', 'url': 'https://www.semanticscholar.org/author/2630153', 'name': 'Alban Rrustemi', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 240}, {'authorId': '145884449', 'url': 'https://www.semanticscholar.org/author/145884449', 'name': 'W. Buxton', 'affiliations': [], 'homepage': None, 'paperCount': 193, 'citationCount': 20477}]}",159.0,"{'MAG': '2140280838', 'DBLP': 'conf/uist/HodgesIBRB07', 'DOI': '10.1145/1294211.1294258', 'CorpusId': 8827622}",['Computer Science'],7.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference']",29.0,ThinSight: versatile multi-touch sensing for thin form-factor displays,https://www.semanticscholar.org/paper/038580ef558ae5f51071bad1cecb67e6f0e197ef,UIST,2007
2143536389,"The paper describes the VB2 architecture for the construction of three-dimensional interactive applications. The system's state and behavior are uniformly represented as a network of interrelated objects. Dynamic components are modeled by active variables, while multi-way relations are modeled by hierarchical constraints. Daemons are used to sequence between system states in reaction to changes in variable values. The constraint network is efficiently maintained by an incremental constraint solver based on an enhancement of SkyBlue. Multiple devices are used to interact with the synthetic world through the use of various interaction paradigms, including immersive environments with visual and audio feedback. Interaction techniques range from direct manipulation, to gestural input and three-dimensional virtual tools. Adaptive pattern recognition is used to increase input device expressiveness by enhancing sensor data with classification information. Virtual tools, which are encapsulations of visual appearance and behavior, present a selective view of manipulated models' information and offer an interaction metaphor to control it. Since virtual tools are first class objects, they can be assembled into more complex tools, much in the same way that simple tools are built on top of a modeling hierarchy. The architecture is currently being used to build a virtual reality animation system",1.0,"The paper describes the VB2 architecture for the construction of three-dimensional interactive applications. The system's state and behavior are uniformly represented as a network of interrelated objects. Dynamic components are modeled by active variables, while multi-way relations are modeled by hierarchical constraints. Daemons are used to sequence between system states in reaction to changes in variable values. The constraint network is efficiently maintained by an incremental constraint solver based on an enhancement of SkyBlue. Multiple devices are used to interact with the synthetic world through the use of various interaction paradigms, including immersive environments with visual and audio feedback. Interaction techniques range from direct manipulation, to gestural input and three-dimensional virtual tools. Adaptive pattern recognition is used to increase input device expressiveness by enhancing sensor data with classification information. Virtual tools, which are encapsulations of visual appearance and behavior, present a selective view of manipulated models' information and offer an interaction metaphor to control it. Since virtual tools are first class objects, they can be assembled into more complex tools, much in the same way that simple tools are built on top of a modeling hierarchy. The architecture is currently being used to build a virtual reality animation system","['Computer Graphics Laboratory, Swiss Federal Institute of Technology, CH-1015 Lausanne, Switzerland', 'Computer Graphics Laboratory, Swiss Federal Institute of Technology, CH-1015 Lausanne, Switzerland', 'Computer Graphics Laboratory, Swiss Federal Institute of Technology, CH-1015 Lausanne, Switzerland']","['2046772540', '2118134862', '2121740505']",2143536389.0,"{'offset': 0, 'data': [{'authorId': '1708999', 'url': 'https://www.semanticscholar.org/author/1708999', 'name': 'E. Gobbetti', 'affiliations': [], 'homepage': None, 'paperCount': 271, 'citationCount': 5582}, {'authorId': '35199063', 'url': 'https://www.semanticscholar.org/author/35199063', 'name': 'Jean-Francis Balaguer', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 221}, {'authorId': '143725529', 'url': 'https://www.semanticscholar.org/author/143725529', 'name': 'D. Thalmann', 'affiliations': [], 'homepage': None, 'paperCount': 1013, 'citationCount': 23379}]}",50.0,"{'MAG': '2143536389', 'DBLP': 'conf/uist/GobbettiBT93', 'DOI': '10.1145/168642.168659', 'CorpusId': 15923487}",['Computer Science'],1.0,False,{'pages': '167-178'},12/1/1993,['JournalArticle'],59.0,VB2: an architecture for interaction in synthetic worlds,https://www.semanticscholar.org/paper/0a444bbd998deb8b413a7eec834e67798c769c78,UIST,1993
2143602702,"This paper describes Collabode, a web-based Java integrated development environment designed to support close, synchronous collaboration between programmers. We examine the problem of collaborative coding in the face of program compilation errors introduced by other users which make collaboration more difficult, and describe an algorithm for error-mediated integration of program code. Concurrent editors see the text of changes made by collaborators, but the errors reported in their view are based only on their own changes. Editors may run the program at any time, using only error-free edits supplied so far, and ignoring incomplete or otherwise error-generating changes. We evaluate this algorithm and interface on recorded data from previous pilot experiments with Collabode, and via a user study with student and professional programmers. We conclude that it offers appreciable benefits over naive continuous synchronization without regard to errors and over manual version control.",1.0,"This paper describes Collabode, a web-based Java integrated development environment designed to support close, synchronous collaboration between programmers. We examine the problem of collaborative coding in the face of program compilation errors introduced by other users which make collaboration more difficult, and describe an algorithm for error-mediated integration of program code. Concurrent editors see the text of changes made by collaborators, but the errors reported in their view are based only on their own changes. Editors may run the program at any time, using only error-free edits supplied so far, and ignoring incomplete or otherwise error-generating changes. We evaluate this algorithm and interface on recorded data from previous pilot experiments with Collabode, and via a user study with student and professional programmers. We conclude that it offers appreciable benefits over naive continuous synchronization without regard to errors and over manual version control.","['Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA', 'Massachusetts Institute of Technology Cambridge, MA USA']","['2104582966', '2106696903', '2156768364']",2143602702.0,"{'offset': 0, 'data': [{'authorId': '34874616', 'url': 'https://www.semanticscholar.org/author/34874616', 'name': 'Max Goldman', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 968}, {'authorId': '48155668', 'url': 'https://www.semanticscholar.org/author/48155668', 'name': 'Greg Little', 'affiliations': [], 'homepage': None, 'paperCount': 37, 'citationCount': 3712}, {'authorId': '152160465', 'url': 'https://www.semanticscholar.org/author/152160465', 'name': 'Rob Miller', 'affiliations': [], 'homepage': None, 'paperCount': 120, 'citationCount': 9034}]}",108.0,"{'DBLP': 'conf/uist/GoldmanLM11', 'MAG': '2143602702', 'DOI': '10.1145/2047196.2047215', 'CorpusId': 11982148}",['Computer Science'],4.0,True,{'name': 'Proceedings of the 24th annual ACM symposium on User interface software and technology'},10/16/2011,"['Book', 'JournalArticle', 'Conference']",36.0,Real-time collaborative coding in a web IDE,https://www.semanticscholar.org/paper/1d164d02580d5f4f29b0cda07919e238c68bd08b,UIST,2011
2145491077,"Interactive surfaces have great potential for co-located collaboration because of their ability to track multiple inputs simultaneously. However, the multi-user experience on these devices could be enriched significantly if touch points could be associated with a particular user. Existing approaches to user identification are intrusive, require users to stay in a fixed position, or suffer from poor accuracy. We present a non-intrusive, high-accuracy technique for mapping touches to their corresponding user in a collaborative environment. By mounting a high-resolution camera above the interactive surface, we are able to identify touches reliably without any extra instrumentation, and users are able to move around the surface at will. Our technique, which leverages the back of users' hands as identifiers, supports walk-up-and-use situations in which multiple people interact on a shared surface.",0.0,"Interactive surfaces have great potential for co-located collaboration because of their ability to track multiple inputs simultaneously. However, the multi-user experience on these devices could be enriched significantly if touch points could be associated with a particular user. Existing approaches to user identification are intrusive, require users to stay in a fixed position, or suffer from poor accuracy. We present a non-intrusive, high-accuracy technique for mapping touches to their corresponding user in a collaborative environment. By mounting a high-resolution camera above the interactive surface, we are able to identify touches reliably without any extra instrumentation, and users are able to move around the surface at will. Our technique, which leverages the back of users' hands as identifiers, supports walk-up-and-use situations in which multiple people interact on a shared surface.","['Hasselt University, Diepenbeek, Belgium#TAB#', 'Hasselt University, Diepenbeek, Belgium#TAB#', 'Hasselt University, Diepenbeek, Belgium#TAB#', 'German Research Center for Artificial Intelligence DFKI, Saarbrücken, Germany#TAB#', 'Hasselt University, Diepenbeek, Belgium#TAB#']","['1900644889', '2014874916', '2161998151', '2290731219', '69698256']",2145491077.0,"{'offset': 0, 'data': [{'authorId': '1950213', 'url': 'https://www.semanticscholar.org/author/1950213', 'name': 'Raf Ramakers', 'affiliations': [], 'homepage': None, 'paperCount': 30, 'citationCount': 469}, {'authorId': '3202396', 'url': 'https://www.semanticscholar.org/author/3202396', 'name': 'D. Vanacken', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 319}, {'authorId': '1681624', 'url': 'https://www.semanticscholar.org/author/1681624', 'name': 'K. Luyten', 'affiliations': [], 'homepage': None, 'paperCount': 271, 'citationCount': 3363}, {'authorId': '1695519', 'url': 'https://www.semanticscholar.org/author/1695519', 'name': 'K. Coninx', 'affiliations': [], 'homepage': None, 'paperCount': 379, 'citationCount': 4889}, {'authorId': '2070910', 'url': 'https://www.semanticscholar.org/author/2070910', 'name': 'Johannes Schöning', 'affiliations': [], 'homepage': None, 'paperCount': 201, 'citationCount': 4135}]}",44.0,"{'MAG': '2145491077', 'DBLP': 'conf/uist/RamakersVLCS12', 'DOI': '10.1145/2380116.2380123', 'CorpusId': 5790428}",['Computer Science'],4.0,False,{'name': 'Proceedings of the 25th annual ACM symposium on User interface software and technology'},10/7/2012,"['JournalArticle', 'Book', 'Conference']",38.0,Carpus: a non-intrusive user identification technique for interactive surfaces,https://www.semanticscholar.org/paper/d29f2774e22c23ad1f6bad464f335ad3b3ec8f7c,UIST,2012
2152252442,"When users handle large amounts of data, errors are hard to notice. Outlier finding is a new way to reduce errors by directing the user's attention to inconsistent data which may indicate errors. We have implemented an outlier finder for text, which can detect both unusual matches and unusual mismatches to a text pattern. When integrated into the user interface of a PBD text editor and tested in a user study, outlier finding substantially reduced errors.",1.0,"When users handle large amounts of data, errors are hard to notice. Outlier finding is a new way to reduce errors by directing the user's attention to inconsistent data which may indicate errors. We have implemented an outlier finder for text, which can detect both unusual matches and unusual mismatches to a text pattern. When integrated into the user interface of a PBD text editor and tested in a user study, outlier finding substantially reduced errors.","['Carnegie Mellon University, Pittsburgh, PA.', 'Carnegie Mellon University, Pittsburgh, PA.']","['2104582966', '2117127927']",2152252442.0,"{'offset': 0, 'data': [{'authorId': '1930409460', 'url': 'https://www.semanticscholar.org/author/1930409460', 'name': 'Robert C. Miller', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 1373}, {'authorId': '1707801', 'url': 'https://www.semanticscholar.org/author/1707801', 'name': 'B. Myers', 'affiliations': [], 'homepage': None, 'paperCount': 498, 'citationCount': 26319}]}",65.0,"{'MAG': '2152252442', 'DBLP': 'conf/uist/MillerM01', 'DOI': '10.1145/502348.502361', 'CorpusId': 8231404}",['Computer Science'],1.0,False,{'pages': '81-90'},11/11/2001,['JournalArticle'],28.0,Outlier finding: focusing user attention on possible errors,https://www.semanticscholar.org/paper/7559ab6bc70f76692b96803b6a67b509644cdb31,UIST,2001
2152461980,"Audio producers often use musical underlays to emphasize key moments in spoken content and give listeners time to reflect on what was said. Yet, creating such underlays is time-consuming as producers must carefully (1) mark an emphasis point in the speech (2) select music with the appropriate style, (3) align the music with the emphasis point, and (4) adjust dynamics to produce a harmonious composition. We present UnderScore, a set of semi-automated tools designed to facilitate the creation of such underlays. The producer simply marks an emphasis point in the speech and selects a music track. UnderScore automatically refines, aligns and adjusts the speech and music to generate a high-quality underlay. UnderScore allows producers to focus on the high-level design of the underlay; they can quickly try out a variety of music and test different points of emphasis in the story. Amateur producers, who may lack the time or skills necessary to author underlays, can quickly add music to their stories. An informal evaluation of UnderScore suggests that it can produce high-quality underlays for a variety of examples while significantly reducing the time and effort required of radio producers.",1.0,"Audio producers often use musical underlays to emphasize key moments in spoken content and give listeners time to reflect on what was said. Yet, creating such underlays is time-consuming as producers must carefully (1) mark an emphasis point in the speech (2) select music with the appropriate style, (3) align the music with the emphasis point, and (4) adjust dynamics to produce a harmonious composition. We present UnderScore, a set of semi-automated tools designed to facilitate the creation of such underlays. The producer simply marks an emphasis point in the speech and selects a music track. UnderScore automatically refines, aligns and adjusts the speech and music to generate a high-quality underlay. UnderScore allows producers to focus on the high-level design of the underlay; they can quickly try out a variety of music and test different points of emphasis in the story. Amateur producers, who may lack the time or skills necessary to author underlays, can quickly add music to their stories. An informal evaluation of UnderScore suggests that it can produce high-quality underlays for a variety of examples while significantly reducing the time and effort required of radio producers.","['University of California Berkeley, Berkeley, California, USA; ', 'Adobe Systems, San Francisco, California, USA#TAB#', 'University of California Berkeley, Berkeley, California, USA; ', 'Adobe Systems, San Francisco, California, USA#TAB#', 'University of California Berkeley, Berkeley, California, USA; ']","['2016204054', '2144483826', '2147979816', '2303505900', '718039462']",2152461980.0,"{'offset': 0, 'data': [{'authorId': '36400990', 'url': 'https://www.semanticscholar.org/author/36400990', 'name': 'Steve Rubin', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 130}, {'authorId': '2842099', 'url': 'https://www.semanticscholar.org/author/2842099', 'name': 'Floraine Berthouzoz', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 788}, {'authorId': '1781063', 'url': 'https://www.semanticscholar.org/author/1781063', 'name': 'G. Mysore', 'affiliations': [], 'homepage': None, 'paperCount': 75, 'citationCount': 2246}, {'authorId': '2108638905', 'url': 'https://www.semanticscholar.org/author/2108638905', 'name': 'Wilmot Li', 'affiliations': [], 'homepage': None, 'paperCount': 86, 'citationCount': 3275}, {'authorId': '1820412', 'url': 'https://www.semanticscholar.org/author/1820412', 'name': 'Maneesh Agrawala', 'affiliations': [], 'homepage': None, 'paperCount': 230, 'citationCount': 13867}]}",14.0,"{'DBLP': 'conf/uist/RubinBMLA12', 'MAG': '2152461980', 'DOI': '10.1145/2380116.2380163', 'CorpusId': 8428363}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 25th annual ACM symposium on User interface software and technology'},10/7/2012,"['Book', 'JournalArticle', 'Conference']",47.0,UnderScore: musical underlays for audio stories,https://www.semanticscholar.org/paper/f48da2136aac3e0b6dce557aac06013163bde59d,UIST,2012
2153052098,"Location-enhanced applications use the location of people, places, and things to augment or streamline interaction. Location-enhanced applications are just starting to emerge in several different domains, and many people believe that this type of application will experience tremendous growth in the near future. However, it currently requires a high level of technical expertise to build location-enhanced applications, making it hard to iterate on designs. To address this problem we introduce Topiary, a tool for rapidly prototyping location-enhanced applications. Topiary lets designers create a map that models the location of people, places, and things; use this active map to demonstrate scenarios depicting location contexts; use these scenarios in creating storyboards that describe interaction sequences; and then run these storyboards on mobile devices, with a wizard updating the location of people and things on a separate device. We performed an informal evaluation with seven researchers and interface designers and found that they reacted positively to the concept.",1.0,"Location-enhanced applications use the location of people, places, and things to augment or streamline interaction. Location-enhanced applications are just starting to emerge in several different domains, and many people believe that this type of application will experience tremendous growth in the near future. However, it currently requires a high level of technical expertise to build location-enhanced applications, making it hard to iterate on designs. To address this problem we introduce Topiary, a tool for rapidly prototyping location-enhanced applications. Topiary lets designers create a map that models the location of people, places, and things; use this active map to demonstrate scenarios depicting location contexts; use these scenarios in creating storyboards that describe interaction sequences; and then run these storyboards on mobile devices, with a wizard updating the location of people and things on a separate device. We performed an informal evaluation with seven researchers and interface designers and found that they reacted positively to the concept.","['University of California-Berkeley, Berkeley, CA', 'University of California-Berkeley, Berkeley, CA', 'Intel Research Seattle, WA and University of Washington, Seattle, WA']","['2141131632', '2413905935', '29409133']",2153052098.0,"{'offset': 0, 'data': [{'authorId': '1678662', 'url': 'https://www.semanticscholar.org/author/1678662', 'name': 'Yang Li', 'affiliations': ['Google Research'], 'homepage': 'http://yangl.org', 'paperCount': 45, 'citationCount': 3481}, {'authorId': '2110688724', 'url': 'https://www.semanticscholar.org/author/2110688724', 'name': 'Jason I. Hong', 'affiliations': [], 'homepage': None, 'paperCount': 293, 'citationCount': 20590}, {'authorId': '9522307', 'url': 'https://www.semanticscholar.org/author/9522307', 'name': 'J. Landay', 'affiliations': [], 'homepage': None, 'paperCount': 285, 'citationCount': 22329}]}",181.0,"{'DBLP': 'conf/uist/LiHL04', 'MAG': '2153052098', 'DOI': '10.1145/1029632.1029671', 'CorpusId': 3968249}",['Computer Science'],16.0,False,{'pages': '217-226'},10/24/2004,['JournalArticle'],47.0,Topiary: a tool for prototyping location-enhanced applications,https://www.semanticscholar.org/paper/e3cc53d6bf658f022b7fe7162abd7f1dbda6e2a6,UIST,2004
2156611338,"Progress bars are prevalent in modern user interfaces. Typically, a linear function is employed such that the progress of the bar is directly proportional to how much work has been completed. However, numerous factors cause progress bars to proceed at non-linear rates. Additionally, humans perceive time in a non-linear way. This paper explores the impact of various progress bar behaviors on user perception of process duration. The results are used to suggest several design considerations that can make progress bars appear faster and ultimately improve users' computing experience.",1.0,"Progress bars are prevalent in modern user interfaces. Typically, a linear function is employed such that the progress of the bar is directly proportional to how much work has been completed. However, numerous factors cause progress bars to proceed at non-linear rates. Additionally, humans perceive time in a non-linear way. This paper explores the impact of various progress bar behaviors on user perception of process duration. The results are used to suggest several design considerations that can make progress bars appear faster and ultimately improve users' computing experience.","['AT&T Labs-Research, Florham Park NJ#TAB#', 'New York University. New York, NY', 'Carnegie Mellon University and AT&T Labs-Research, Pittsburgh, PA', 'AT&T Labs-Research, Florham Park NJ#TAB#']","['2085239589', '2104353407', '2123491528', '3157188477']",2156611338.0,"{'offset': 0, 'data': [{'authorId': '145078227', 'url': 'https://www.semanticscholar.org/author/145078227', 'name': 'Chris Harrison', 'affiliations': [], 'homepage': None, 'paperCount': 154, 'citationCount': 8137}, {'authorId': '1730851', 'url': 'https://www.semanticscholar.org/author/1730851', 'name': 'B. Amento', 'affiliations': [], 'homepage': None, 'paperCount': 33, 'citationCount': 1781}, {'authorId': '144228596', 'url': 'https://www.semanticscholar.org/author/144228596', 'name': 'S. Kuznetsov', 'affiliations': [], 'homepage': None, 'paperCount': 49, 'citationCount': 1516}, {'authorId': '2113591957', 'url': 'https://www.semanticscholar.org/author/2113591957', 'name': 'Robert M. Bell', 'affiliations': [], 'homepage': None, 'paperCount': 38, 'citationCount': 11623}]}",86.0,"{'DBLP': 'conf/uist/HarrisonAKB07', 'MAG': '2156611338', 'DOI': '10.1145/1294211.1294231', 'CorpusId': 17549653}",['Computer Science'],9.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference']",15.0,Rethinking the progress bar,https://www.semanticscholar.org/paper/8dff60b3a929b85d096ba10008c29f34b273f07c,UIST,2007
2168468508,"Spoken language interfaces provide highly mobile, small form-factor, hands-free, eyes-free interaction with information. Uniform access to large lists of information using spoken interfaces is highly desirable, but problematic due to inherent limitations of speech. A speech widget for lists of attributed objects is described that provides for approximate queries to retrieve desired items. User tests demonstrate that this is an effective technique for accessing information using speech.",1.0,"Spoken language interfaces provide highly mobile, small form-factor, hands-free, eyes-free interaction with information. Uniform access to large lists of information using spoken interfaces is highly desirable, but problematic due to inherent limitations of speech. A speech widget for lists of attributed objects is described that provides for approximate queries to retrieve desired items. User tests demonstrate that this is an effective technique for accessing information using speech.","['Brigham Young University , Provo, UT', 'Brigham Young University , Provo, UT']","['2148302434', '2230714040']",2168468508.0,"{'offset': 0, 'data': [{'authorId': '1733794', 'url': 'https://www.semanticscholar.org/author/1733794', 'name': 'D. Olsen', 'affiliations': [], 'homepage': None, 'paperCount': 107, 'citationCount': 4379}, {'authorId': '2168728', 'url': 'https://www.semanticscholar.org/author/2168728', 'name': 'Jon R. Peachey', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 5}]}",5.0,"{'DBLP': 'conf/uist/OlsenP02', 'MAG': '2168468508', 'DOI': '10.1145/571985.572004', 'CorpusId': 15190388}",['Computer Science'],0.0,False,{'pages': '131-140'},10/27/2002,['JournalArticle'],13.0,Query-by-critique: spoken language access to large lists,https://www.semanticscholar.org/paper/be5ed377407a2a85f987cb95e77fd845eeb10736,UIST,2002
2173674698,"Modalities such as pen and touch are associated with direct input but can also be used for indirect input. We propose to combine the two modes for direct-indirect input modulated by gaze. We introduce gaze-shifting as a novel mechanism for switching the input mode based on the alignment of manual input and the user's visual attention. Input in the user's area of attention results in direct manipulation whereas input offset from the user's gaze is redirected to the visual target. The technique is generic and can be used in the same manner with different input modalities. We show how gaze-shifting enables novel direct-indirect techniques with pen, touch, and combinations of pen and touch input.",0.0,"Modalities such as pen and touch are associated with direct input but can also be used for indirect input. We propose to combine the two modes for direct-indirect input modulated by gaze. We introduce gaze-shifting as a novel mechanism for switching the input mode based on the alignment of manual input and the user's visual attention. Input in the user's area of attention results in direct manipulation whereas input offset from the user's gaze is redirected to the visual target. The technique is generic and can be used in the same manner with different input modalities. We show how gaze-shifting enables novel direct-indirect techniques with pen, touch, and combinations of pen and touch input.","['Lancaster Univ., Lancaster, United Kingdom', 'Lancaster Univ., Lancaster, United Kingdom', 'Lancaster Univ., Lancaster, United Kingdom', 'Lancaster Univ., Lancaster, United Kingdom', 'Lancaster Univ., Lancaster, United Kingdom']","['2011140363', '2075011842', '2111997323', '2115437627', '2159107908']",2173674698.0,"{'offset': 0, 'data': [{'authorId': '3171800', 'url': 'https://www.semanticscholar.org/author/3171800', 'name': 'Ken Pfeuffer', 'affiliations': [], 'homepage': None, 'paperCount': 48, 'citationCount': 825}, {'authorId': '143867540', 'url': 'https://www.semanticscholar.org/author/143867540', 'name': 'Jason Alexander', 'affiliations': [], 'homepage': None, 'paperCount': 113, 'citationCount': 3163}, {'authorId': '2577932', 'url': 'https://www.semanticscholar.org/author/2577932', 'name': 'M. K. Chong', 'affiliations': [], 'homepage': None, 'paperCount': 31, 'citationCount': 729}, {'authorId': '2108655980', 'url': 'https://www.semanticscholar.org/author/2108655980', 'name': 'Yanxia Zhang', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 418}, {'authorId': '4919595', 'url': 'https://www.semanticscholar.org/author/4919595', 'name': 'H. Gellersen', 'affiliations': [], 'homepage': None, 'paperCount': 317, 'citationCount': 13318}]}",58.0,"{'MAG': '2173674698', 'DBLP': 'conf/uist/PfeufferACZG15', 'DOI': '10.1145/2807442.2807460', 'CorpusId': 15305806}",['Computer Science'],3.0,True,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['Book', 'JournalArticle', 'Conference']",42.0,Gaze-Shifting: Direct-Indirect Input with Pen and Touch Modulated by Gaze,https://www.semanticscholar.org/paper/cd4e5083a9c79fc455d4dc2bd5f7d0c23dda31d1,UIST,2015
2180067629,"We propose a block-stacking system based on capacitance. The system, called Capacitive Blocks, allows users to build 3D models in a virtual space by stacking physical blocks. The construction of the block-stacking system is simple, and fundamental components including physical blocks can be made with a 3D printer. The block is a capacitor that consists of two layers made of conductive plastic filament and between them a layer made of non-conductive plastic filament. In this paper, we present a prototype of this block-stacking system and the mechanism that detects the height of blocks (i.e., the number of stacked blocks) by measuring the capacitance of the stacked blocks, which changes in accordance with the number of stacked blocks.",0.0,"We propose a block-stacking system based on capacitance. The system, called Capacitive Blocks, allows users to build 3D models in a virtual space by stacking physical blocks. The construction of the block-stacking system is simple, and fundamental components including physical blocks can be made with a 3D printer. The block is a capacitor that consists of two layers made of conductive plastic filament and between them a layer made of non-conductive plastic filament. In this paper, we present a prototype of this block-stacking system and the mechanism that detects the height of blocks (i.e., the number of stacked blocks) by measuring the capacitance of the stacked blocks, which changes in accordance with the number of stacked blocks.","['[University of Tsukuba, Tsukuba, Ibaraki, Japan]', '[University of Tsukuba, Tsukuba, Ibaraki, Japan]', '[University of Tsukuba, Tsukuba, Ibaraki, Japan]']","['2284288079', '2343421951', '271348325']",2180067629.0,"{'offset': 0, 'data': [{'authorId': '2015617', 'url': 'https://www.semanticscholar.org/author/2015617', 'name': 'A. Yoshida', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 7}, {'authorId': '1765222', 'url': 'https://www.semanticscholar.org/author/1765222', 'name': 'B. Shizuki', 'affiliations': [], 'homepage': None, 'paperCount': 168, 'citationCount': 798}, {'authorId': '144705533', 'url': 'https://www.semanticscholar.org/author/144705533', 'name': 'J. Tanaka', 'affiliations': [], 'homepage': None, 'paperCount': 297, 'citationCount': 1467}]}",3.0,"{'DBLP': 'conf/uist/YoshidaST15', 'MAG': '2180067629', 'DOI': '10.1145/2815585.2815731', 'CorpusId': 15386527}",['Computer Science'],0.0,False,{'name': 'Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/6/2015,"['Book', 'JournalArticle', 'Conference']",5.0,Capacitive Blocks: A Block System that Connects the Physical with the Virtual using Changes of Capacitance,https://www.semanticscholar.org/paper/ca888120394cfdd8d8d2e920595d31ea65bed2ac,UIST,2015
2236466394,"Functional near-infrared spectroscopy (fNIRS) holds increasing potential for Brain-Computer Interfaces (BCI) due to its portability, ease of application, robustness to movement artifacts, and relatively low cost. The use of fNIRS to support the development of affective BCI has received comparatively less attention, despite the role played by the prefrontal cortex in affective control, and the appropriateness of fNIRS to measure prefrontal activity. We present an active, fNIRS-based neurofeedback (NF) interface, which uses differential changes in oxygenation between the left and right sides of the dorsolateral prefrontal cortex to operationalize BCI input. The system is activated by users generating a state of anger, which has been previously linked to increased left prefrontal asymmetry. We have incorporated this NF interface into an experimental platform adapted from a virtual 3D narrative, in which users can express anger at a virtual character perceived as evil, causing the character to disappear progressively. Eleven subjects used the system and were able to successfully perform NF despite minimal training. Extensive analysis confirms that success was associated with the intent to express anger. This has positive implications for the design of affective BCI based on prefrontal asymmetry.",0.0,"Functional near-infrared spectroscopy (fNIRS) holds increasing potential for Brain-Computer Interfaces (BCI) due to its portability, ease of application, robustness to movement artifacts, and relatively low cost. The use of fNIRS to support the development of affective BCI has received comparatively less attention, despite the role played by the prefrontal cortex in affective control, and the appropriateness of fNIRS to measure prefrontal activity. We present an active, fNIRS-based neurofeedback (NF) interface, which uses differential changes in oxygenation between the left and right sides of the dorsolateral prefrontal cortex to operationalize BCI input. The system is activated by users generating a state of anger, which has been previously linked to increased left prefrontal asymmetry. We have incorporated this NF interface into an experimental platform adapted from a virtual 3D narrative, in which users can express anger at a virtual character perceived as evil, causing the character to disappear progressively. Eleven subjects used the system and were able to successfully perform NF despite minimal training. Extensive analysis confirms that success was associated with the intent to express anger. This has positive implications for the design of affective BCI based on prefrontal asymmetry.","['Teesside University, Middlesbrough, United Kingdom.', 'Teesside University, Middlesbrough, United Kingdom.', 'Teesside University, Middlesbrough, United Kingdom.']","['2138961986', '2212277184', '2246577087']",2236466394.0,"{'offset': 0, 'data': [{'authorId': '1786905', 'url': 'https://www.semanticscholar.org/author/1786905', 'name': 'Gabor Aranyi', 'affiliations': [], 'homepage': None, 'paperCount': 27, 'citationCount': 279}, {'authorId': '144553087', 'url': 'https://www.semanticscholar.org/author/144553087', 'name': 'Fred Charles', 'affiliations': ['Bournemouth University'], 'homepage': 'https://staffprofiles.bournemouth.ac.uk/display/fcharles', 'paperCount': 132, 'citationCount': 2658}, {'authorId': '1696638', 'url': 'https://www.semanticscholar.org/author/1696638', 'name': 'M. Cavazza', 'affiliations': [], 'homepage': None, 'paperCount': 298, 'citationCount': 4893}]}",17.0,"{'DBLP': 'conf/uist/AranyiCC15', 'MAG': '2236466394', 'DOI': '10.1145/2807442.2807447', 'CorpusId': 27775634}",['Computer Science'],1.0,True,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['JournalArticle', 'Book', 'Conference']",47.0,Anger-based BCI Using fNIRS Neurofeedback,https://www.semanticscholar.org/paper/adb787e1ee7d2de650b800e0b07582d4852c4208,UIST,2015
2246664447,"Procedural modeling systems allow users to create high quality content through parametric, conditional or stochastic rule sets. While such approaches create an abstraction layer by freeing the user from direct geometry editing, the nonlinear nature and the high number of parameters associated with such design spaces result in arduous modeling experiences for non-expert users. We propose a method to enable intuitive exploration of such high dimensional procedural modeling spaces within a lower dimensional space learned through autoencoder network training. Our method automatically generates a representative training dataset from the procedural modeling rule set based on shape similarity features. We then leverage the samples in this dataset to train an autoencoder neural network, while also structuring the learned lower dimensional space for continuous exploration with respect to shape features. We demonstrate the efficacy our method with user studies where designers create content with more than 10-fold faster speeds using our system compared to the classic procedural modeling interface.",1.0,"Procedural modeling systems allow users to create high quality content through parametric, conditional or stochastic rule sets. While such approaches create an abstraction layer by freeing the user from direct geometry editing, the nonlinear nature and the high number of parameters associated with such design spaces result in arduous modeling experiences for non-expert users. We propose a method to enable intuitive exploration of such high dimensional procedural modeling spaces within a lower dimensional space learned through autoencoder network training. Our method automatically generates a representative training dataset from the procedural modeling rule set based on shape similarity features. We then leverage the samples in this dataset to train an autoencoder neural network, while also structuring the learned lower dimensional space for continuous exploration with respect to shape features. We demonstrate the efficacy our method with user studies where designers create content with more than 10-fold faster speeds using our system compared to the classic procedural modeling interface.","['[Adobe Res., San Jose, CA, USA]', '[Adobe Res., San Jose, CA, USA]', '[Adobe Res., San Jose, CA, USA]', ' Carnegie Mellon University Pittsburgh PA USA']","['1508797771', '2064952311', '338272188', '662787973']",2246664447.0,"{'offset': 0, 'data': [{'authorId': '19435052', 'url': 'https://www.semanticscholar.org/author/19435052', 'name': 'M. E. Yümer', 'affiliations': [], 'homepage': None, 'paperCount': 17, 'citationCount': 495}, {'authorId': '2934421', 'url': 'https://www.semanticscholar.org/author/2934421', 'name': 'Paul Asente', 'affiliations': [], 'homepage': None, 'paperCount': 63, 'citationCount': 1202}, {'authorId': '41193203', 'url': 'https://www.semanticscholar.org/author/41193203', 'name': 'R. Mech', 'affiliations': [], 'homepage': None, 'paperCount': 86, 'citationCount': 5565}, {'authorId': '1808848', 'url': 'https://www.semanticscholar.org/author/1808848', 'name': 'L. Kara', 'affiliations': [], 'homepage': None, 'paperCount': 114, 'citationCount': 2192}]}",59.0,"{'DBLP': 'conf/uist/YumerAMK15', 'MAG': '2246664447', 'DOI': '10.1145/2807442.2807448', 'CorpusId': 18422848}",['Computer Science'],2.0,False,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['Book', 'JournalArticle', 'Conference']",62.0,Procedural Modeling Using Autoencoder Networks,https://www.semanticscholar.org/paper/63d8bf25c8909d5b718f0a6e9122ccf25512f6fa,UIST,2015
2280228649,"We present a two-handed indirect touch interaction technique for the extrusion of polygons within a 3D modeling tool that we have built for a horizontal/vertical dual touch screen setup. In particular, we introduce perspective-dependent touch gestures: using several graphical input areas on the horizontal display, the non-dominant hand navigates the virtual camera and thus continuously updates the spatial frame of reference within which the dominant hand performs extrusions with dragging gestures.",0.0,"We present a two-handed indirect touch interaction technique for the extrusion of polygons within a 3D modeling tool that we have built for a horizontal/vertical dual touch screen setup. In particular, we introduce perspective-dependent touch gestures: using several graphical input areas on the horizontal display, the non-dominant hand navigates the virtual camera and thus continuously updates the spatial frame of reference within which the dominant hand performs extrusions with dragging gestures.","['University of Munich (LMU), Munich, Bavaria, Germany', 'University of Munich (LMU), Munich, Bavaria, Germany', 'University of Munich LMU, Munich, Germany']","['1978169675', '2001761377', '2222136494']",2280228649.0,"{'offset': 0, 'data': [{'authorId': '3062440', 'url': 'https://www.semanticscholar.org/author/3062440', 'name': 'H. Palleis', 'affiliations': [], 'homepage': None, 'paperCount': 20, 'citationCount': 45}, {'authorId': '144906824', 'url': 'https://www.semanticscholar.org/author/144906824', 'name': 'Julie Wagner', 'affiliations': [], 'homepage': None, 'paperCount': 21, 'citationCount': 868}, {'authorId': '46751563', 'url': 'https://www.semanticscholar.org/author/46751563', 'name': 'H. Hussmann', 'affiliations': [], 'homepage': None, 'paperCount': 334, 'citationCount': 6078}]}",1.0,"{'DBLP': 'conf/uist/PalleisWH15', 'MAG': '2280228649', 'DOI': '10.1145/2815585.2815733', 'CorpusId': 226189}",['Computer Science'],0.0,False,{'name': 'Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/6/2015,"['Book', 'JournalArticle', 'Conference']",7.0,Perspective-dependent Indirect Touch Input for 3D Polygon Extrusion,https://www.semanticscholar.org/paper/4b6ad660cd85a32cbda33d0e2da67f5a62c00467,UIST,2015
2339750270,"Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.",0.0,"Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.","['Brigham Young University & Sandia National Laboratories, Provo, UT, USA', 'Brigham Young University Provo, UT, USA.', 'Brigham Young University Provo, UT, USA.', 'Brigham Young University; Provo USA', 'Brigham Young University Provo, UT, USA.']","['1140008411', '2176732160', '2212557901', '2486794528', '740363620']",2339750270.0,"{'offset': 0, 'data': [{'authorId': '2554572', 'url': 'https://www.semanticscholar.org/author/2554572', 'name': 'Scott Ruoti', 'affiliations': [], 'homepage': None, 'paperCount': 39, 'citationCount': 646}, {'authorId': '143830935', 'url': 'https://www.semanticscholar.org/author/143830935', 'name': 'J. Andersen', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 243}, {'authorId': '38716248', 'url': 'https://www.semanticscholar.org/author/38716248', 'name': 'Travis Hendershot', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 59}, {'authorId': '1963419', 'url': 'https://www.semanticscholar.org/author/1963419', 'name': 'D. Zappala', 'affiliations': [], 'homepage': None, 'paperCount': 105, 'citationCount': 2690}, {'authorId': '1774940', 'url': 'https://www.semanticscholar.org/author/1774940', 'name': 'K. Seamons', 'affiliations': [], 'homepage': None, 'paperCount': 108, 'citationCount': 4750}]}",33.0,"{'MAG': '2339750270', 'ArXiv': '1510.08435', 'DBLP': 'conf/uist/RuotiAHZS16', 'DOI': '10.1145/2984511.2984580', 'CorpusId': 11533453}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/28/2015,"['Book', 'JournalArticle', 'Conference']",31.0,Private Webmail 2.0: Simple and Easy-to-Use Secure Email,https://www.semanticscholar.org/paper/6f612f508d4784339735e524fca3a56fb138b3d7,UIST,2015
2532407274,"To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users' unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline-before the system sees any user-defined gestures and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone's built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems.",0.0,"To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users' unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline-before the system sees any user-defined gestures and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone's built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems.","['Carnegie Mellon University & Google Research, Pittsburgh, PA, USA#TAB#', '[Google Research, Mountain View, CA, USA]']","['2108826813', '2413905935']",2532407274.0,"{'offset': 0, 'data': [{'authorId': '2028468', 'url': 'https://www.semanticscholar.org/author/2028468', 'name': ""Xiang 'Anthony' Chen"", 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 1594}, {'authorId': '98177814', 'url': 'https://www.semanticscholar.org/author/98177814', 'name': 'Y. Li', 'affiliations': [], 'homepage': None, 'paperCount': 803, 'citationCount': 7824}]}",12.0,"{'DBLP': 'conf/uist/ChenL16', 'MAG': '2532407274', 'DOI': '10.1145/2984511.2984541', 'CorpusId': 16171114}",['Computer Science'],0.0,True,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['Book', 'JournalArticle', 'Conference']",16.0,Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation,https://www.semanticscholar.org/paper/f1de3403a60386d4aa75709dd6639c26fc545364,UIST,2016
2532776193,"Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time. In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.",0.0,"Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time. In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.","['Tokyo Institute of Technology, Tokyo, (Japan)', 'Tokyo Institute of Technology, Tokyo, (Japan)', 'Tokyo Institute of Technology, Tokyo, (Japan)', 'Tokyo Institute of Technology, Tokyo, (Japan)']","['2134774260', '2225851674', '2342921829', '2537053113']",2532776193.0,"{'offset': 0, 'data': [{'authorId': '1390751870', 'url': 'https://www.semanticscholar.org/author/1390751870', 'name': 'Zhengqing Li', 'affiliations': [], 'homepage': None, 'paperCount': 17, 'citationCount': 97}, {'authorId': '2784800', 'url': 'https://www.semanticscholar.org/author/2784800', 'name': 'Shio Miyafuji', 'affiliations': [], 'homepage': None, 'paperCount': 29, 'citationCount': 114}, {'authorId': '3269647', 'url': 'https://www.semanticscholar.org/author/3269647', 'name': 'Toshiki Sato', 'affiliations': [], 'homepage': None, 'paperCount': 58, 'citationCount': 336}, {'authorId': '1684942', 'url': 'https://www.semanticscholar.org/author/1684942', 'name': 'H. Koike', 'affiliations': [], 'homepage': None, 'paperCount': 237, 'citationCount': 3506}]}",14.0,"{'DBLP': 'conf/uist/LiMSK16', 'MAG': '2532776193', 'DOI': '10.1145/2984751.2984765', 'CorpusId': 14257766}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",2.0,OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping,https://www.semanticscholar.org/paper/cf6d150b187770940cc7740194624bd5ba8a4688,UIST,2016
2765135114,"Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.",0.0,"Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.","['University of Illinois at Urbana Champaign & Adobe Research, Champaign, IL, USA', '[Adobe Research, San Francisco, CA, USA]', 'University of Illinois at Urbana Champaign , Champaign , IL , USA ']","['1984479034', '2144483826', '2659566598']",2765135114.0,"{'offset': 0, 'data': [{'authorId': '37445038', 'url': 'https://www.semanticscholar.org/author/37445038', 'name': 'Shrikant Venkataramani', 'affiliations': [], 'homepage': None, 'paperCount': 19, 'citationCount': 283}, {'authorId': '1718742', 'url': 'https://www.semanticscholar.org/author/1718742', 'name': 'P. Smaragdis', 'affiliations': [], 'homepage': None, 'paperCount': 194, 'citationCount': 8883}, {'authorId': '1781063', 'url': 'https://www.semanticscholar.org/author/1781063', 'name': 'G. Mysore', 'affiliations': [], 'homepage': None, 'paperCount': 75, 'citationCount': 2246}]}",0.0,"{'DBLP': 'conf/uist/VenkataramaniSM17', 'MAG': '2765135114', 'DOI': '10.1145/3126594.3126661', 'CorpusId': 5989843}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",11.0,AutoDub: Automatic Redubbing for Voiceover Editing,https://www.semanticscholar.org/paper/dbf22e064d64ff0e9b99190da254d7d31445385a,UIST,2017
2765470813,"Conquer it! is a lightweight proof-of-concept exertion game that demonstrates Body Channel Communication (BCC) in a smart environment. BCC employs the human body as communication medium to transfer digital data between physical objects by using electric fields that are coupled to the body. During the game participants are provided with BCC wearables, each of which represents a specific RGB color. When the user stands, walks on, or touches with a hand the BCC tiles, communication is automatically established: the corresponding sensor area decodes the message (RGB value) originating from the wearable and lights up according to that color for two seconds. The goal of the game is to try to light up as many tile cells simultaneously as possible. Participants can try to keep alive the colors by continuously moving around on the tiles. In the multiuser version, by stepping on or touching a blinking cell, users can immediately claim the area and overwrite the color of that subtile.",0.0,"Conquer it! is a lightweight proof-of-concept exertion game that demonstrates Body Channel Communication (BCC) in a smart environment. BCC employs the human body as communication medium to transfer digital data between physical objects by using electric fields that are coupled to the body. During the game participants are provided with BCC wearables, each of which represents a specific RGB color. When the user stands, walks on, or touches with a hand the BCC tiles, communication is automatically established: the corresponding sensor area decodes the message (RGB value) originating from the wearable and lights up according to that color for two seconds. The goal of the game is to try to light up as many tile cells simultaneously as possible. Participants can try to keep alive the colors by continuously moving around on the tiles. In the multiuser version, by stepping on or touching a blinking cell, users can immediately claim the area and overwrite the color of that subtile.","['Disney Research, Pittsburgh, PA, USA', 'ETH Zurich Zurich Switzerland', 'ETH Zurich and Disney Research Zurich, Switzerland', 'ETH Zurich and Disney Research Zurich, Switzerland']","['2040045115', '2112219004', '2226604045', '281591809']",2765470813.0,"{'offset': 0, 'data': [{'authorId': '20044029', 'url': 'https://www.semanticscholar.org/author/20044029', 'name': 'Virag Varga', 'affiliations': [], 'homepage': None, 'paperCount': 9, 'citationCount': 113}, {'authorId': '2550862', 'url': 'https://www.semanticscholar.org/author/2550862', 'name': 'G. Vakulya', 'affiliations': [], 'homepage': None, 'paperCount': 25, 'citationCount': 152}, {'authorId': '1749219', 'url': 'https://www.semanticscholar.org/author/1749219', 'name': 'A. Sample', 'affiliations': [], 'homepage': None, 'paperCount': 85, 'citationCount': 5512}, {'authorId': '49773674', 'url': 'https://www.semanticscholar.org/author/49773674', 'name': 'T. Gross', 'affiliations': [], 'homepage': None, 'paperCount': 297, 'citationCount': 10582}]}",4.0,"{'DBLP': 'conf/uist/VargaVSG17', 'MAG': '2765470813', 'DOI': '10.1145/3131785.3131798', 'CorpusId': 5011037}",['Computer Science'],0.0,False,{'name': 'Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",3.0,Playful Interactions with Body Channel Communication: Conquer it!,https://www.semanticscholar.org/paper/c6e85d3b212eddd078718717651566206ea263e1,UIST,2017
2765526880,"We propose a new type of printing system that incorporates sensors in a handheld printer to reflect in real time user intent in the results of printing on paper. This system achieves two key functions: ""real-time embellishment"" for altering printed content by reading user hand movements by pressure and optical sensors, and ""local transcription"" for selecting content to be output by tracing existing content on paper with a linear camera. We performed experiments to measure the accuracy of both techniques and evaluate their usefulness.",0.0,"We propose a new type of printing system that incorporates sensors in a handheld printer to reflect in real time user intent in the results of printing on paper. This system achieves two key functions: ""real-time embellishment"" for altering printed content by reading user hand movements by pressure and optical sensors, and ""local transcription"" for selecting content to be output by tracing existing content on paper with a linear camera. We performed experiments to measure the accuracy of both techniques and evaluate their usefulness.","['Waseda University, Shinjuku-ku, Tokyo, JAPAN', 'Waseda University, Shinjuku-ku, Tokyo, JAPAN']","['1837771262', '2767102006']",2765526880.0,"{'offset': 0, 'data': [{'authorId': '2076267586', 'url': 'https://www.semanticscholar.org/author/2076267586', 'name': 'Yuya Kitazawa', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 9}, {'authorId': '36717343', 'url': 'https://www.semanticscholar.org/author/36717343', 'name': 'Tomoko Hashida', 'affiliations': [], 'homepage': None, 'paperCount': 39, 'citationCount': 154}]}",0.0,"{'MAG': '2765526880', 'DBLP': 'conf/uist/KitazawaH17', 'DOI': '10.1145/3131785.3131790', 'CorpusId': 24327622}",['Computer Science'],0.0,False,{'name': 'Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['JournalArticle', 'Book', 'Conference']",6.0,Printing System Reflecting User's Intent in Real Time Using a Handheld Printer,https://www.semanticscholar.org/paper/572c981086d095ba0cf8973bf4636119fef915d0,UIST,2017
2765887622,"When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.",1.0,"When bringing animated characters to life, artists often augment the primary motion of a figure by adding secondary animation -- subtle movement of parts like hair, foliage or cloth that complements and emphasizes the primary motion. Traditionally, artists add secondary motion to animated illustrations only through arduous manual effort, and often eschew it entirely. Emerging ``live' performance applications allow both novices and experts to perform the primary motion of a character, but only a virtuoso performer could manage the degrees of freedom needed to specify both primary and secondary motion together. This paper introduces physically-inspired rigs that propagate the primary motion of layered, illustrated characters to produce plausible secondary motion. These composable elements are rigged and controlled via a small number of parameters to produce an expressive range of effects. Our approach supports a variety of the most common secondary effects, which we demonstrate with an assortment of characters of varying complexity.","['Adobe Systems, Seattle, WA, USA', 'Adobe Syst., San Francisco, CA, USA', 'Princeton University, Princeton, NJ USA.', 'Adobe Systems, Seattle, WA, USA', 'Princeton University, Princeton, NJ USA.']","['1978546182', '2016204054', '2136484952', '2303505900', '2765814297']",2765887622.0,"{'offset': 0, 'data': [{'authorId': '7493091', 'url': 'https://www.semanticscholar.org/author/7493091', 'name': 'Nora S. Willett', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 72}, {'authorId': '2108638905', 'url': 'https://www.semanticscholar.org/author/2108638905', 'name': 'Wilmot Li', 'affiliations': [], 'homepage': None, 'paperCount': 86, 'citationCount': 3275}, {'authorId': '145492783', 'url': 'https://www.semanticscholar.org/author/145492783', 'name': 'Jovan Popović', 'affiliations': [], 'homepage': None, 'paperCount': 74, 'citationCount': 7454}, {'authorId': '2842099', 'url': 'https://www.semanticscholar.org/author/2842099', 'name': 'Floraine Berthouzoz', 'affiliations': [], 'homepage': None, 'paperCount': 22, 'citationCount': 788}, {'authorId': '37737599', 'url': 'https://www.semanticscholar.org/author/37737599', 'name': 'A. Finkelstein', 'affiliations': [], 'homepage': None, 'paperCount': 141, 'citationCount': 11278}]}",17.0,"{'DBLP': 'conf/uist/WillettLPBF17', 'MAG': '2765887622', 'DOI': '10.1145/3126594.3126641', 'CorpusId': 20531851}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",46.0,Secondary Motion for Performed 2D Animation,https://www.semanticscholar.org/paper/d8ecccdc6e869b3cdfadec7ef662d8f110fe1b42,UIST,2017
2766338868,"We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft's localization and classification capabilities in non-noisy and noisy environments.",0.0,"We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft's localization and classification capabilities in non-noisy and noisy environments.","['Univ. of Manitoba, Winnipeg (Canada)', 'Univ. of Manitoba, Winnipeg (Canada)', 'Honda Research Institution, Wako, Saitama, Japan', 'Univ. of Manitoba, Winnipeg (Canada)', 'Honda Research Institution, Wako, Saitama, Japan']","['1569396467', '2117810699', '2135093609', '2149045840', '2469184101']",2766338868.0,"{'offset': 0, 'data': [{'authorId': '40643992', 'url': 'https://www.semanticscholar.org/author/40643992', 'name': 'Teng Han', 'affiliations': [], 'homepage': None, 'paperCount': 35, 'citationCount': 505}, {'authorId': '40407309', 'url': 'https://www.semanticscholar.org/author/40407309', 'name': 'Khalad Hasan', 'affiliations': [], 'homepage': None, 'paperCount': 43, 'citationCount': 472}, {'authorId': '2110883901', 'url': 'https://www.semanticscholar.org/author/2110883901', 'name': 'Keisuke Nakamura', 'affiliations': [], 'homepage': None, 'paperCount': 108, 'citationCount': 893}, {'authorId': '145674883', 'url': 'https://www.semanticscholar.org/author/145674883', 'name': 'R. Gomez', 'affiliations': [], 'homepage': None, 'paperCount': 78, 'citationCount': 351}, {'authorId': '1773923', 'url': 'https://www.semanticscholar.org/author/1773923', 'name': 'Pourang Irani', 'affiliations': [], 'homepage': None, 'paperCount': 231, 'citationCount': 5076}]}",19.0,"{'DBLP': 'conf/uist/HanHNGI17', 'MAG': '2766338868', 'DOI': '10.1145/3126594.3126612', 'CorpusId': 23408697}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['Book', 'JournalArticle', 'Conference']",43.0,SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics,https://www.semanticscholar.org/paper/e70071576859cf9f60e088b77350b15c924d7644,UIST,2017
2766551050,"Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas. However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them. In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester-designers or end-users-describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach. These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.",0.0,"Low-fidelity prototyping at the early stages of user interface (UI) design can help designers and system builders quickly explore their ideas. However, interactive behaviors in such prototypes are often replaced by textual descriptions because it usually takes even professionals hours or days to create animated interactive elements due to the complexity of creating them. In this paper, we introduce SketchExpress, a crowd-powered prototyping tool that enables crowd workers to create reusable interactive behaviors easily and accurately. With the system, a requester-designers or end-users-describes aloud how an interface should behave and crowd workers make the sketched prototype interactive within minutes using a demonstrate-remix-replay approach. These behaviors are manually demonstrated, refined using remix functions, and then can be replayed later. The recorded behaviors persist for future reuse to help users communicate with the animated prototype. We conducted a study with crowd workers recruited from Mechanical Turk, which demonstrated that workers could create animations using SketchExpress in 2.9 minutes on average with 27% gain in the quality of animations compared to the baseline condition of manual demonstration.","['‡University of Michigan, Ann Arbor, MI, USA', '‡University of Michigan, Ann Arbor, MI, USA', '‡University of Michigan, Ann Arbor, MI, USA', '‡University of Michigan, Ann Arbor, MI, USA', '‡University of Michigan, Ann Arbor, MI, USA', '‡University of Michigan, Ann Arbor, MI, USA']","['2052517607', '2297618074', '2493949780', '2520421184', '2520528966', '2930854904']",2766551050.0,"{'offset': 0, 'data': [{'authorId': '115936483', 'url': 'https://www.semanticscholar.org/author/115936483', 'name': 'Sang Won Lee', 'affiliations': [], 'homepage': None, 'paperCount': 26, 'citationCount': 145}, {'authorId': '2145064231', 'url': 'https://www.semanticscholar.org/author/2145064231', 'name': 'Yujin Zhang', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 25}, {'authorId': '3468572', 'url': 'https://www.semanticscholar.org/author/3468572', 'name': 'Isabelle Wong', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 25}, {'authorId': '2143685798', 'url': 'https://www.semanticscholar.org/author/2143685798', 'name': 'Yi Wei Yang', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 98}, {'authorId': '1410492769', 'url': 'https://www.semanticscholar.org/author/1410492769', 'name': ""S. D. O'Keefe"", 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 109}, {'authorId': '2598433', 'url': 'https://www.semanticscholar.org/author/2598433', 'name': 'Walter S. Lasecki', 'affiliations': [], 'homepage': None, 'paperCount': 160, 'citationCount': 3541}]}",25.0,"{'DBLP': 'conf/uist/LeeZWYOL17', 'MAG': '2766551050', 'DOI': '10.1145/3126594.3126595', 'CorpusId': 25231320}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology'},10/20/2017,"['JournalArticle', 'Book', 'Conference']",40.0,SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces,https://www.semanticscholar.org/paper/0127182e5320bcbdcdad87813a58bc78eb2d2e9c,UIST,2017
2896027418,"Recent research has presented quadcopters to enable mid-air interaction. Using quadcopters to provide tactile feedback, navigation, or user input are the current scope of related work. However, most quadcopter steering systems are complicated to use for non-expert users or require an expensive tracking system for autonomous flying. Safety-critical scenarios require trained and expensive personnel to navigate quadcopters through crucial flight paths within narrow spaces. To simplify the input and manual operation of quadcopters, we present DroneCTRL, a tangible pointing device to navigate quadcopters. DroneCTRL resembles a remote control including optional visual feedback by a laser pointer and tangibility to improve the quadcopter control usability for non-expert users. In a preliminary user study, we compare the efficiency of hardware and software-based controller with DroneCTRL. Our results favor the usage of DroneCTRL with and without visual feedback to achieve more precision and accuracy.",0.0,"Recent research has presented quadcopters to enable mid-air interaction. Using quadcopters to provide tactile feedback, navigation, or user input are the current scope of related work. However, most quadcopter steering systems are complicated to use for non-expert users or require an expensive tracking system for autonomous flying. Safety-critical scenarios require trained and expensive personnel to navigate quadcopters through crucial flight paths within narrow spaces. To simplify the input and manual operation of quadcopters, we present DroneCTRL, a tangible pointing device to navigate quadcopters. DroneCTRL resembles a remote control including optional visual feedback by a laser pointer and tangibility to improve the quadcopter control usability for non-expert users. In a preliminary user study, we compare the efficiency of hardware and software-based controller with DroneCTRL. Our results favor the usage of DroneCTRL with and without visual feedback to achieve more precision and accuracy.","['LMU Munich, Munich, Germany;', '[University of Stuttgart, Stuttgart, Germany]', '[University of Stuttgart, Stuttgart, Germany]', 'TU Darmstadt, Darmstadt, Germany ', '[University of Stuttgart, Stuttgart, Germany]', 'LMU Munich, Munich, Germany;']","['2332341299', '2534088944', '2541832639', '2894272168', '3011459011', '3165213004']",2896027418.0,"{'offset': 0, 'data': [{'authorId': '40628906', 'url': 'https://www.semanticscholar.org/author/40628906', 'name': 'T. Kosch', 'affiliations': [], 'homepage': None, 'paperCount': 83, 'citationCount': 1141}, {'authorId': '1732960', 'url': 'https://www.semanticscholar.org/author/1732960', 'name': 'Markus Funk', 'affiliations': [], 'homepage': None, 'paperCount': 112, 'citationCount': 2455}, {'authorId': '79302928', 'url': 'https://www.semanticscholar.org/author/79302928', 'name': 'Daniel Vietz', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 84}, {'authorId': '27345155', 'url': 'https://www.semanticscholar.org/author/27345155', 'name': 'Marc Weise', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 59}, {'authorId': '145423483', 'url': 'https://www.semanticscholar.org/author/145423483', 'name': 'Tamara Müller', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 22}, {'authorId': '145823914', 'url': 'https://www.semanticscholar.org/author/145823914', 'name': 'A. Schmidt', 'affiliations': [], 'homepage': None, 'paperCount': 603, 'citationCount': 19695}]}",14.0,"{'MAG': '2896027418', 'DBLP': 'conf/uist/KoschFVWM018', 'DOI': '10.1145/3266037.3266121', 'CorpusId': 52981354}",['Computer Science'],1.0,False,{'name': 'The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings'},10/11/2018,"['Book', 'JournalArticle', 'Conference']",16.0,DroneCTRL: A Tangible Remote Input Control for Quadcopters,https://www.semanticscholar.org/paper/d14e6cf4b506b4e0c02a3f5abffe2a880dabca66,UIST,2018
2896165269,"We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.",1.0,"We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.","['Aarhus University, Aarhus, DENMARK,', 'University of Bayreuth, Bayreuth, Germany']","['2536419504', '2908656269']",2896165269.0,"{'offset': 0, 'data': [{'authorId': '2314755', 'url': 'https://www.semanticscholar.org/author/2314755', 'name': 'Tiare M. Feuchtner', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 156}, {'authorId': '1666509792', 'url': 'https://www.semanticscholar.org/author/1666509792', 'name': 'Jörg Müller', 'affiliations': [], 'homepage': None, 'paperCount': 24, 'citationCount': 205}]}",20.0,"{'MAG': '2896165269', 'DBLP': 'conf/uist/Feuchtner018', 'DOI': '10.1145/3242587.3242594', 'CorpusId': 52978127}",['Computer Science'],3.0,False,{'name': 'Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology'},10/11/2018,"['Book', 'JournalArticle', 'Conference']",48.0,Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift,https://www.semanticscholar.org/paper/3316e3fbc8580a5d5caf3f14eb9d1afc34926aaf,UIST,2018
2896578197,"ZEUSSS (Zero Energy Ubiquitous Sound Sensing Surface), allows physical objects and surfaces to be instrumented with a thin, self-sustainable material that provides acoustic sensing and communication capabilities. We have built a prototype ZEUSSS tag using minimal hardware and flexible electronic components, extending our original self-sustaining SATURN microphone with a printed, flexible antenna to support passive communication via analog backscatter. ZEUSSS enables objects to have ubiquitous wire-free battery-free audio based context sensing, interaction, and surveillance capabilities.",0.0,"ZEUSSS (Zero Energy Ubiquitous Sound Sensing Surface), allows physical objects and surfaces to be instrumented with a thin, self-sustainable material that provides acoustic sensing and communication capabilities. We have built a prototype ZEUSSS tag using minimal hardware and flexible electronic components, extending our original self-sustaining SATURN microphone with a printed, flexible antenna to support passive communication via analog backscatter. ZEUSSS enables objects to have ubiquitous wire-free battery-free audio based context sensing, interaction, and surveillance capabilities.","['Georgia Institute of Technology, Atlanta, GA (USA)', 'Georgia Institute of Technology, Atlanta, GA (USA)']","['2849139426', '620732773']",2896578197.0,"{'offset': 0, 'data': [{'authorId': '1556201504', 'url': 'https://www.semanticscholar.org/author/1556201504', 'name': 'Nivedita Arora', 'affiliations': [], 'homepage': None, 'paperCount': 16, 'citationCount': 101}, {'authorId': '9267108', 'url': 'https://www.semanticscholar.org/author/9267108', 'name': 'G. Abowd', 'affiliations': [], 'homepage': None, 'paperCount': 482, 'citationCount': 38493}]}",14.0,"{'DBLP': 'conf/uist/AroraA18', 'MAG': '2896578197', 'DOI': '10.1145/3266037.3266108', 'CorpusId': 52981349}",['Computer Science'],0.0,False,{'name': 'The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings'},10/11/2018,"['JournalArticle', 'Book', 'Conference']",29.0,ZEUSSS: Zero Energy Ubiquitous Sound Sensing Surface Leveraging Triboelectric Nanogenerator and Analog Backscatter Communication,https://www.semanticscholar.org/paper/c9302b5763bdeee31f551af056f168a67d52a74b,UIST,2018
2897049956,"Biased perceptions of others are known to negatively influence the outcomes of social and professional interactions in many regards. Theses biases can be informed by a multitude of non-verbal cues such as voice pitch and voice volume. This project explores how haptic effects, generated from speech, could attenuate listeners' perceived voice-related biases formed from a speaker's voice pitch. Promising preliminary results collected during a decision-making task suggest that the speech to haptic mapping and vibration delivery mechanism employed does attenuate voice-related biases. Accordingly, it is anticipated that such a system could be introduced in the workplace to equalize people's contribution opportunities and to create a more inclusive environment by reversing voice-related biases.",0.0,"Biased perceptions of others are known to negatively influence the outcomes of social and professional interactions in many regards. Theses biases can be informed by a multitude of non-verbal cues such as voice pitch and voice volume. This project explores how haptic effects, generated from speech, could attenuate listeners' perceived voice-related biases formed from a speaker's voice pitch. Promising preliminary results collected during a decision-making task suggest that the speech to haptic mapping and vibration delivery mechanism employed does attenuate voice-related biases. Accordingly, it is anticipated that such a system could be introduced in the workplace to equalize people's contribution opportunities and to create a more inclusive environment by reversing voice-related biases.","['McGill University  Montreal PQ Canada', 'McGill University  Montreal PQ Canada', 'McGill University  Montreal PQ Canada', 'McGill University  Montreal PQ Canada']","['103067196', '2766526343', '2799118494', '2896555786']",2897049956.0,"{'offset': 0, 'data': [{'authorId': '40974664', 'url': 'https://www.semanticscholar.org/author/40974664', 'name': 'Feras Al Taha', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 0}, {'authorId': '50269436', 'url': 'https://www.semanticscholar.org/author/50269436', 'name': 'Pascal E. Fortin', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 64}, {'authorId': '1404600734', 'url': 'https://www.semanticscholar.org/author/1404600734', 'name': 'Antoine Weill--Duflos', 'affiliations': [], 'homepage': None, 'paperCount': 24, 'citationCount': 22}, {'authorId': '2242019', 'url': 'https://www.semanticscholar.org/author/2242019', 'name': 'J. Cooperstock', 'affiliations': [], 'homepage': None, 'paperCount': 224, 'citationCount': 3485}]}",0.0,"{'DBLP': 'conf/uist/TahaFWC18', 'MAG': '2897049956', 'DOI': '10.1145/3266037.3266101', 'CorpusId': 52979965}",['Computer Science'],0.0,False,{'name': 'The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings'},10/11/2018,"['JournalArticle', 'Book', 'Conference']",20.0,Reversing Voice-Related Biases Through Haptic Reinforcement,https://www.semanticscholar.org/paper/d81f26f14e39f60953990c6a20e3abcb8c02f4da,UIST,2018
2897318969,"Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user's perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems.",0.0,"Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user's perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems.","['[Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]', '[Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]', '[Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]', '[Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]', '[Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]']","['2104991031', '2128401584', '2794530334', '2896834136', '2897940245']",2897318969.0,"{'offset': 0, 'data': [{'authorId': '2054813967', 'url': 'https://www.semanticscholar.org/author/2054813967', 'name': 'Yoonjeong Cha', 'affiliations': [], 'homepage': None, 'paperCount': 8, 'citationCount': 15}, {'authorId': '84230941', 'url': 'https://www.semanticscholar.org/author/84230941', 'name': 'Sungu Nam', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 7}, {'authorId': '1796342', 'url': 'https://www.semanticscholar.org/author/1796342', 'name': 'M. Yi', 'affiliations': [], 'homepage': None, 'paperCount': 119, 'citationCount': 6361}, {'authorId': '2112510900', 'url': 'https://www.semanticscholar.org/author/2112510900', 'name': 'Jaeseung Jeong', 'affiliations': [], 'homepage': None, 'paperCount': 52, 'citationCount': 801}, {'authorId': '144111726', 'url': 'https://www.semanticscholar.org/author/144111726', 'name': 'Woontack Woo', 'affiliations': ['Korea Advanced Institute of Science and Technology'], 'homepage': 'uvrlab.org', 'paperCount': 453, 'citationCount': 4637}]}",6.0,"{'DBLP': 'conf/uist/ChaNYJW18', 'MAG': '2897318969', 'DOI': '10.1145/3266037.3266086', 'CorpusId': 52978360}",['Computer Science'],0.0,False,{'name': 'The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings'},10/11/2018,"['Book', 'JournalArticle', 'Conference', 'Review']",14.0,Augmented Collaboration in Shared Space Design with Shared Attention and Manipulation,https://www.semanticscholar.org/paper/a2896d2544dd96824d9e989a02ed8aae945162ee,UIST,2018
2897578782,"The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFröst, a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFröst automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.",0.0,"The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFröst, a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFröst automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.","['University of California Berkeley, Berkeley, CA, USA ', 'Stanford University & University of California, Berkeley, Stanford, CA, USA#TAB#', 'University of California Berkeley, Berkeley, CA, USA ', 'University of California Berkeley, Berkeley, CA, USA ', 'University of California Berkeley, Berkeley, CA, USA ', 'University of California Berkeley, Berkeley, CA, USA ']","['2057195679', '2253501153', '2318425904', '2573886501', '2765535273', '2778353176']",2897578782.0,"{'offset': 0, 'data': [{'authorId': '40166636', 'url': 'https://www.semanticscholar.org/author/40166636', 'name': 'William McGrath', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 238}, {'authorId': '3257018', 'url': 'https://www.semanticscholar.org/author/3257018', 'name': 'Jeremy Warner', 'affiliations': ['UC Berkeley'], 'homepage': 'https://jeremywrnr.com/', 'paperCount': 11, 'citationCount': 223}, {'authorId': '27086017', 'url': 'https://www.semanticscholar.org/author/27086017', 'name': 'Mitchell Karchemsky', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 62}, {'authorId': '144468073', 'url': 'https://www.semanticscholar.org/author/144468073', 'name': 'Andrew Head', 'affiliations': ['University of Pennsylvania'], 'homepage': 'https://andrewhead.info', 'paperCount': 19, 'citationCount': 531}, {'authorId': '2060689759', 'url': 'https://www.semanticscholar.org/author/2060689759', 'name': 'D. Drew', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 152}, {'authorId': '28226629', 'url': 'https://www.semanticscholar.org/author/28226629', 'name': 'B. Hartmann', 'affiliations': [], 'homepage': None, 'paperCount': 147, 'citationCount': 8720}]}",6.0,"{'DBLP': 'conf/uist/McGrathWKHDH18', 'MAG': '2897578782', 'DOI': '10.1145/3242587.3242668', 'CorpusId': 52980910}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology'},10/11/2018,"['Book', 'JournalArticle', 'Conference']",41.0,WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems,https://www.semanticscholar.org/paper/688bd4cc472a406e0355911ceaa9febbcfbbe843,UIST,2018
2897838278,"End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.",0.0,"End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.","['Inria & Université de Lille, Lille, France#TAB#', 'CentraleSupélec, Rennes, France#TAB#', 'Inria & Université de Lille, Lille, France#TAB#', 'Université de Lille & Inria, Lille, France', 'Inria & Université de Lille, Lille, France#TAB#', 'Inria & Université de Lille, Lille, France#TAB#', 'Inria & Université de Lille, Lille, France#TAB#']","['142005345', '1970253328', '2149931815', '2256161107', '2321742981', '2896384725', '3123688790']",2897838278.0,"{'offset': 0, 'data': [{'authorId': '1793712', 'url': 'https://www.semanticscholar.org/author/1793712', 'name': 'Mathieu Nancel', 'affiliations': ['Inria'], 'homepage': 'http://mathieu.nancel.net', 'paperCount': 33, 'citationCount': 959}, {'authorId': '144066315', 'url': 'https://www.semanticscholar.org/author/144066315', 'name': 'S. Aranovskiy', 'affiliations': [], 'homepage': None, 'paperCount': 90, 'citationCount': 966}, {'authorId': '2772381', 'url': 'https://www.semanticscholar.org/author/2772381', 'name': 'R. Ushirobira', 'affiliations': [], 'homepage': None, 'paperCount': 93, 'citationCount': 510}, {'authorId': '153094115', 'url': 'https://www.semanticscholar.org/author/153094115', 'name': 'D. Efimov', 'affiliations': [], 'homepage': None, 'paperCount': 474, 'citationCount': 6604}, {'authorId': '27008110', 'url': 'https://www.semanticscholar.org/author/27008110', 'name': 'Sébastien Poulmane', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 30}, {'authorId': '144784319', 'url': 'https://www.semanticscholar.org/author/144784319', 'name': 'Nicolas Roussel', 'affiliations': [], 'homepage': None, 'paperCount': 92, 'citationCount': 2861}, {'authorId': '3051289', 'url': 'https://www.semanticscholar.org/author/3051289', 'name': 'Géry Casiez', 'affiliations': [], 'homepage': None, 'paperCount': 119, 'citationCount': 2460}]}",10.0,"{'DBLP': 'conf/uist/NancelAUEPRC18', 'MAG': '2897838278', 'DOI': '10.1145/3242587.3242646', 'CorpusId': 52979194}",['Computer Science'],1.0,True,{'name': 'Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology'},10/11/2018,"['JournalArticle', 'Book', 'Conference']",39.0,Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation,https://www.semanticscholar.org/paper/4b730b99fed7fa6a7b4079111c98d4e817e9aa96,UIST,2018
1974736281,"It is our great pleasure to welcome you to the 21st ACM Symposium on User Interface Software and Technology -- UIST'08. This year's symposium continues its tradition of being the premier forum for innovations in the software and technology of human-computer interfaces. UIST is an intimate, single-track conference designed to make it easy to exchange both research results and implementation experiences.  Last year's symposium celebrated the first 20 years of UIST. This year's symposium looks forward to the next 20 years. What will our technology interfaces look like 20 years from now? This year's keynote speakers will help us address this question. Dan Olsen, the ""father of UIST"", will kick off the conference with his perspective on the next 20 years. On Wednesday, Cynthia Breazeal will give a perspective on human-robot interaction, an upcoming field that will become important as the number of robots interacting with humans increases over the next two decades.  UIST'08 attracted 177 submissions (115 papers and 62 technotes) from all over the world. The program committee accepted 34 submissions (25 papers and 9 technotes) after a thorough review process: Each submission was assigned to two program committee members, a primary and a secondary. For each submission, the primary assigned at least two external reviewers and the secondary at least one. After the reviews were received, both program committee members wrote a meta-review for every submission. Authors were then given access to their reviews and metareviews and allowed to write a short rebuttal. The program committee then met face-to-face in San Francisco on 20-21 June 2008 to examine each submission and select the best ones. Submissions were finally accepted only after the authors provided a final version that took committee members' comments into account and was approved by at least one program committee member.",0.0,"It is our great pleasure to welcome you to the 21st ACM Symposium on User Interface Software and Technology -- UIST'08. This year's symposium continues its tradition of being the premier forum for innovations in the software and technology of human-computer interfaces. UIST is an intimate, single-track conference designed to make it easy to exchange both research results and implementation experiences.  Last year's symposium celebrated the first 20 years of UIST. This year's symposium looks forward to the next 20 years. What will our technology interfaces look like 20 years from now? This year's keynote speakers will help us address this question. Dan Olsen, the ""father of UIST"", will kick off the conference with his perspective on the next 20 years. On Wednesday, Cynthia Breazeal will give a perspective on human-robot interaction, an upcoming field that will become important as the number of robots interacting with humans increases over the next two decades.  UIST'08 attracted 177 submissions (115 papers and 62 technotes) from all over the world. The program committee accepted 34 submissions (25 papers and 9 technotes) after a thorough review process: Each submission was assigned to two program committee members, a primary and a secondary. For each submission, the primary assigned at least two external reviewers and the secondary at least one. After the reviews were received, both program committee members wrote a meta-review for every submission. Authors were then given access to their reviews and metareviews and allowed to write a short rebuttal. The program committee then met face-to-face in San Francisco on 20-21 June 2008 to examine each submission and select the best ones. Submissions were finally accepted only after the authors provided a final version that took committee members' comments into account and was approved by at least one program committee member.","['[Université Paris Sud, France]', '[Willow Garage, USA]']","['2674390412', '2973848920']",1974736281.0,"{'offset': 0, 'data': [{'authorId': '2070002084', 'url': 'https://www.semanticscholar.org/author/2070002084', 'name': 'S. Cousins', 'affiliations': [], 'homepage': None, 'paperCount': 27, 'citationCount': 5120}, {'authorId': '1401678555', 'url': 'https://www.semanticscholar.org/author/1401678555', 'name': 'M. Beaudouin-Lafon', 'affiliations': [], 'homepage': None, 'paperCount': 177, 'citationCount': 7255}]}",0.0,"{'DBLP': 'conf/uist/2008', 'MAG': '1974736281', 'DOI': '10.1145/1449715', 'CorpusId': 40348361}",['Computer Science'],0.0,False,"{'name': '', 'volume': ''}",10/19/2008,"['Conference', 'Review']",0.0,"Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology, Monterey, CA, USA, October 19-22, 2008",https://www.semanticscholar.org/paper/37458c38d81d453de0cbf48497ec41bd5cfc389c,UIST,2008
2001025563,"As application developers today we all face a problem of great complexity. Because of the diversity of our users, and the variety of their equipment, applications must run in many different configurations. They must support displays of varying size, resolution, and color depth. Different types of input devices are required, from keyboards to touch screens. Applications must run in different countries, being able to reformat messages in varying lengths in each language. Messages should be available in large font sizes for vision impaired users. Interface style should be consistent with other applications running on similar hardware. Style should at the same time conform to guidelines being developed by many organizations for presentation and interaction behaviors.",0.0,"As application developers today we all face a problem of great complexity. Because of the diversity of our users, and the variety of their equipment, applications must run in many different configurations. They must support displays of varying size, resolution, and color depth. Different types of input devices are required, from keyboards to touch screens. Applications must run in different countries, being able to reformat messages in varying lengths in each language. Messages should be available in large font sizes for vision impaired users. Interface style should be consistent with other applications running on similar hardware. Style should at the same time conform to guidelines being developed by many organizations for presentation and interaction behaviors.","['IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY', 'IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY']","['2076680043', '222389444']",2001025563.0,"{'offset': 0, 'data': [{'authorId': '3182960', 'url': 'https://www.semanticscholar.org/author/3182960', 'name': 'Charles Wiecha', 'affiliations': [], 'homepage': None, 'paperCount': 28, 'citationCount': 462}, {'authorId': '2882275', 'url': 'https://www.semanticscholar.org/author/2882275', 'name': 'S. Boies', 'affiliations': [], 'homepage': None, 'paperCount': 50, 'citationCount': 3145}]}",27.0,"{'DBLP': 'conf/uist/WiechaB90', 'MAG': '2001025563', 'DOI': '10.1145/97924.97927', 'CorpusId': 838753}",['Computer Science'],2.0,False,{'pages': '21-30'},8/1/1990,['JournalArticle'],28.0,Generating user interfaces: principles and use of it style rules,https://www.semanticscholar.org/paper/4da49239adbb29177dca521fb07b173c4f9e9e05,UIST,1990
2029697180,"Creating high-quality label layouts in a particular visual style is a time-consuming process. Although automated labeling algorithms can aid the layout process, expert design knowledge is required to tune these algorithms so that they produce layouts which meet the designer's expectations. We propose a system which can learn a labellayout style from a single example layout and then apply this style to new labeling problems. Because designers find it much easier to create example layouts than tune algorithmic parameters, our system provides a more natural workflow for graphic designers. We demonstrate that our system is capable of learning a variety of label layout styles from examples.",1.0,"Creating high-quality label layouts in a particular visual style is a time-consuming process. Although automated labeling algorithms can aid the layout process, expert design knowledge is required to tune these algorithms so that they produce layouts which meet the designer's expectations. We propose a system which can learn a labellayout style from a single example layout and then apply this style to new labeling problems. Because designers find it much easier to create example layouts than tune algorithmic parameters, our system provides a more natural workflow for graphic designers. We demonstrate that our system is capable of learning a variety of label layout styles from examples.","['Univ. of Toronto, Toronto ON Canada', 'Univ. of Toronto, Toronto ON Canada', 'Univ. of Toronto, Toronto ON Canada', 'University of California-Berkeley, Berkeley, CA']","['1965004193', '2154794983', '2232442745', '718039462']",2029697180.0,"{'offset': 0, 'data': [{'authorId': '2397179', 'url': 'https://www.semanticscholar.org/author/2397179', 'name': 'Ian Vollick', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 99}, {'authorId': '145731603', 'url': 'https://www.semanticscholar.org/author/145731603', 'name': 'Daniel Vogel', 'affiliations': [], 'homepage': None, 'paperCount': 111, 'citationCount': 3712}, {'authorId': '1820412', 'url': 'https://www.semanticscholar.org/author/1820412', 'name': 'Maneesh Agrawala', 'affiliations': [], 'homepage': None, 'paperCount': 230, 'citationCount': 13867}, {'authorId': '1747779', 'url': 'https://www.semanticscholar.org/author/1747779', 'name': 'Aaron Hertzmann', 'affiliations': [], 'homepage': None, 'paperCount': 169, 'citationCount': 17711}]}",42.0,"{'DBLP': 'conf/uist/VollickVAH07', 'MAG': '2029697180', 'DOI': '10.1145/1294211.1294252', 'CorpusId': 3038732}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 20th annual ACM symposium on User interface software and technology'},10/7/2007,"['Book', 'JournalArticle', 'Conference']",30.0,Specifying label layout style by example,https://www.semanticscholar.org/paper/67e48059dc104caed8879fef417f059469dd5220,UIST,2007
2030867627,"We describe WEST, a WEb browser for Small Terminals, that aims to solve some of the problems associated with accessing web pages on hand-held devices. Through a novel combination of text reduction and focus+context visualization, users can access web pages from a very limited display environment, since the system will provide an overview of the contents of a web page even when it is too large to be displayed in its entirety. To make maximum use of the limited resources available on a typical hand-held terminal, much of the most demanding work is done by a proxy server, allowing the terminal to concentrate on the task of providing responsive user interaction. The system makes use of some interaction concepts reminiscent of those defined in the Wireless Application Protocol (WAP), making it possible to utilize the techniques described here for WAP-compliant devices and services that may become available in the near future.",1.0,"We describe WEST, a WEb browser for Small Terminals, that aims to solve some of the problems associated with accessing web pages on hand-held devices. Through a novel combination of text reduction and focus+context visualization, users can access web pages from a very limited display environment, since the system will provide an overview of the contents of a web page even when it is too large to be displayed in its entirety. To make maximum use of the limited resources available on a typical hand-held terminal, much of the most demanding work is done by a proxy server, allowing the terminal to concentrate on the task of providing responsive user interaction. The system makes use of some interaction concepts reminiscent of those defined in the Wireless Application Protocol (WAP), making it possible to utilize the techniques described here for WAP-compliant devices and services that may become available in the near future.","['Swedish Institute of Computer Science, Box 1263, SE-164 29 Kista, Sweden', 'Viktoria Institute, Box 620, SE-405 30, Göteborg, Sweden', 'Viktoria Institute, Box 620, SE-405 30, Göteborg, Sweden', 'Viktoria Institute, Box 620, SE-405 30, Göteborg, Sweden', 'Swedish Institute of Computer Science, Box 1263, SE-164 29 Kista, Sweden', 'Telia Mobile AB, SE-131 86 Nacka Strand, Sweden', 'Telia Research AB, SE-123 86 Farsta, Sweden']","['1073530958', '1984859990', '2026149416', '2090645225', '2105206177', '2284064335', '2676770102']",2030867627.0,"{'offset': 0, 'data': [{'authorId': '145548421', 'url': 'https://www.semanticscholar.org/author/145548421', 'name': 'Staffan Björk', 'affiliations': [], 'homepage': None, 'paperCount': 129, 'citationCount': 3590}, {'authorId': '1696081', 'url': 'https://www.semanticscholar.org/author/1696081', 'name': 'L. Holmquist', 'affiliations': [], 'homepage': None, 'paperCount': 176, 'citationCount': 4168}, {'authorId': '2011570', 'url': 'https://www.semanticscholar.org/author/2011570', 'name': 'Johan Redström', 'affiliations': [], 'homepage': None, 'paperCount': 103, 'citationCount': 4082}, {'authorId': '1692618', 'url': 'https://www.semanticscholar.org/author/1692618', 'name': 'I. Bretan', 'affiliations': [], 'homepage': None, 'paperCount': 32, 'citationCount': 454}, {'authorId': '2074944410', 'url': 'https://www.semanticscholar.org/author/2074944410', 'name': 'Rolf Danielsson', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 146}, {'authorId': '1742359', 'url': 'https://www.semanticscholar.org/author/1742359', 'name': 'Jussi Karlgren', 'affiliations': [], 'homepage': None, 'paperCount': 247, 'citationCount': 2941}, {'authorId': '21769455', 'url': 'https://www.semanticscholar.org/author/21769455', 'name': 'Kristofer Franzén', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 425}]}",146.0,"{'MAG': '2030867627', 'DBLP': 'conf/uist/BjorkHRBDKF99', 'DOI': '10.1145/320719.322601', 'CorpusId': 10499236}",['Computer Science'],5.0,False,{'pages': '187-196'},11/7/1999,"['JournalArticle', 'Review']",43.0,WEST: a Web browser for small terminals,https://www.semanticscholar.org/paper/40e80e0d02c9b26c5fd383ef5a6dce3aab981e7b,UIST,1999
2036972019,"Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Navigational techniques which utilize some representation of the entire information space provide context to support more detailed information views. However, the limited number of pixels on the screen makes it difficult to completely display large information spaces. The Zrz.irrnation Mural is a twodimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The mural creates a miniature version of the information space using visual attributes such as grayscale shading, intensity, color, and pixel size, along with anti-aliased compression techniques. Information murals can be used as stand-alone visualizations or in global navigational views.",1.0,"Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Navigational techniques which utilize some representation of the entire information space provide context to support more detailed information views. However, the limited number of pixels on the screen makes it difficult to completely display large information spaces. The Zrz.irrnation Mural is a twodimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The mural creates a miniature version of the information space using visual attributes such as grayscale shading, intensity, color, and pixel size, along with anti-aliased compression techniques. Information murals can be used as stand-alone visualizations or in global navigational views.","['Graphics, Visualization & Usability Center, College of Computing, Georgia Institute of Technology, Atlanta, GA#TAB#', 'Graphics, Visualization & Usability Center, College of Computing, Georgia Institute of Technology, Atlanta, GA#TAB#']","['1990261876', '2087252583']",2036972019.0,"{'offset': 0, 'data': [{'authorId': '1903352', 'url': 'https://www.semanticscholar.org/author/1903352', 'name': 'Dean F. Jerding', 'affiliations': [], 'homepage': None, 'paperCount': 13, 'citationCount': 660}, {'authorId': '1691661', 'url': 'https://www.semanticscholar.org/author/1691661', 'name': 'J. Stasko', 'affiliations': [], 'homepage': None, 'paperCount': 347, 'citationCount': 25601}]}",14.0,"{'DBLP': 'conf/uist/JerdingS95', 'MAG': '2036972019', 'DOI': '10.1145/215585.215660', 'CorpusId': 15325948}",['Computer Science'],0.0,False,{'pages': '73-74'},12/1/1995,['JournalArticle'],6.0,Using information murals in visualization applications,https://www.semanticscholar.org/paper/375a170b27cd5772ecfbd3dc586c74bcad6770ca,UIST,1995
2050427684,"We present experimental work which explores how the match (or mismatch) between the input space of the hands and the output space of a graphical display influences two-handed input performance. During interaction with computers, a direct correspondence between the input and output spaces is often lacking. Not only are the hands disjoint from the display space, but the reference frames of the hands may in fact be disjoint from one another if two separate input devices (e.g. two mice) are used for two-handed input. In general, we refer to the workspace and origin within which the hands operate as kinesthetic reference frames. Our goal is to better understand how an interface designer's choice of kinesthetic reference frames influences a user's ability to coordinate two-handed movements, and to explore how the answer to this question may depend on the availability of visual feedback. Understanding this issue has implications for the design of two-handed interaction techniques and input devices, as well as for the reference principle of Guiard's Kinematic Chain model of human bimanual action. Our results suggest that the Guiard reference principle is robust with respect to variances in the kinesthetic reference frames as long as appropriate visual feedback is present.",1.0,"We present experimental work which explores how the match (or mismatch) between the input space of the hands and the output space of a graphical display influences two-handed input performance. During interaction with computers, a direct correspondence between the input and output spaces is often lacking. Not only are the hands disjoint from the display space, but the reference frames of the hands may in fact be disjoint from one another if two separate input devices (e.g. two mice) are used for two-handed input. In general, we refer to the workspace and origin within which the hands operate as kinesthetic reference frames. Our goal is to better understand how an interface designer's choice of kinesthetic reference frames influences a user's ability to coordinate two-handed movements, and to explore how the answer to this question may depend on the availability of visual feedback. Understanding this issue has implications for the design of two-handed interaction techniques and input devices, as well as for the reference principle of Guiard's Kinematic Chain model of human bimanual action. Our results suggest that the Guiard reference principle is robust with respect to variances in the kinesthetic reference frames as long as appropriate visual feedback is present.","['Microsoft Research, One Microsoft Way Redmond, WA#TAB#', 'Alias|wavefront, 210 King Street East, Toronto, Ontario, Canada M5A 1J7 and Dept. of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 3G4#TAB#']","['1560725665', '2130130894']",2050427684.0,"{'offset': 0, 'data': [{'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}, {'authorId': '1738072', 'url': 'https://www.semanticscholar.org/author/1738072', 'name': 'K. Hinckley', 'affiliations': [], 'homepage': None, 'paperCount': 163, 'citationCount': 9081}]}",127.0,"{'DBLP': 'conf/uist/BalakrishnanH99', 'MAG': '2050427684', 'DOI': '10.1145/320719.322599', 'CorpusId': 14318231}",['Computer Science'],5.0,True,{'pages': '171-178'},11/7/1999,['JournalArticle'],31.0,The role of kinesthetic reference frames in two-handed input performance,https://www.semanticscholar.org/paper/0aae99df980156f308f382ccec79afa6aa1ec612,UIST,1999
2059522049,"Nova is a simple technique for animating a data sequence whose elements include a primary numeric component and possibly one or more secondary dimensions. We use nova to visualize program behavior such as individual memory allocations, where the number of bytes in each allocation is a natural primary numeric dimension.",0.0,"Nova is a simple technique for animating a data sequence whose elements include a primary numeric component and possibly one or more secondary dimensions. We use nova to visualize program behavior such as individual memory allocations, where the number of bytes in each allocation is a natural primary numeric dimension.","['Division of MCSS, The University of Texas at San Antonio, San Antonio, TX', 'Department of Computer Science, The University of Arizona, Tucson, AZ#TAB#']","['2114141880', '2180934655']",2059522049.0,"{'offset': 0, 'data': [{'authorId': '3021626', 'url': 'https://www.semanticscholar.org/author/3021626', 'name': 'R. Griswold', 'affiliations': [], 'homepage': None, 'paperCount': 117, 'citationCount': 1446}, {'authorId': '1771900', 'url': 'https://www.semanticscholar.org/author/1771900', 'name': 'C. Jeffery', 'affiliations': [], 'homepage': None, 'paperCount': 106, 'citationCount': 608}]}",5.0,"{'MAG': '2059522049', 'DBLP': 'conf/uist/GriswoldJ94', 'DOI': '10.1145/192426.192477', 'CorpusId': 30849131}",['Computer Science'],0.0,False,{'pages': '131-132'},11/2/1994,['JournalArticle'],1.0,Nova: low-cost data animation using a radar-sweep metaphor,https://www.semanticscholar.org/paper/2b2e29750b2713621d1d8a8a6303a8ea6f8e2655,UIST,1994
2141716420,"Managing large amounts of dynamic visual information involves understanding changes happening out of the user's sight. In this paper, we show how current software does not adequately support users in this task, and motivate the need for a more general approach. We propose an image-based storage, visualization, and implicit interaction paradigm called mnemonic rendering that provides better support for handling visual changes. Once implemented on a system, mnemonic rendering techniques can benefit all applications. We explore its rich design space and discuss its expected benefits as well as limitations based on feedback from users of a small-screen and a wall-size prototype.",1.0,"Managing large amounts of dynamic visual information involves understanding changes happening out of the user's sight. In this paper, we show how current software does not adequately support users in this task, and motivate the need for a more general approach. We propose an image-based storage, visualization, and implicit interaction paradigm called mnemonic rendering that provides better support for handling visual changes. Once implemented on a system, mnemonic rendering techniques can benefit all applications. We explore its rich design space and discuss its expected benefits as well as limitations based on feedback from users of a small-screen and a wall-size prototype.","['University of Toronto', 'University of Toronto', 'University of Toronto']","['2130130894', '2676222217', '89785269']",2141716420.0,"{'offset': 0, 'data': [{'authorId': '34790289', 'url': 'https://www.semanticscholar.org/author/34790289', 'name': 'A. Bezerianos', 'affiliations': [], 'homepage': None, 'paperCount': 99, 'citationCount': 2041}, {'authorId': '3297322', 'url': 'https://www.semanticscholar.org/author/3297322', 'name': 'Pierre Dragicevic', 'affiliations': [], 'homepage': None, 'paperCount': 128, 'citationCount': 4992}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}]}",66.0,"{'DBLP': 'conf/uist/BezerianosDB06', 'MAG': '2141716420', 'DOI': '10.1145/1166253.1166279', 'CorpusId': 1530768}",['Computer Science'],4.0,False,{'pages': '159-168'},10/15/2006,"['JournalArticle', 'Conference']",40.0,Mnemonic rendering: an image-based approach for exposing hidden changes in dynamic displays,https://www.semanticscholar.org/paper/8c6fc4ffddd52723dce69d18370ac2a28d4c273f,UIST,2006
2141964067,"Previous work has demonstrated the viability of applying offline analysis to interpret forearm electromyography (EMG) and classify finger gestures on a physical surface. We extend those results to bring us closer to using muscle-computer interfaces for always-available input in real-world applications. We leverage existing taxonomies of natural human grips to develop a gesture set covering interaction in free space even when hands are busy with other objects. We present a system that classifies these gestures in real-time and we introduce a bi-manual paradigm that enables use in interactive systems. We report experimental results demonstrating four-finger classification accuracies averaging 79% for pinching, 85% while holding a travel mug, and 88% when carrying a weighted bag. We further show generalizability across different arm postures and explore the tradeoffs of providing real-time visual feedback.",1.0,"Previous work has demonstrated the viability of applying offline analysis to interpret forearm electromyography (EMG) and classify finger gestures on a physical surface. We extend those results to bring us closer to using muscle-computer interfaces for always-available input in real-world applications. We leverage existing taxonomies of natural human grips to develop a gesture set covering interaction in free space even when hands are busy with other objects. We present a system that classifies these gestures in real-time and we introduce a bi-manual paradigm that enables use in interactive systems. We report experimental results demonstrating four-finger classification accuracies averaging 79% for pinching, 85% while holding a travel mug, and 88% when carrying a weighted bag. We further show generalizability across different arm postures and explore the tradeoffs of providing real-time visual feedback.","['Microsoft Research, redmond, WA, USA#TAB#', '[Microsoft Corp., Redmond, WA, USA]', 'Univ. of Toronto, Toronto ON Canada', 'Microsoft Research, redmond, WA, USA#TAB#', ' Univ. of Washington, Seattle, WA, USA', ' Univ. of Washington, Seattle, WA, USA']","['2105140892', '2120281375', '2130130894', '2168727892', '29409133', '400626676']",2141964067.0,"{'offset': 0, 'data': [{'authorId': '1766388', 'url': 'https://www.semanticscholar.org/author/1766388', 'name': 'T. S. Saponas', 'affiliations': [], 'homepage': None, 'paperCount': 38, 'citationCount': 2446}, {'authorId': '1719056', 'url': 'https://www.semanticscholar.org/author/1719056', 'name': 'Desney S. Tan', 'affiliations': [], 'homepage': None, 'paperCount': 151, 'citationCount': 9894}, {'authorId': '144864481', 'url': 'https://www.semanticscholar.org/author/144864481', 'name': 'Dan Morris', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 4182}, {'authorId': '1748870', 'url': 'https://www.semanticscholar.org/author/1748870', 'name': 'Ravin Balakrishnan', 'affiliations': [], 'homepage': None, 'paperCount': 179, 'citationCount': 14686}, {'authorId': '2115150052', 'url': 'https://www.semanticscholar.org/author/2115150052', 'name': 'Jim Turner', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 432}, {'authorId': '9522307', 'url': 'https://www.semanticscholar.org/author/9522307', 'name': 'J. Landay', 'affiliations': [], 'homepage': None, 'paperCount': 285, 'citationCount': 22329}]}",302.0,"{'MAG': '2141964067', 'DBLP': 'conf/uist/SaponasTMBTL09', 'DOI': '10.1145/1622176.1622208', 'CorpusId': 9079288}",['Computer Science'],10.0,False,{'pages': '167-176'},10/4/2009,['JournalArticle'],28.0,Enabling always-available input with muscle-computer interfaces,https://www.semanticscholar.org/paper/3fd89cf73d1705befe7a08dcc5df2abc6271d496,UIST,2009
2532049200,"Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn't need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.",0.0,"Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn't need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.","['NTT docomo, Yokosuka, Kanagawa, Japan', 'NTT docomo, Yokosuka, Kanagawa, Japan']","['2214682394', '2937340977']",2532049200.0,"{'offset': 0, 'data': [{'authorId': '2090553785', 'url': 'https://www.semanticscholar.org/author/2090553785', 'name': 'Wataru Yamada', 'affiliations': [], 'homepage': None, 'paperCount': 32, 'citationCount': 136}, {'authorId': '2962182', 'url': 'https://www.semanticscholar.org/author/2962182', 'name': 'H. Manabe', 'affiliations': [], 'homepage': None, 'paperCount': 63, 'citationCount': 455}]}",11.0,"{'MAG': '2532049200', 'DBLP': 'conf/uist/YamadaM16', 'DOI': '10.1145/2984751.2985735', 'CorpusId': 3062649}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['Book', 'JournalArticle', 'Conference']",2.0,Expanding the Field-of-View of Head-Mounted Displays with Peripheral Blurred Images,https://www.semanticscholar.org/paper/ff3f4019d9a8caf07a1289abe590d14fc5d5c253,UIST,2016
2082506607,"The merits of direct manipulation are now widely recognized. However, direct manipulation interfaces incur high cost in their creation. To cope with this problem, we present a model of bidirectional translation between pictures and abstract application data, and a prototype system, TRIP2, based on this model. Using this model, general mapping from abstract data to pictures and from pictures to abstract data is realized merely by giving declarative mapping rules, allowing fast and easy creation of direct manipulation interfaces. We apply the prototype system to the generation of the interfaces for kinship diagrams, Graph Editors, E-R diagrams, and an Othello game.",1.0,"The merits of direct manipulation are now widely recognized. However, direct manipulation interfaces incur high cost in their creation. To cope with this problem, we present a model of bidirectional translation between pictures and abstract application data, and a prototype system, TRIP2, based on this model. Using this model, general mapping from abstract data to pictures and from pictures to abstract data is realized merely by giving declarative mapping rules, allowing fast and easy creation of direct manipulation interfaces. We apply the prototype system to the generation of the interfaces for kinship diagrams, Graph Editors, E-R diagrams, and an Othello game.","['Research and Development, ACCESS CO., LTD., 1-7-1 Sarugaku-cho, Chiyoda-ku, Tokyo, 101 Japan#TAB#', 'Department of Information Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-Ku, Tokyo, 113 Japan#TAB#', 'Department of Information Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-Ku, Tokyo, 113 Japan#TAB#', 'Department of Information Science, University of Tokyo, 7-3-1 Hongo, Bunkyo-Ku, Tokyo, 113 Japan#TAB#']","['2098643000', '2164966972', '2250226055', '660900409']",2082506607.0,"{'offset': 0, 'data': [{'authorId': '2049528639', 'url': 'https://www.semanticscholar.org/author/2049528639', 'name': 'Shin Takahashi', 'affiliations': [], 'homepage': None, 'paperCount': 23, 'citationCount': 262}, {'authorId': '145936011', 'url': 'https://www.semanticscholar.org/author/145936011', 'name': 'S. Matsuoka', 'affiliations': [], 'homepage': None, 'paperCount': 483, 'citationCount': 11902}, {'authorId': '1748361', 'url': 'https://www.semanticscholar.org/author/1748361', 'name': 'A. Yonezawa', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 5624}, {'authorId': '3237240', 'url': 'https://www.semanticscholar.org/author/3237240', 'name': 'Tomihisa Kamada', 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 3075}]}",44.0,"{'DBLP': 'conf/uist/TakahashiMYK91', 'MAG': '2082506607', 'DOI': '10.1145/120782.120800', 'CorpusId': 10328193}",['Computer Science'],1.0,False,{'pages': '165-174'},11/11/1991,['JournalArticle'],65.0,A general framework for Bi-directional translation between abstract and pictorial data,https://www.semanticscholar.org/paper/10dd6444b627f2ce3ef915a0f617eac7eb9170ad,UIST,1991
2117043006,"We present a survey of design issues for developing effective free-space three-dimensional (3D) user interfaces. Our survey is based upon previous work in 3D interaction, our experience in developing free-space interfaces, and our informal observations of test users. We illustrate our design issues using examples drawn from instances of 3D interfaces.For example, our first issue suggests that users have difficulty understanding three-dimensional space. We offer a set of strategies which may help users to better perceive a 3D virtual environment, including the use of spatial references, relative gesture, two-handed interaction, multisensory feedback, physical constraints, and head tracking. We describe interfaces which employ these strategies.Our major contribution is the synthesis of many scattered results, observations, and examples into a common framework. This framework should serve as a guide to researchers or systems builders who may not be familiar with design issues in spatial input. Where appropriate, we also try to identify areas in free-space 3D interaction which we see as likely candidates for additional research.An extended and annotated version of the references list for this paper is available on-line through mosaic at address http://uvacs.cs.virginia.edu/~kph2q/.",1.0,"We present a survey of design issues for developing effective free-space three-dimensional (3D) user interfaces. Our survey is based upon previous work in 3D interaction, our experience in developing free-space interfaces, and our informal observations of test users. We illustrate our design issues using examples drawn from instances of 3D interfaces.For example, our first issue suggests that users have difficulty understanding three-dimensional space. We offer a set of strategies which may help users to better perceive a 3D virtual environment, including the use of spatial references, relative gesture, two-handed interaction, multisensory feedback, physical constraints, and head tracking. We describe interfaces which employ these strategies.Our major contribution is the synthesis of many scattered results, observations, and examples into a common framework. This framework should serve as a guide to researchers or systems builders who may not be familiar with design issues in spatial input. Where appropriate, we also try to identify areas in free-space 3D interaction which we see as likely candidates for additional research.An extended and annotated version of the references list for this paper is available on-line through mosaic at address http://uvacs.cs.virginia.edu/~kph2q/.","['University of Virginia, Departments of Neurosurgery and Computer Science', 'University of Virginia, Departments of Neurosurgery#TAB#', 'University of Virginia, Departments of Computer Science', 'University of Virginia, Departments of Neurosurgery#TAB#']","['1560725665', '2133807043', '2139439197', '604364466']",2117043006.0,"{'offset': 0, 'data': [{'authorId': '1738072', 'url': 'https://www.semanticscholar.org/author/1738072', 'name': 'K. Hinckley', 'affiliations': [], 'homepage': None, 'paperCount': 163, 'citationCount': 9081}, {'authorId': '1717974', 'url': 'https://www.semanticscholar.org/author/1717974', 'name': 'R. Pausch', 'affiliations': [], 'homepage': None, 'paperCount': 138, 'citationCount': 11436}, {'authorId': '2592863', 'url': 'https://www.semanticscholar.org/author/2592863', 'name': 'J. Goble', 'affiliations': [], 'homepage': None, 'paperCount': 51, 'citationCount': 2202}, {'authorId': '47013687', 'url': 'https://www.semanticscholar.org/author/47013687', 'name': 'N. Kassell', 'affiliations': [], 'homepage': None, 'paperCount': 240, 'citationCount': 18786}]}",417.0,"{'DBLP': 'conf/uist/HinckleyPGK94', 'MAG': '2117043006', 'DOI': '10.1145/192426.192501', 'CorpusId': 83668}",['Computer Science'],23.0,False,{'pages': '213-222'},11/2/1994,"['JournalArticle', 'Review']",91.0,A survey of design issues in spatial input,https://www.semanticscholar.org/paper/97c9c7451cf45ab237b884eb7b7efef076efe1a7,UIST,1994
1969665019,"Intrabody communication (IBC) is a wireless communications technology that uses a person's body as the transmission medium for imperceptible electrical signals. Because communication is limited to the vicinity of a person's body, ambiguities arising from communication between personal devices and environmental devices when multiple people are present can, in theory, be solved simply. Intrabody communication also potentially allows data to be transferred when a person touches an IBC-enabled device. We have designed and constructed an intrabody communication system, modeled after Zimmerman's original design, and extended it to operate up to 38.4Kbps and to calculate signal strength. In this paper, we present quantitative measurements of data error rates and signal strength while varying hand distance to transceiver plate, electrode location on the body, touch plate size and shape, and several other factors. We find that plate size and shape have only minor effects, but that the distance to plate and the coupling mechanism significantly effect signal strength. We also find that portable devices, with poor ground coupling, suffer more significant signal attenuation. Our goal is to promote design guidelines for this technology and identify the best contexts for its effective deployment.",1.0,"Intrabody communication (IBC) is a wireless communications technology that uses a person's body as the transmission medium for imperceptible electrical signals. Because communication is limited to the vicinity of a person's body, ambiguities arising from communication between personal devices and environmental devices when multiple people are present can, in theory, be solved simply. Intrabody communication also potentially allows data to be transferred when a person touches an IBC-enabled device. We have designed and constructed an intrabody communication system, modeled after Zimmerman's original design, and extended it to operate up to 38.4Kbps and to calculate signal strength. In this paper, we present quantitative measurements of data error rates and signal strength while varying hand distance to transceiver plate, electrode location on the body, touch plate size and shape, and several other factors. We find that plate size and shape have only minor effects, but that the distance to plate and the coupling mechanism significantly effect signal strength. We also find that portable devices, with poor ground coupling, suffer more significant signal attenuation. Our goal is to promote design guidelines for this technology and identify the best contexts for its effective deployment.","[' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA', ' †University of Washington, Seattle, WA']","['141796421', '2066352752', '2225370072', '2226180680', '2230861503', '2231298662', '3186707992']",1969665019.0,"{'offset': 0, 'data': [{'authorId': '145271096', 'url': 'https://www.semanticscholar.org/author/145271096', 'name': 'K. Partridge', 'affiliations': [], 'homepage': None, 'paperCount': 50, 'citationCount': 1827}, {'authorId': '2079972410', 'url': 'https://www.semanticscholar.org/author/2079972410', 'name': 'Bradley Dahlquist', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 100}, {'authorId': '133679150', 'url': 'https://www.semanticscholar.org/author/133679150', 'name': 'Alireza Veiseh', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 101}, {'authorId': '40573604', 'url': 'https://www.semanticscholar.org/author/40573604', 'name': 'Annie Cain', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 98}, {'authorId': '2078666340', 'url': 'https://www.semanticscholar.org/author/2078666340', 'name': 'Ann Foreman', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 98}, {'authorId': '2057319022', 'url': 'https://www.semanticscholar.org/author/2057319022', 'name': 'Joseph Goldberg', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 235}, {'authorId': '1735801', 'url': 'https://www.semanticscholar.org/author/1735801', 'name': 'G. Borriello', 'affiliations': [], 'homepage': None, 'paperCount': 308, 'citationCount': 20795}]}",98.0,"{'MAG': '1969665019', 'DBLP': 'conf/uist/PartridgeDVCFGB01', 'DOI': '10.1145/502348.502381', 'CorpusId': 8371590}",['Computer Science'],5.0,False,{'pages': '183-190'},11/11/2001,['JournalArticle'],19.0,Empirical measurements of intrabody communication performance under varied physical configurations,https://www.semanticscholar.org/paper/730527d54a5e27c8ebc5ffd57e670801f9ca5f3e,UIST,2001
2039884255,"This paper describes Doppler a new, fast algorithm for supporting concurrent, one-way constraints between objects situated in multiple address spaces. Because of their declarative nature, convenience, low amortized cost, and good match to interface tasks, constraints have been used to support a variety of user-interface activities. Unfortunately, nearly all existing constraint maintenance algorithms are sequential in nature, and cannot function effectively in a concurrent or dktributed setting. The Doppler algorithm overcomes these limitations. It is a highly efficient distributed and concurrent algorithm (based on an efficient sequential algorithm for incremental, lazy updates). Doppler relies solely on asynchronous message passing, and does not require shared memory, synchronized clocks, or a global synchronization mechanism. It supports a high degree of concurrency by efficiently tracking potential cause and effect relationships between reads and writes, and allowing all causally independent operations to execute in parallel. This makes it scalable, and optimizes reads and writes by minimizing their blocking time.",0.0,"This paper describes Doppler a new, fast algorithm for supporting concurrent, one-way constraints between objects situated in multiple address spaces. Because of their declarative nature, convenience, low amortized cost, and good match to interface tasks, constraints have been used to support a variety of user-interface activities. Unfortunately, nearly all existing constraint maintenance algorithms are sequential in nature, and cannot function effectively in a concurrent or dktributed setting. The Doppler algorithm overcomes these limitations. It is a highly efficient distributed and concurrent algorithm (based on an efficient sequential algorithm for incremental, lazy updates). Doppler relies solely on asynchronous message passing, and does not require shared memory, synchronized clocks, or a global synchronization mechanism. It supports a high degree of concurrency by efficiently tracking potential cause and effect relationships between reads and writes, and allowing all causally independent operations to execute in parallel. This makes it scalable, and optimizes reads and writes by minimizing their blocking time.","['Graphics, Visualization & Usability Center, Georgia Institute of Technology, Atlanta GA#TAB#', 'Graphics, Visualization & Usability Center, Georgia Institute of Technology, Atlanta GA#TAB#']","['2171298838', '2893140440']",2039884255.0,"{'offset': 0, 'data': [{'authorId': '145845790', 'url': 'https://www.semanticscholar.org/author/145845790', 'name': 'K. Bharat', 'affiliations': [], 'homepage': None, 'paperCount': 38, 'citationCount': 3627}, {'authorId': '1749296', 'url': 'https://www.semanticscholar.org/author/1749296', 'name': 'S. Hudson', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 13652}]}",21.0,"{'MAG': '2039884255', 'DBLP': 'conf/uist/BharatH95', 'DOI': '10.1145/215585.215708', 'CorpusId': 15220838}",['Computer Science'],0.0,False,{'pages': '121-132'},12/1/1995,['JournalArticle'],24.0,"Supporting distributed, concurrent, one-way constraints in user interface applications",https://www.semanticscholar.org/paper/72982c045f8a8cd60bec8efd410a1ef617e42c92,UIST,1995
2083204176,"We present a system for entering common music notation based on 2D gestural input. The key feature of the system is the look-and-feel of the interface which approximates sketching music with paper and pencil. A probability-based interpreter integrates sequences of gestural input to perform the most common notation and editing operations. In this paper, we present the user’s model of the system, the components of the high-level recognition system, and a discussion of the evolution of the system including user feedback.",1.0,"We present a system for entering common music notation based on 2D gestural input. The key feature of the system is the look-and-feel of the interface which approximates sketching music with paper and pencil. A probability-based interpreter integrates sequences of gestural input to perform the most common notation and editing operations. In this paper, we present the user’s model of the system, the components of the high-level recognition system, and a discussion of the evolution of the system including user feedback.","['Brown University, Department of Computer Science, Providence, RI', 'Brown University, Department of Computer Science, Providence, RI', 'Brown University, Department of Computer Science, Providence, RI']","['2126516391', '2131577418', '49818817']",2083204176.0,"{'offset': 0, 'data': [{'authorId': '32959330', 'url': 'https://www.semanticscholar.org/author/32959330', 'name': 'A. Forsberg', 'affiliations': [], 'homepage': None, 'paperCount': 52, 'citationCount': 1860}, {'authorId': '2069061670', 'url': 'https://www.semanticscholar.org/author/2069061670', 'name': 'Mark Dieterich', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 99}, {'authorId': '1713625', 'url': 'https://www.semanticscholar.org/author/1713625', 'name': 'R. Zeleznik', 'affiliations': [], 'homepage': None, 'paperCount': 81, 'citationCount': 4987}]}",99.0,"{'DBLP': 'conf/uist/ForsbergDZ98', 'MAG': '2083204176', 'DOI': '10.1145/288392.288608', 'CorpusId': 2508536}",['Computer Science'],6.0,False,{'pages': '203-210'},11/1/1998,['JournalArticle'],18.0,The music notepad,https://www.semanticscholar.org/paper/8df523d5393346e1729618a23a9be627cf4eb0e4,UIST,1998
2180120384,"Physical sketching of 3D wireframe models, using a hand-held plastic extruder, allows users to explore the design space of 3D models efficiently. Unfortunately, the scale of these devices limits users' design explorations to small-scale objects. We present protopiper, a computer aided, hand-held fabrication device, that allows users to sketch room-sized objects at actual scale. The key idea behind protopiper is that it forms adhesive tape into tubes as its main building material, rather than extruded plastic or photopolymer lines. Since the resulting tubes are hollow they offer excellent strength-to-weight ratio, thus scale well to large structures. Since the tape is pre-coated with adhesive it allows connecting tubes quickly, unlike extruded plastic that would require heating and cooling in the kilowatt range. We demonstrate protopiper's use through several demo objects, ranging from more constructive objects, such as furniture, to more decorative objects, such as statues. In our exploratory user study, 16 participants created objects based on their own ideas. They rated the device as being ""useful for creative exploration"", ""its ability to sketch at actual scale helped judge fit"", and ""fun to use.""",0.0,"Physical sketching of 3D wireframe models, using a hand-held plastic extruder, allows users to explore the design space of 3D models efficiently. Unfortunately, the scale of these devices limits users' design explorations to small-scale objects. We present protopiper, a computer aided, hand-held fabrication device, that allows users to sketch room-sized objects at actual scale. The key idea behind protopiper is that it forms adhesive tape into tubes as its main building material, rather than extruded plastic or photopolymer lines. Since the resulting tubes are hollow they offer excellent strength-to-weight ratio, thus scale well to large structures. Since the tape is pre-coated with adhesive it allows connecting tubes quickly, unlike extruded plastic that would require heating and cooling in the kilowatt range. We demonstrate protopiper's use through several demo objects, ranging from more constructive objects, such as furniture, to more decorative objects, such as statues. In our exploratory user study, 16 participants created objects based on their own ideas. They rated the device as being ""useful for creative exploration"", ""its ability to sketch at actual scale helped judge fit"", and ""fun to use.""","['[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]', '[Hasso Plattner Institute, Potsdam, Germany]']","['2009751849', '2125802668', '2130928247', '2176225772', '2226584906', '2598011007', '267332255']",2180120384.0,"{'offset': 0, 'data': [{'authorId': '34748728', 'url': 'https://www.semanticscholar.org/author/34748728', 'name': 'Harshit Agrawal', 'affiliations': [], 'homepage': None, 'paperCount': 35, 'citationCount': 153}, {'authorId': '2754729', 'url': 'https://www.semanticscholar.org/author/2754729', 'name': 'Udayan Umapathi', 'affiliations': [], 'homepage': None, 'paperCount': 12, 'citationCount': 259}, {'authorId': '39998843', 'url': 'https://www.semanticscholar.org/author/39998843', 'name': 'Róbert Kovács', 'affiliations': [], 'homepage': None, 'paperCount': 37, 'citationCount': 851}, {'authorId': '1710690', 'url': 'https://www.semanticscholar.org/author/1710690', 'name': 'Johannes Frohnhofen', 'affiliations': [], 'homepage': None, 'paperCount': 10, 'citationCount': 397}, {'authorId': '3327016', 'url': 'https://www.semanticscholar.org/author/3327016', 'name': 'Hsiang-Ting Chen', 'affiliations': [], 'homepage': None, 'paperCount': 43, 'citationCount': 638}, {'authorId': '143637542', 'url': 'https://www.semanticscholar.org/author/143637542', 'name': 'Stefanie Müller', 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 919}, {'authorId': '1729393', 'url': 'https://www.semanticscholar.org/author/1729393', 'name': 'Patrick Baudisch', 'affiliations': [], 'homepage': None, 'paperCount': 220, 'citationCount': 12226}]}",53.0,"{'MAG': '2180120384', 'DBLP': 'conf/uist/AgrawalUKFCMB15', 'DOI': '10.1145/2807442.2807505', 'CorpusId': 804203}",['Computer Science'],2.0,False,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['Book', 'JournalArticle', 'Conference']",29.0,Protopiper: Physically Sketching Room-Sized Objects at Actual Scale,https://www.semanticscholar.org/paper/2376106eb10f26c0545c078d256d3921a32a080d,UIST,2015
2535447426,"Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.",1.0,"Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.","['Stanford Univ., Stanford, CA (USA)', 'Stanford Univ., Stanford, CA (USA)']","['2069682576', '2536959262']",2535447426.0,"{'offset': 0, 'data': [{'authorId': '3492590', 'url': 'https://www.semanticscholar.org/author/3492590', 'name': 'Evan Strasnick', 'affiliations': [], 'homepage': None, 'paperCount': 11, 'citationCount': 182}, {'authorId': '2770912', 'url': 'https://www.semanticscholar.org/author/2770912', 'name': 'S. Follmer', 'affiliations': [], 'homepage': None, 'paperCount': 122, 'citationCount': 3908}]}",10.0,"{'DBLP': 'conf/uist/StrasnickF16', 'MAG': '2535447426', 'DOI': '10.1145/2984751.2985728', 'CorpusId': 19699028}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",20.0,Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display,https://www.semanticscholar.org/paper/3c28c313d4eb76c7348ad5dbaebfc8600a55b2de,UIST,2016
2087634802,"Modern applications provide interfaces for scripting, but many users do not know how to write script commands. However, many users are familiar with the idea of entering keywords into a web search engine. Hence, if a user is familiar with the vocabulary of an application domain, we anticipate that they could write a set of keywords expressing a command in that domain. For instance, in the web browsing domain, a user might enter <B>click search button</B>. We call expressions of this form keyword commands, and we present a novel approach for translating keyword commands directly into executable code. Our prototype of this system in the web browsing domain translates <B>click search button</B> into the Chickenfoot code <B>click(findButton(""search""))</B>. This code is then executed in the context of a web browser to carry out the effect. We also present an implementation of this system in the domain of Microsoft Word. A user study revealed that subjects could use keyword commands to successfully complete 90% of the web browsing tasks in our study without instructions or training. Conversely, we would expect users to complete close to 0% of the tasks if they had to guess the underlying JavaScript commands with no instructions or training.",1.0,"Modern applications provide interfaces for scripting, but many users do not know how to write script commands. However, many users are familiar with the idea of entering keywords into a web search engine. Hence, if a user is familiar with the vocabulary of an application domain, we anticipate that they could write a set of keywords expressing a command in that domain. For instance, in the web browsing domain, a user might enter <B>click search button</B>. We call expressions of this form keyword commands, and we present a novel approach for translating keyword commands directly into executable code. Our prototype of this system in the web browsing domain translates <B>click search button</B> into the Chickenfoot code <B>click(findButton(""search""))</B>. This code is then executed in the context of a web browser to carry out the effect. We also present an implementation of this system in the domain of Microsoft Word. A user study revealed that subjects could use keyword commands to successfully complete 90% of the web browsing tasks in our study without instructions or training. Conversely, we would expect users to complete close to 0% of the tasks if they had to guess the underlying JavaScript commands with no instructions or training.","['MIT, CSAIL, Cambridge, MA', 'MIT, CSAIL, Cambridge, MA']","['2104582966', '2156768364']",2087634802.0,"{'offset': 0, 'data': [{'authorId': '48155668', 'url': 'https://www.semanticscholar.org/author/48155668', 'name': 'Greg Little', 'affiliations': [], 'homepage': None, 'paperCount': 37, 'citationCount': 3712}, {'authorId': '152160465', 'url': 'https://www.semanticscholar.org/author/152160465', 'name': 'Rob Miller', 'affiliations': [], 'homepage': None, 'paperCount': 120, 'citationCount': 9034}]}",81.0,"{'MAG': '2087634802', 'DBLP': 'conf/uist/LittleM06', 'DOI': '10.1145/1166253.1166275', 'CorpusId': 8952304}",['Computer Science'],2.0,True,{'pages': '135-144'},10/15/2006,"['JournalArticle', 'Conference']",37.0,Translating keyword commands into executable code,https://www.semanticscholar.org/paper/ce8ab580786769bcbafa7cd5750327c10e82d7b0,UIST,2006
2120093000,"Rapid, early, but rough system prototypes are becoming a standard and valued part of the user interface design process. Pen, paper, and tools like Flash™ and Director™ are well suited to creating such prototypes. However, in the case of physical forms with embedded technology, there is a lack of tools for developing rapid, early prototypes. Instead, the process tends to be fragmented into prototypes exploring forms that look like the intended product or explorations of functioning interactions that work like the intended product - bringing these aspects together into full design concepts only later in the design process. To help alleviate this problem, we present a simple tool for very rapidly creating functioning, rough physical prototypes early in the design process - supporting what amounts to interactive physical sketching. Our tool allows a designer to combine exploration of form and interactive function, using objects constructed from materials such as thumbtacks, foil, cardboard and masking tape, enhanced with a small electronic sensor board. By means of a simple and fluid tool for delivering events to ""screen clippings,"" these physical sketches can then be easily connected to any existing (or new) program running on a PC to provide real or Wizard of Oz supported functionality.",1.0,"Rapid, early, but rough system prototypes are becoming a standard and valued part of the user interface design process. Pen, paper, and tools like Flash™ and Director™ are well suited to creating such prototypes. However, in the case of physical forms with embedded technology, there is a lack of tools for developing rapid, early prototypes. Instead, the process tends to be fragmented into prototypes exploring forms that look like the intended product or explorations of functioning interactions that work like the intended product - bringing these aspects together into full design concepts only later in the design process. To help alleviate this problem, we present a simple tool for very rapidly creating functioning, rough physical prototypes early in the design process - supporting what amounts to interactive physical sketching. Our tool allows a designer to combine exploration of form and interactive function, using objects constructed from materials such as thumbtacks, foil, cardboard and masking tape, enhanced with a small electronic sensor board. By means of a simple and fluid tool for delivering events to ""screen clippings,"" these physical sketches can then be easily connected to any existing (or new) program running on a PC to provide real or Wizard of Oz supported functionality.","['Carnegie Mellon University, Pittsburgh, PA.', 'Carnegie Mellon University, Pittsburgh, PA.']","['1987134293', '2171298838']",2120093000.0,"{'offset': 0, 'data': [{'authorId': '1749296', 'url': 'https://www.semanticscholar.org/author/1749296', 'name': 'S. Hudson', 'affiliations': [], 'homepage': None, 'paperCount': 280, 'citationCount': 13652}, {'authorId': '3055754', 'url': 'https://www.semanticscholar.org/author/3055754', 'name': 'Jennifer Mankoff', 'affiliations': ['Make4all', 'CREATE'], 'homepage': 'make4all.org', 'paperCount': 183, 'citationCount': 8732}]}",93.0,"{'DBLP': 'conf/uist/HudsonM06', 'MAG': '2120093000', 'DOI': '10.1145/1166253.1166299', 'CorpusId': 16903001}",['Computer Science'],4.0,False,{'pages': '289-298'},10/15/2006,"['JournalArticle', 'Conference']",24.0,"Rapid construction of functioning physical interfaces from cardboard, thumbtacks, tin foil and masking tape",https://www.semanticscholar.org/paper/d78cd2423c500159bb158305e9a6a5fd5d6b643a,UIST,2006
2007685793,"Image triage is a common task in digital photography. Determining which photos are worth processing for sharing with friends and family and which should be deleted to make room for new ones can be a challenge, especially on a device with a small screen like a mobile phone or camera. In this work we explore the importance of local structure changes?e.g. human pose, appearance changes, object orientation, etc.?to the photographic triage task. We perform a user study in which subjects are asked to mark regions of image pairs most useful in making triage decisions. From this data, we train a model for image saliency in the context of other images that we call cosaliency. This allows us to create collection-aware crops that can augment the information provided by existing thumbnailing techniques for the image triage task.",1.0,"Image triage is a common task in digital photography. Determining which photos are worth processing for sharing with friends and family and which should be deleted to make room for new ones can be a challenge, especially on a device with a small screen like a mobile phone or camera. In this work we explore the importance of local structure changes?e.g. human pose, appearance changes, object orientation, etc.?to the photographic triage task. We perform a user study in which subjects are asked to mark regions of image pairs most useful in making triage decisions. From this data, we train a model for image saliency in the context of other images that we call cosaliency. This allows us to create collection-aware crops that can augment the information provided by existing thumbnailing techniques for the image triage task.","['Stanford Univ., Stanford, CA (USA)', 'Adobe Systems, Inc., Seattle, WA, USA#TAB#', 'Adobe Systems, Inc., Seattle, WA, USA#TAB#']","['2100341992', '2152336186', '269985891']",2007685793.0,"{'offset': 0, 'data': [{'authorId': '2059096618', 'url': 'https://www.semanticscholar.org/author/2059096618', 'name': 'David E. Jacobs', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 583}, {'authorId': '1976171', 'url': 'https://www.semanticscholar.org/author/1976171', 'name': 'Dan B. Goldman', 'affiliations': [], 'homepage': None, 'paperCount': 66, 'citationCount': 6459}, {'authorId': '2177801', 'url': 'https://www.semanticscholar.org/author/2177801', 'name': 'E. Shechtman', 'affiliations': [], 'homepage': None, 'paperCount': 153, 'citationCount': 22414}]}",88.0,"{'DBLP': 'conf/uist/JacobsGS10', 'MAG': '2007685793', 'DOI': '10.1145/1866029.1866066', 'CorpusId': 6626167}",['Computer Science'],3.0,False,{'name': 'Proceedings of the 23nd annual ACM symposium on User interface software and technology'},10/3/2010,"['Book', 'JournalArticle', 'Conference']",30.0,Cosaliency: where people look when comparing images,https://www.semanticscholar.org/paper/a15e8e3bcbe48b3a2d0ae269365195d46cba09ff,UIST,2010
1971706773,"Touch-screens are becoming increasingly ubiquitous. They have great appeal due to their capabilities to support new forms of human interaction, including their abilities to interpret rich gestural inputs, render flexible user interfaces and enable multi-user interactions. However, the technology creates new challenges and barriers for users with limited levels of vision and motor abilities. The PhD work described in this paper proposes a technique combining Shared User Models (SUM) and adaptive interfaces to improve the accessibility of touch-screen devices for people with low levels of vision and motor ability. SUM, built from an individual's interaction data across multiple applications and devices, is used to infer new knowledge of their abilities and characteristics, without the need for continuous calibration exercises or user configurations. This approach has been realized through the development of an open source software framework to support the creation of applications that make use of SUM to adapt interfaces that match the needs of individual users.",0.0,"Touch-screens are becoming increasingly ubiquitous. They have great appeal due to their capabilities to support new forms of human interaction, including their abilities to interpret rich gestural inputs, render flexible user interfaces and enable multi-user interactions. However, the technology creates new challenges and barriers for users with limited levels of vision and motor abilities. The PhD work described in this paper proposes a technique combining Shared User Models (SUM) and adaptive interfaces to improve the accessibility of touch-screen devices for people with low levels of vision and motor ability. SUM, built from an individual's interaction data across multiple applications and devices, is used to infer new knowledge of their abilities and characteristics, without the need for continuous calibration exercises or user configurations. This approach has been realized through the development of an open source software framework to support the creation of applications that make use of SUM to adapt interfaces that match the needs of individual users.","['\u2002University of Dundee, Dundee, UK']",['2681198741'],1971706773.0,"{'offset': 0, 'data': [{'authorId': '2414592', 'url': 'https://www.semanticscholar.org/author/2414592', 'name': 'Kyle Montague', 'affiliations': ['Northumbria University'], 'homepage': 'https://kylemontague.co.uk', 'paperCount': 74, 'citationCount': 770}]}",3.0,"{'MAG': '1971706773', 'DBLP': 'conf/uist/Montague12', 'DOI': '10.1145/2380296.2380315', 'CorpusId': 18585225}",['Computer Science'],0.0,False,{'pages': '39-42'},10/7/2012,['JournalArticle'],12.0,Interactions speak louder than words: shared user models and adaptive interfaces,https://www.semanticscholar.org/paper/681ee93e7c1f598149ff78a29e929d7460b7ae25,UIST,2012
2007948005,"We describe a system for associating the user interface entities of an application with their underlying semantic objects. The associations are classified by arranging the user interface entities in a type lattice in an object-oriented fashion. The interactive behavior of the application is described by defining application operations in terms of methods on the types in the type lattice. This scheme replaces the usual “active region” interaction model, and allows application interfaces to be specified directly in terms of the objects of the application itself. We discuss the benefits of this system and some of the difficulties we encountered.",1.0,"We describe a system for associating the user interface entities of an application with their underlying semantic objects. The associations are classified by arranging the user interface entities in a type lattice in an object-oriented fashion. The interactive behavior of the application is described by defining application operations in terms of methods on the types in the type lattice. This scheme replaces the usual “active region” interaction model, and allows application interfaces to be specified directly in terms of the objects of the application itself. We discuss the benefits of this system and some of the difficulties we encountered.","['Symbolics, Inc.', 'Symbolics, Inc.', 'International Lisp Associates']","['2309467452', '2497676152', '2664481123']",2007948005.0,"{'offset': 0, 'data': [{'authorId': '144182596', 'url': 'https://www.semanticscholar.org/author/144182596', 'name': 'S. McKay', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 35}, {'authorId': '28452086', 'url': 'https://www.semanticscholar.org/author/28452086', 'name': 'W. York', 'affiliations': [], 'homepage': None, 'paperCount': 5, 'citationCount': 503}, {'authorId': '144508120', 'url': 'https://www.semanticscholar.org/author/144508120', 'name': 'M. McMahon', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 23}]}",14.0,"{'DBLP': 'conf/uist/McKayYM89', 'MAG': '2007948005', 'DOI': '10.1145/73660.73678', 'CorpusId': 14154079}",['Computer Science'],0.0,False,{'pages': '141-148'},11/13/1989,['JournalArticle'],22.0,A presentation manager based on application semantics,https://www.semanticscholar.org/paper/8c3ce606d9afa95afdaf7a001383ca1766913492,UIST,1989
2041311157,"Conventional toolkits today require the programmer to attach call-back procedures to most buttons, scroll bars, menu items, and other widgets in the interface. These procedures are called by the system when the user operates the widget in order to notify the application of the user’s actions. Unfortunately, real interfaces contain hundreds or thousands of widgets, and therefore many call-back procedures, most of which perform trivial tasks, resulting in a maintenance nightmare. This paper describes a system that allows the majority of these procedures to be eliminated. The user interface designer can specify by demonstration many of the desired actions and connections among the widgets, so call-backs are only needed for the most significant application actions. In addition, the callbacks that remain are completely insulated from the widgets, so that the application code is better separated from the user interface.",1.0,"Conventional toolkits today require the programmer to attach call-back procedures to most buttons, scroll bars, menu items, and other widgets in the interface. These procedures are called by the system when the user operates the widget in order to notify the application of the user’s actions. Unfortunately, real interfaces contain hundreds or thousands of widgets, and therefore many call-back procedures, most of which perform trivial tasks, resulting in a maintenance nightmare. This paper describes a system that allows the majority of these procedures to be eliminated. The user interface designer can specify by demonstration many of the desired actions and connections among the widgets, so call-backs are only needed for the most significant application actions. In addition, the callbacks that remain are completely insulated from the widgets, so that the application code is better separated from the user interface.","['School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA']",['2117127927'],2041311157.0,"{'offset': 0, 'data': [{'authorId': '1707801', 'url': 'https://www.semanticscholar.org/author/1707801', 'name': 'B. Myers', 'affiliations': [], 'homepage': None, 'paperCount': 498, 'citationCount': 26319}]}",166.0,"{'DBLP': 'conf/uist/Myers91', 'MAG': '2041311157', 'DOI': '10.1145/120782.120805', 'CorpusId': 2983798}",['Computer Science'],8.0,False,{'pages': '211-220'},11/11/1991,['JournalArticle'],34.0,Separating application code from toolkits: eliminating the spaghetti of call-backs,https://www.semanticscholar.org/paper/8bdcad9bcc5b4dcd0511b86fec6e9557809a4692,UIST,1991
2111200034,"We investigate crowd-powered interfaces: interfaces that embed human activity to support high-level conceptual activities such as writing, editing and question-answering. For example, a crowd-ppowered interface using paid crowd workers can compute a series of textual cuts and edits to a paragraph, then provide the user with an interface to condense his or her writing. We map out the design space of interfaces that depend on outsourced, friendsourced, and data mined resources, and report on designs for each of these. We discuss technical and motivational challenges inherent in human-powered interfaces.",0.0,"We investigate crowd-powered interfaces: interfaces that embed human activity to support high-level conceptual activities such as writing, editing and question-answering. For example, a crowd-ppowered interface using paid crowd workers can compute a series of textual cuts and edits to a paragraph, then provide the user with an interface to condense his or her writing. We map out the design space of interfaces that depend on outsourced, friendsourced, and data mined resources, and report on designs for each of these. We discuss technical and motivational challenges inherent in human-powered interfaces.","['Massachusettes Institute of Texhnology, Cambridge, MA, USA']",['1974803209'],2111200034.0,"{'offset': 0, 'data': [{'authorId': '145879842', 'url': 'https://www.semanticscholar.org/author/145879842', 'name': 'Michael S. Bernstein', 'affiliations': ['Stanford University'], 'homepage': 'https://hci.stanford.edu/msb/', 'paperCount': 187, 'citationCount': 41173}]}",8.0,"{'DBLP': 'conf/uist/Bernstein10', 'MAG': '2111200034', 'DOI': '10.1145/1866218.1866220', 'CorpusId': 15910582}",['Computer Science'],0.0,False,{'pages': '347-350'},10/3/2010,['JournalArticle'],9.0,Crowd-powered interfaces,https://www.semanticscholar.org/paper/1bd0e1086694f0b8fc144b6ab5b3123fff565779,UIST,2010
1973595223,"We describe COTERIE, a toolkit that provides languagelevel support for building distributed virtual environments. COTERIE is based on the distributed data-object paradigm for distributed shared memory. Any data object in COTERIE can be declared to be a Shared Object that is replicated fully in any process that is interested in it. These Shared Objects support asynchronous data propagation with atomic serializable updates, and asynchronous notification of updates. COTERIE is built in Modula-3 and uses existing Modula-3 packages that support an integrated interpreted language, multithreading, and 3D animation. Unlike other VE toolkits, COTERIE is based on a set of general-purpose parallel and distributed language concepts designed with the needs of virtual environments in mind. We summarize the requirements that we identified for COTERIE, describe its implementation, compare it with other toolkits, and provide examples that show COTERIE’s advantages.",1.0,"We describe COTERIE, a toolkit that provides languagelevel support for building distributed virtual environments. COTERIE is based on the distributed data-object paradigm for distributed shared memory. Any data object in COTERIE can be declared to be a Shared Object that is replicated fully in any process that is interested in it. These Shared Objects support asynchronous data propagation with atomic serializable updates, and asynchronous notification of updates. COTERIE is built in Modula-3 and uses existing Modula-3 packages that support an integrated interpreted language, multithreading, and 3D animation. Unlike other VE toolkits, COTERIE is based on a set of general-purpose parallel and distributed language concepts designed with the needs of virtual environments in mind. We summarize the requirements that we identified for COTERIE, describe its implementation, compare it with other toolkits, and provide examples that show COTERIE’s advantages.","['Department of Computer Science, Columbia University New York, NY#TAB#', 'Department of Computer Science, Columbia University New York, NY#TAB#']","['2108490771', '252695556']",1973595223.0,"{'offset': 0, 'data': [{'authorId': '145931052', 'url': 'https://www.semanticscholar.org/author/145931052', 'name': 'B. MacIntyre', 'affiliations': [], 'homepage': None, 'paperCount': 151, 'citationCount': 11664}, {'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}]}",66.0,"{'DBLP': 'conf/uist/MacIntyreF96', 'MAG': '1973595223', 'DOI': '10.1145/237091.237104', 'CorpusId': 933877}",['Computer Science'],2.0,True,{'pages': '83-94'},11/1/1996,['JournalArticle'],61.0,Language-level support for exploratory programming of distributed virtual environments,https://www.semanticscholar.org/paper/cc6a82c166bf7098eb7fc9152c4f485065c34582,UIST,1996
2010757597,"INTRODUCTION interfacing concerns: As the computing environment’s lowest common denominator, the operating system must always play a game of catchup to provide system-wide support for the changing demands and opportunities of its user environment. Never has this been more true than now. Dramatic changes in the economics of computing now make workstation power accessible to tens of millions of ordinary users. Gone are the days of single-purpose computing (e.g., computer as spreadsheet, computer as word processor). The increasing diversity and sophistication of applications software now make interoperability an imperative. l real-time performance. In the stand-alone, single application environment of a personal computer, users have come to expect immediate feedback. How can we preserve this responsiveness in a computing environment that also supports multi-tasking, networking, and distributed processing? Can the operating system help to promote more realistic models of computer response?",1.0,"INTRODUCTION interfacing concerns: As the computing environment’s lowest common denominator, the operating system must always play a game of catchup to provide system-wide support for the changing demands and opportunities of its user environment. Never has this been more true than now. Dramatic changes in the economics of computing now make workstation power accessible to tens of millions of ordinary users. Gone are the days of single-purpose computing (e.g., computer as spreadsheet, computer as word processor). The increasing diversity and sophistication of applications software now make interoperability an imperative. l real-time performance. In the stand-alone, single application environment of a personal computer, users have come to expect immediate feedback. How can we preserve this responsiveness in a computing environment that also supports multi-tasking, networking, and distributed processing? Can the operating system help to promote more realistic models of computer response?","['International Business Machines, Austin#TAB#', 'Hewlett-Packard, Bristol Research Laboratory#TAB#', 'Xerox, Parc', 'Arthur D. Little Enterprises, Acorn Park, Cambridge, MA#TAB#']","['2131569541', '2155160379', '2172094290', '2223144080']",2010757597.0,"{'offset': 0, 'data': [{'authorId': '118049359', 'url': 'https://www.semanticscholar.org/author/118049359', 'name': 'W. Jones', 'affiliations': [], 'homepage': None, 'paperCount': 14, 'citationCount': 982}, {'authorId': '144034780', 'url': 'https://www.semanticscholar.org/author/144034780', 'name': 'P. Williams', 'affiliations': [], 'homepage': None, 'paperCount': 399, 'citationCount': 6769}, {'authorId': '1699184', 'url': 'https://www.semanticscholar.org/author/1699184', 'name': 'G. Robertson', 'affiliations': [], 'homepage': None, 'paperCount': 126, 'citationCount': 14059}, {'authorId': '143852981', 'url': 'https://www.semanticscholar.org/author/143852981', 'name': 'Mike Conner', 'affiliations': [], 'homepage': None, 'paperCount': 9, 'citationCount': 75}]}",2.0,"{'MAG': '2010757597', 'DBLP': 'conf/uist/JonesWRC90', 'DOI': '10.1145/97924.97928', 'CorpusId': 2158000}",['Computer Science'],0.0,False,{'pages': '31-35'},8/1/1990,['JournalArticle'],0.0,In search for ideal operating system for user interfacing,https://www.semanticscholar.org/paper/835868af0c70a4347ddf682c9c7cf96d4550685e,UIST,1990
2011824517,"End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond naïve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.",0.0,"End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond naïve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.","[' Univ. of Washington, Seattle, WA, USA']",['2073051687'],2011824517.0,"{'offset': 0, 'data': [{'authorId': '1719124', 'url': 'https://www.semanticscholar.org/author/1719124', 'name': 'Saleema Amershi', 'affiliations': [], 'homepage': None, 'paperCount': 58, 'citationCount': 3810}]}",37.0,"{'MAG': '2011824517', 'DBLP': 'conf/uist/Amershi11', 'DOI': '10.1145/2046396.2046416', 'CorpusId': 11425371}",['Computer Science'],3.0,True,{'pages': '47-50'},10/16/2011,['JournalArticle'],259.0,Designing for effective end-user interaction with machine learning,https://www.semanticscholar.org/paper/e30a5ce358d6285b592c8000769802392e8128e0,UIST,2011
2026682061,"The conference this year is the tenth anniversary of UIST. The keynote talk discusses the history of UIST over the last ten years; this panel looks into the future of the field over the next ten. Each of the panelists will describe a scenario for what life will be like when we meet for UIST’O’I, ten years from now. They will also have a chance to challenge or question each others’ scenarios and to participate in open discussion with the audience.",0.0,"The conference this year is the tenth anniversary of UIST. The keynote talk discusses the history of UIST over the last ten years; this panel looks into the future of the field over the next ten. Each of the panelists will describe a scenario for what life will be like when we meet for UIST’O’I, ten years from now. They will also have a chance to challenge or question each others’ scenarios and to participate in open discussion with the audience.","['Xerox, Parc', 'Carnegie Mellon University', 'Mitsubishi Electric Research Laboratories,#TAB#', 'Columbia University ', 'Department of Electrical Engineering and Computer Science, Tufts University, Medford, Mass.']","['2044770413', '2148302434', '2193907057', '252695556', '673017202']",2026682061.0,"{'offset': 0, 'data': [{'authorId': '1723792', 'url': 'https://www.semanticscholar.org/author/1723792', 'name': 'R. Jacob', 'affiliations': [], 'homepage': None, 'paperCount': 197, 'citationCount': 11361}, {'authorId': '1809403', 'url': 'https://www.semanticscholar.org/author/1809403', 'name': 'Steven K. Feiner', 'affiliations': [], 'homepage': None, 'paperCount': 361, 'citationCount': 20836}, {'authorId': '144973157', 'url': 'https://www.semanticscholar.org/author/144973157', 'name': 'J. Foley', 'affiliations': [], 'homepage': None, 'paperCount': 157, 'citationCount': 7466}, {'authorId': '1777393', 'url': 'https://www.semanticscholar.org/author/1777393', 'name': 'J. Mackinlay', 'affiliations': [], 'homepage': None, 'paperCount': 174, 'citationCount': 17359}, {'authorId': '2067898679', 'url': 'https://www.semanticscholar.org/author/2067898679', 'name': 'Dan R. Olsen', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 11}]}",6.0,"{'MAG': '2026682061', 'DBLP': 'conf/uist/JacobFFMO97', 'DOI': '10.1145/263407.263527', 'CorpusId': 1289157}",['Computer Science'],0.0,False,{'pages': '115-118'},10/1/1997,['JournalArticle'],0.0,UIST'007 (panel): where will we be ten years from now?,https://www.semanticscholar.org/paper/42e4e1fca978ed983694978da3e154abca1ef7dd,UIST,1997
2028610730,"Synchronous multi-user applications are designed to support two or more simultaneous users. The RENDEZVOUSTM1 system is an infrastructure for building such multi-user applications. Several multi-user applications, such as a tic-tac-toe game. a multi-user CardTable application, and a multi-user whiteboard have been or are being constructed with the RENDEZVOUS system. We argue that there are at least three dimensions of programming complexity that are differentially affected by the programming of multi-user applications as compared to the programming of single-user applications. The first, concurrency, addresses the need to cope with parallel activities. The second dimension, abstraction, addresses the need to separate the user-int~rface from an underlying application abstraction, The thmd dimension, roles, addresses the need to differentially characterize users and customize the user-interface appropriately. Certainly, single-user applications often deal with these complexities; we argue that multi-user applications cannot avoid them. [Keyworcls: User Interface Management System, Computer-Supported Cooperative Work, groupware, programming, dialogue separation, concurrency, roles]",1.0,"Synchronous multi-user applications are designed to support two or more simultaneous users. The RENDEZVOUSTM1 system is an infrastructure for building such multi-user applications. Several multi-user applications, such as a tic-tac-toe game. a multi-user CardTable application, and a multi-user whiteboard have been or are being constructed with the RENDEZVOUS system. We argue that there are at least three dimensions of programming complexity that are differentially affected by the programming of multi-user applications as compared to the programming of single-user applications. The first, concurrency, addresses the need to cope with parallel activities. The second dimension, abstraction, addresses the need to separate the user-int~rface from an underlying application abstraction, The thmd dimension, roles, addresses the need to differentially characterize users and customize the user-interface appropriately. Certainly, single-user applications often deal with these complexities; we argue that multi-user applications cannot avoid them. [Keyworcls: User Interface Management System, Computer-Supported Cooperative Work, groupware, programming, dialogue separation, concurrency, roles]","['Bellcore, 445 South St. Morristown, NJ']",['2653817259'],2028610730.0,"{'offset': 0, 'data': [{'authorId': '145543692', 'url': 'https://www.semanticscholar.org/author/145543692', 'name': 'J. Patterson', 'affiliations': [], 'homepage': None, 'paperCount': 57, 'citationCount': 2016}]}",65.0,"{'MAG': '2028610730', 'DBLP': 'conf/uist/Patterson91', 'DOI': '10.1145/120782.120792', 'CorpusId': 6461919}",['Computer Science'],5.0,False,{'pages': '87-94'},11/11/1991,['JournalArticle'],18.0,Comparing the programming demands of single-user and multi-user applications,https://www.semanticscholar.org/paper/9f476813db335534a92a0c3ab1a7390975d71e58,UIST,1991
2048971063,"When teaching programming or hardware design, it is pedagogically valuable for students to generate examples of functions, circuits, or system designs. Teachers can be overwhelmed by these types of student submissions when running large residential or recently released massive online courses. The underlying distribution of student solutions submitted in response to a particular assignment may be complex, but the newly available volume of student solutions represents a denser sampling of that distribution. Working with large datasets of students' solutions, I am building systems with user interfaces that allow teachers to explore the variety of their students' correct and incorrect solutions. Forum posts, grading rubrics, and automatic graders can be based on student solution data, and turn massive engineering and computer science classrooms into useful insight and feedback for teachers. In the development process, I hope to describe essential design principles for such systems.",0.0,"When teaching programming or hardware design, it is pedagogically valuable for students to generate examples of functions, circuits, or system designs. Teachers can be overwhelmed by these types of student submissions when running large residential or recently released massive online courses. The underlying distribution of student solutions submitted in response to a particular assignment may be complex, but the newly available volume of student solutions represents a denser sampling of that distribution. Working with large datasets of students' solutions, I am building systems with user interfaces that allow teachers to explore the variety of their students' correct and incorrect solutions. Forum posts, grading rubrics, and automatic graders can be based on student solution data, and turn massive engineering and computer science classrooms into useful insight and feedback for teachers. In the development process, I hope to describe essential design principles for such systems.","['Massachusetts Institute of Technology Cambridge, MA USA']",['2138841778'],2048971063.0,"{'offset': 0, 'data': [{'authorId': '143730651', 'url': 'https://www.semanticscholar.org/author/143730651', 'name': 'Elena L. Glassman', 'affiliations': ['Harvard University, Radcliffe Institute for Advanced Study'], 'homepage': 'http://glassmanlab.seas.harvard.edu/', 'paperCount': 53, 'citationCount': 845}]}",2.0,"{'DBLP': 'conf/uist/Glassman14', 'MAG': '2048971063', 'DOI': '10.1145/2658779.2661167', 'CorpusId': 18170547}",['Computer Science'],0.0,False,{'pages': '17-20'},10/5/2014,['JournalArticle'],13.0,Interacting with massive numbers of student solutions,https://www.semanticscholar.org/paper/a15f78310f71e5706b34bcce213c1e650bb7bb01,UIST,2014
2064528063,"The VIMSYS project provides environmental scientists with the ability to perform content-based querying over a database of satellite images. This paper descriks the enduser query interface which facilitates identification of multiple object types, reduces emphasis on numerical data, and simplifies the use of numerous sets of parameters. To address these issues in a way that can be applied to similar databases, the end-user query interface utilizes abstract query structures called clusters, in addition to frames and links. We describe the requirements of the system, review commonly available query methods, discuss how the VIMSYS interface meets the needs of the audience, present user reaction to the prototype, and summarize other relevant details of VIMSYS.",0.0,"The VIMSYS project provides environmental scientists with the ability to perform content-based querying over a database of satellite images. This paper descriks the enduser query interface which facilitates identification of multiple object types, reduces emphasis on numerical data, and simplifies the use of numerous sets of parameters. To address these issues in a way that can be applied to similar databases, the end-user query interface utilizes abstract query structures called clusters, in addition to frames and links. We describe the requirements of the system, review commonly available query methods, discuss how the VIMSYS interface meets the needs of the audience, present user reaction to the prototype, and summarize other relevant details of VIMSYS.","['The University of Michigan, Advanced Technology Laboratory, 1101 Beal Avenue, Ann Arbor, Michigan#TAB#', 'The University of Michigan, Advanced Technology Laboratory, 1101 Beal Avenue, Ann Arbor, Michigan#TAB#', 'The University of Michigan, Advanced Technology Laboratory, 1101 Beal Avenue, Ann Arbor, Michigan#TAB#']","['2133201272', '2229192503', '2303962706']",2064528063.0,"{'offset': 0, 'data': [{'authorId': '143955014', 'url': 'https://www.semanticscholar.org/author/143955014', 'name': 'Jill Kliger', 'affiliations': [], 'homepage': None, 'paperCount': 3, 'citationCount': 12}, {'authorId': '145814117', 'url': 'https://www.semanticscholar.org/author/145814117', 'name': 'D. Swanberg', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 330}, {'authorId': '144938732', 'url': 'https://www.semanticscholar.org/author/144938732', 'name': 'R. Jain', 'affiliations': [], 'homepage': None, 'paperCount': 172, 'citationCount': 11180}]}",4.0,"{'MAG': '2064528063', 'DBLP': 'conf/uist/KligerSJ93', 'DOI': '10.1145/168642.168644', 'CorpusId': 5919424}",['Computer Science'],0.0,False,{'pages': '11-21'},12/1/1993,"['JournalArticle', 'Review']",17.0,Concept clustering ina query interface to an image database,https://www.semanticscholar.org/paper/d6a6e112f97ad2ec70aa6d33ac2db42bf70f2b7e,UIST,1993
2078129788,"Languages based on the event model are widely regarded as expressive and flexible notations for the specification of interactive graphical user interfaces. However, until now, they have only been used to specify and implement the dialogue control component of user interfaces.This paper presents an extension of the event model. A computable notation, the event language, based on this is used to construct a complete user interface framework. The framework forms the runtime component of a UIMS.The event language allows the modular construction of complex event systems. This is supported by the addition of a tagged addressing mode. Furthermore, the control structure of event handlers is extended with exception management, permitting unspecified events and thereby facilitating the use of predefined building blocks.A general purpose run-time framework for user interfaces has been constructed using the event language. We present the architecture of the presentation component of this framework including the window manager and the I/O model.",1.0,"Languages based on the event model are widely regarded as expressive and flexible notations for the specification of interactive graphical user interfaces. However, until now, they have only been used to specify and implement the dialogue control component of user interfaces.This paper presents an extension of the event model. A computable notation, the event language, based on this is used to construct a complete user interface framework. The framework forms the runtime component of a UIMS.The event language allows the modular construction of complex event systems. This is supported by the addition of a tagged addressing mode. Furthermore, the control structure of event handlers is extended with exception management, permitting unspecified events and thereby facilitating the use of predefined building blocks.A general purpose run-time framework for user interfaces has been constructed using the event language. We present the architecture of the presentation component of this framework including the window manager and the I/O model.","['Department of Graphical Communication, Technical Uniwzrsii of Denmark, Building 116, 2800 Lyngby, Denmark', 'Department of Graphical Communication, Technical Uniwzrsii of Denmark, Building 116, 2800 Lyngby, Denmark', 'Department of Graphical Communication, Technical Uniwzrsii of Denmark, Building 116, 2800 Lyngby, Denmark']","['2251765442', '2713696980', '2973960103']",2078129788.0,"{'offset': 0, 'data': [{'authorId': '2564559', 'url': 'https://www.semanticscholar.org/author/2564559', 'name': 'Niels Vejrup Carlsen', 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 19}, {'authorId': '153637517', 'url': 'https://www.semanticscholar.org/author/153637517', 'name': 'N. Christensen', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 29}, {'authorId': '2085468572', 'url': 'https://www.semanticscholar.org/author/2085468572', 'name': 'H. A. Tucker', 'affiliations': [], 'homepage': None, 'paperCount': 4, 'citationCount': 12}]}",8.0,"{'DBLP': 'conf/uist/CarlsenCT89', 'MAG': '2078129788', 'DOI': '10.1145/73660.73677', 'CorpusId': 18206205}",['Computer Science'],0.0,False,{'pages': '133-139'},11/13/1989,['JournalArticle'],24.0,An event language for building user interface frameworks,https://www.semanticscholar.org/paper/7a3c760b0283a1b2794691dc48a1e28367b1ebec,UIST,1989
2096269523,"Soap is a pointing device based on hardware found in a mouse, yet works in mid-air. Soap consists of an optical sensor device moving freely inside a hull made of fabric. As the user applies pressure from the outside, the optical sensor moves independent from the hull. The optical sensor perceives this relative motion and reports it as position input. Soap offers many of the benefits of optical mice, such as high-accuracy sensing. We describe the design of a soap prototype and report our experiences with four application scenarios, including a wall display, Windows Media Center, slide presentation, and interactive video games.",1.0,"Soap is a pointing device based on hardware found in a mouse, yet works in mid-air. Soap consists of an optical sensor device moving freely inside a hull made of fabric. As the user applies pressure from the outside, the optical sensor moves independent from the hull. The optical sensor perceives this relative motion and reports it as position input. Soap offers many of the benefits of optical mice, such as high-accuracy sensing. We describe the design of a soap prototype and report our experiences with four application scenarios, including a wall display, Windows Media Center, slide presentation, and interactive video games.","['[Microsoft research, Redmond, WA]', '[Microsoft research, Redmond, WA]', '[Microsoft research, Redmond, WA]']","['2009751849', '2105571773', '2154671057']",2096269523.0,"{'offset': 0, 'data': [{'authorId': '1729393', 'url': 'https://www.semanticscholar.org/author/1729393', 'name': 'Patrick Baudisch', 'affiliations': [], 'homepage': None, 'paperCount': 220, 'citationCount': 12226}, {'authorId': '39453376', 'url': 'https://www.semanticscholar.org/author/39453376', 'name': 'M. Sinclair', 'affiliations': [], 'homepage': None, 'paperCount': 61, 'citationCount': 2643}, {'authorId': '145771244', 'url': 'https://www.semanticscholar.org/author/145771244', 'name': 'Andrew D. Wilson', 'affiliations': [], 'homepage': None, 'paperCount': 123, 'citationCount': 12219}]}",59.0,"{'DBLP': 'conf/uist/BaudischSW06', 'MAG': '2096269523', 'DOI': '10.1145/1166253.1166261', 'CorpusId': 13087513}",['Computer Science'],2.0,False,{'pages': '43-46'},10/15/2006,"['JournalArticle', 'Conference']",18.0,Soap: a pointing device that works in mid-air,https://www.semanticscholar.org/paper/18d44b5245893d631b648b4ffb1431e67f710420,UIST,2006
2136116262,"Large online courses often assign problems that are easy to grade because they have a fixed set of solutions (such as multiple choice), but grading and guiding students is more difficult in problem domains that have an unbounded number of correct answers. One such domain is derivations: sequences of logical steps commonly used in assignments for technical, mathematical and scientific subjects. We present DeduceIt, a system for creating, grading, and analyzing derivation assignments in any formal domain. DeduceIt supports assignments in any logical formalism, provides students with incremental feedback, and aggregates student paths through each proof to produce instructor analytics. DeduceIt benefits from checking thousands of derivations on the web: it introduces a proof cache, a novel data structure which leverages a crowd of students to decrease the cost of checking derivations and providing real-time, constructive feedback. We evaluate DeduceIt with 990 students in an online compilers course, finding students take advantage of its incremental feedback and instructors benefit from its structured insights into course topics. Our work suggests that automated reasoning can extend online assignments and large-scale education to many new domains.",0.0,"Large online courses often assign problems that are easy to grade because they have a fixed set of solutions (such as multiple choice), but grading and guiding students is more difficult in problem domains that have an unbounded number of correct answers. One such domain is derivations: sequences of logical steps commonly used in assignments for technical, mathematical and scientific subjects. We present DeduceIt, a system for creating, grading, and analyzing derivation assignments in any formal domain. DeduceIt supports assignments in any logical formalism, provides students with incremental feedback, and aggregates student paths through each proof to produce instructor analytics. DeduceIt benefits from checking thousands of derivations on the web: it introduces a proof cache, a novel data structure which leverages a crowd of students to decrease the cost of checking derivations and providing real-time, constructive feedback. We evaluate DeduceIt with 990 students in an online compilers course, finding students take advantage of its incremental feedback and instructors benefit from its structured insights into course topics. Our work suggests that automated reasoning can extend online assignments and large-scale education to many new domains.","['Stanford University, Palo Alto, USA', 'Stanford University, Palo Alto, USA', 'Stanford University, Palo Alto, USA', 'Stanford University, Palo Alto, USA', 'Stanford University, Palo Alto, USA', 'Kestrel Institute, Palo Alto, USA']","['1974803209', '2020052036', '2067453598', '2167404190', '2228006729', '2443945752']",2136116262.0,"{'offset': 0, 'data': [{'authorId': '2660071', 'url': 'https://www.semanticscholar.org/author/2660071', 'name': 'Ethan Fast', 'affiliations': [], 'homepage': None, 'paperCount': 25, 'citationCount': 1056}, {'authorId': '2109478874', 'url': 'https://www.semanticscholar.org/author/2109478874', 'name': 'Colleen Lee', 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 18}, {'authorId': '144653825', 'url': 'https://www.semanticscholar.org/author/144653825', 'name': 'A. Aiken', 'affiliations': [], 'homepage': None, 'paperCount': 263, 'citationCount': 18091}, {'authorId': '145879842', 'url': 'https://www.semanticscholar.org/author/145879842', 'name': 'Michael S. Bernstein', 'affiliations': ['Stanford University'], 'homepage': 'https://hci.stanford.edu/msb/', 'paperCount': 187, 'citationCount': 41173}, {'authorId': '1736370', 'url': 'https://www.semanticscholar.org/author/1736370', 'name': 'D. Koller', 'affiliations': [], 'homepage': None, 'paperCount': 344, 'citationCount': 76226}, {'authorId': '2110593977', 'url': 'https://www.semanticscholar.org/author/2110593977', 'name': 'Eric Smith', 'affiliations': [], 'homepage': None, 'paperCount': 1, 'citationCount': 18}]}",18.0,"{'DBLP': 'conf/uist/FastLABKS13', 'MAG': '2136116262', 'DOI': '10.1145/2501988.2502028', 'CorpusId': 1005363}",['Computer Science'],1.0,False,{'name': 'Proceedings of the 26th annual ACM symposium on User interface software and technology'},10/8/2013,"['Book', 'JournalArticle', 'Conference']",41.0,Crowd-scale interactive formal reasoning and analytics,https://www.semanticscholar.org/paper/b794942c9916bc463cfae4b63c57c21e2d53c77a,UIST,2013
2267998376,"We introduce a technique for furbricating 3D printed hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles. We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands. We provide several examples of output, demonstrating the immediate feasibility of our approach using a low cost, commodity printer. Overall, this technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware.",1.0,"We introduce a technique for furbricating 3D printed hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles. We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands. We provide several examples of output, demonstrating the immediate feasibility of our approach using a low cost, commodity printer. Overall, this technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware.","[' Carnegie Mellon University Pittsburgh PA USA', ' Carnegie Mellon University Pittsburgh PA USA', ' Carnegie Mellon University Pittsburgh PA USA']","['2108826813', '2123491528', '768992449']",2267998376.0,"{'offset': 0, 'data': [{'authorId': '1727999', 'url': 'https://www.semanticscholar.org/author/1727999', 'name': 'Gierad Laput', 'affiliations': [], 'homepage': None, 'paperCount': 55, 'citationCount': 1936}, {'authorId': '2028468', 'url': 'https://www.semanticscholar.org/author/2028468', 'name': ""Xiang 'Anthony' Chen"", 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 1594}, {'authorId': '145078227', 'url': 'https://www.semanticscholar.org/author/145078227', 'name': 'Chris Harrison', 'affiliations': [], 'homepage': None, 'paperCount': 154, 'citationCount': 8137}]}",54.0,"{'MAG': '2267998376', 'DBLP': 'conf/uist/LaputCH15', 'DOI': '10.1145/2807442.2807484', 'CorpusId': 1027309}",['Computer Science'],5.0,False,{'name': 'Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology'},11/5/2015,"['Book', 'JournalArticle', 'Conference']",23.0,"3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers, and Bristles",https://www.semanticscholar.org/paper/47d9f2de92068771399c212d81d3f0f891f66852,UIST,2015
2533771361,"Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.",1.0,"Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.","['University of Illinois at Urbana Champaign , Champaign , IL , USA ', 'University of Illinois at Urbana Champaign , Champaign , IL , USA ', 'University of Illinois at Urbana Champaign , Champaign , IL , USA ']","['2175921574', '2226499346', '2228325511']",2533771361.0,"{'offset': 0, 'data': [{'authorId': '143685671', 'url': 'https://www.semanticscholar.org/author/143685671', 'name': 'Biplab Deka', 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 392}, {'authorId': '2109596193', 'url': 'https://www.semanticscholar.org/author/2109596193', 'name': 'Zifeng Huang', 'affiliations': [], 'homepage': None, 'paperCount': 6, 'citationCount': 309}, {'authorId': '1560581935', 'url': 'https://www.semanticscholar.org/author/1560581935', 'name': 'Ranjitha Kumar', 'affiliations': [], 'homepage': None, 'paperCount': 43, 'citationCount': 1267}]}",68.0,"{'DBLP': 'conf/uist/DekaHK16', 'MAG': '2533771361', 'DOI': '10.1145/2984511.2984581', 'CorpusId': 15158982}",['Computer Science'],3.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",36.0,ERICA: Interaction Mining Mobile Apps,https://www.semanticscholar.org/paper/78f1084aa75b00deb3357e5b73e21bb9c3750fe7,UIST,2016
2535364608,"Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from automated task service, ""If This Then That (IFTTT)"", and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-applications. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.",0.0,"Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from automated task service, ""If This Then That (IFTTT)"", and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-applications. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.","['SungKyunKwan University, Seoul, Republic of Korea;', 'SungKyunKwan University, Seoul, Republic of Korea;']","['2492660757', '2518752497']",2535364608.0,"{'offset': 0, 'data': [{'authorId': '49685517', 'url': 'https://www.semanticscholar.org/author/49685517', 'name': 'Jihye Lee', 'affiliations': [], 'homepage': None, 'paperCount': 126, 'citationCount': 210}, {'authorId': '2108243215', 'url': 'https://www.semanticscholar.org/author/2108243215', 'name': 'Sangwon Lee', 'affiliations': [], 'homepage': None, 'paperCount': 54, 'citationCount': 569}]}",0.0,"{'MAG': '2535364608', 'DBLP': 'conf/uist/LeeL16a', 'DOI': '10.1145/2984751.2984777', 'CorpusId': 11823948}",['Computer Science'],0.0,False,{'name': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology'},10/16/2016,"['JournalArticle', 'Book', 'Conference']",9.0,Analysis of Sequential Tasks in Use Context of Mobile Apps,https://www.semanticscholar.org/paper/8a2e4e616709999c9ebe3822cd82becb43f407c5,UIST,2016
2897459493,"Momentary switches are important building blocks to prototype novel physical user interfaces and enable tactile, explicit and eyes-free interactions. Unfortunately, typical representatives, such as push-buttons or pre-manufactured membrane switches, often do not fulfill individual design requirements and lack customization options for rapid prototyping. With this work, we present Pushables, a DIY fabrication approach for producing thin, bendable and highly customizable membrane dome switches. Therefore, we contribute a three-stage fabrication pipeline that describes the production and assembly on the basis of prototyping methods with different skill levels making our approach suitable for technology-enthusiastic makers, researchers, fab labs and others who require custom membrane switches in small quantities. To demonstrate the wide applicability of Pushables, we present application examples from ubiquitous, mobile and wearable computing.",0.0,"Momentary switches are important building blocks to prototype novel physical user interfaces and enable tactile, explicit and eyes-free interactions. Unfortunately, typical representatives, such as push-buttons or pre-manufactured membrane switches, often do not fulfill individual design requirements and lack customization options for rapid prototyping. With this work, we present Pushables, a DIY fabrication approach for producing thin, bendable and highly customizable membrane dome switches. Therefore, we contribute a three-stage fabrication pipeline that describes the production and assembly on the basis of prototyping methods with different skill levels making our approach suitable for technology-enthusiastic makers, researchers, fab labs and others who require custom membrane switches in small quantities. To demonstrate the wide applicability of Pushables, we present application examples from ubiquitous, mobile and wearable computing.","['Technische Universität Dresden, Dresden,#N#Germany', 'Technische Universität Dresden, Dresden,#N#Germany']","['123419233', '1995914736']",2897459493.0,"{'offset': 0, 'data': [{'authorId': '2126815', 'url': 'https://www.semanticscholar.org/author/2126815', 'name': 'Konstantin Klamka', 'affiliations': [], 'homepage': None, 'paperCount': 24, 'citationCount': 285}, {'authorId': '1724208', 'url': 'https://www.semanticscholar.org/author/1724208', 'name': 'Raimund Dachselt', 'affiliations': [], 'homepage': None, 'paperCount': 225, 'citationCount': 4033}]}",4.0,"{'DBLP': 'conf/uist/KlamkaD18', 'MAG': '2897459493', 'DOI': '10.1145/3266037.3266082', 'CorpusId': 52982843}",['Computer Science'],0.0,False,{'name': 'The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings'},10/11/2018,"['JournalArticle', 'Book', 'Conference']",20.0,Pushables: A DIY Approach for Fabricating Customizable and Self-Contained Tactile Membrane Dome Switches,https://www.semanticscholar.org/paper/4dee12718652e9285f4df09783ce281b8d50b3f1,UIST,2018
